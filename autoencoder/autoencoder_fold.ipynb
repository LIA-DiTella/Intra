{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import raiseExceptions\n",
    "from tokenize import Double\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from vec3 import Vec3\n",
    "import meshplot as mp\n",
    "import torch\n",
    "torch.manual_seed(125)\n",
    "import random\n",
    "random.seed(125)\n",
    "import torch_f as torch_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_fn(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        wrapper.count += 1\n",
    "        return f(*args, **kwargs)\n",
    "    wrapper.count = 0\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase nodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Class Node\n",
    "    \"\"\"\n",
    "    def __init__(self, value, radius, left = None, right = None, position = None, cl_prob= None, ce = None, mse = None):\n",
    "        self.left = left\n",
    "        self.data = value\n",
    "        self.radius = radius\n",
    "        self.position = position\n",
    "        self.right = right\n",
    "        self.prob = cl_prob\n",
    "        self.mse = mse\n",
    "        self.ce = ce\n",
    "        self.children = [self.left, self.right]\n",
    "    \n",
    "    def agregarHijo(self, children):\n",
    "\n",
    "        if self.right is None:\n",
    "            self.right = children\n",
    "        elif self.left is None:\n",
    "            self.left = children\n",
    "\n",
    "        else:\n",
    "            raise ValueError (\"solo arbol binario \")\n",
    "\n",
    "\n",
    "    def is_leaf(self):\n",
    "        if self.right is None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_two_child(self):\n",
    "        if self.right is not None and self.left is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_one_child(self):\n",
    "        if self.is_two_child():\n",
    "            return False\n",
    "        elif self.is_leaf():\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def childs(self):\n",
    "        if self.is_leaf():\n",
    "            return 0\n",
    "        if self.is_one_child():\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    \n",
    "    \n",
    "    def traverseInorder(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorder(root.left)\n",
    "            print (root.data, root.radius)\n",
    "            self.traverseInorder(root.right)\n",
    "\n",
    "    def traverseInorderLoss(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderLoss(root.left, loss)\n",
    "            loss.append(root.prob)\n",
    "            self.traverseInorderLoss(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderMSE(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderMSE(root.left, loss)\n",
    "            loss.append(root.mse)\n",
    "            self.traverseInorderMSE(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderCE(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderCE(root.left, loss)\n",
    "            loss.append(root.ce)\n",
    "            self.traverseInorderCE(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderChilds(self, root, l):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderChilds(root.left, l)\n",
    "            l.append(root.childs())\n",
    "            self.traverseInorderChilds(root.right, l)\n",
    "            return l\n",
    "\n",
    "    def preorder(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            print (root.data, root.radius)\n",
    "            self.preorder(root.left)\n",
    "            self.preorder(root.right)\n",
    "\n",
    "    def cloneBinaryTree(self, root):\n",
    "     \n",
    "        # base case\n",
    "        if root is None:\n",
    "            return None\n",
    "    \n",
    "        # create a new node with the same data as the root node\n",
    "        root_copy = Node(root.data, root.radius)\n",
    "    \n",
    "        # clone the left and right subtree\n",
    "        root_copy.left = self.cloneBinaryTree(root.left)\n",
    "        root_copy.right = self.cloneBinaryTree(root.right)\n",
    "    \n",
    "        # return cloned root node\n",
    "        return root_copy\n",
    "\n",
    "    def height(self, root):\n",
    "    # Check if the binary tree is empty\n",
    "        if root is None:\n",
    "            return 0 \n",
    "        # Recursively call height of each node\n",
    "        leftAns = self.height(root.left)\n",
    "        rightAns = self.height(root.right)\n",
    "    \n",
    "        # Return max(leftHeight, rightHeight) at each iteration\n",
    "        return max(leftAns, rightAns) + 1\n",
    "\n",
    "    # Print nodes at a current level\n",
    "    def printCurrentLevel(self, root, level):\n",
    "        if root is None:\n",
    "            return\n",
    "        if level == 1:\n",
    "            print(root.data, end=\" \")\n",
    "        elif level > 1:\n",
    "            self.printCurrentLevel(root.left, level-1)\n",
    "            self.printCurrentLevel(root.right, level-1)\n",
    "\n",
    "    def printLevelOrder(self, root):\n",
    "        h = self.height(root)\n",
    "        for i in range(1, h+1):\n",
    "            self.printCurrentLevel(root, i)\n",
    "\n",
    "\n",
    "    \n",
    "    def count_nodes(self, root, counter):\n",
    "        if   root is not None:\n",
    "            self.count_nodes(root.left, counter)\n",
    "            counter.append(root.data)\n",
    "            self.count_nodes(root.right, counter)\n",
    "            return counter\n",
    "\n",
    "    \n",
    "    def serialize(self, root):\n",
    "        def post_order(root):\n",
    "            if root:\n",
    "                post_order(root.left)\n",
    "                post_order(root.right)\n",
    "                ret[0] += str(root.data)+'_'+ str(root.radius) +';'\n",
    "                \n",
    "            else:\n",
    "                ret[0] += '#;'           \n",
    "\n",
    "        ret = ['']\n",
    "        post_order(root)\n",
    "        return ret[0][:-1]  # remove last ,\n",
    "\n",
    "    def toGraph( self, graph, index, dec, proc=True):\n",
    "        \n",
    "        \n",
    "        radius = self.radius.cpu().detach().numpy()\n",
    "        if dec:\n",
    "            radius= radius[0]\n",
    "        #print(\"posicion\", self.data, radius)\n",
    "        #print(\"right\", self.right)\n",
    "        \n",
    "        #graph.add_nodes_from( [ (index, {'posicion': radius[0:3], 'radio': radius[3] } ) ])\n",
    "        graph.add_nodes_from( [ (self.data, {'posicion': radius[0:3], 'radio': radius[3] } ) ])\n",
    "        \n",
    "\n",
    "        if self.right is not None:\n",
    "            #leftIndex = self.right.toGraph( graph, index + 1, dec)#\n",
    "            self.right.toGraph( graph, index + 1, dec)#\n",
    "            \n",
    "            #graph.add_edge( index, index + 1 )\n",
    "            graph.add_edge( self.data, self.right.data )\n",
    "            #if proc:\n",
    "            #    nx.set_edge_attributes( graph, {(index, index+1) : {'procesada':False}})\n",
    "        \n",
    "            if self.left is not None:\n",
    "                #retIndex = self.left.toGraph( graph, leftIndex, dec )#\n",
    "                self.left.toGraph( graph, 0, dec )#\n",
    "\n",
    "                #graph.add_edge( index, leftIndex)\n",
    "                graph.add_edge( self.data, self.left.data)\n",
    "                #if proc:\n",
    "                #    nx.set_edge_attributes( graph, {(index, leftIndex) : {'procesada':False}})\n",
    "            \n",
    "            else:\n",
    "                #return leftIndex\n",
    "                return\n",
    "\n",
    "        else:\n",
    "            #return index + 1\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTree( root, dec ):\n",
    "    graph = nx.Graph()\n",
    "    root.toGraph( graph, 0, dec)\n",
    "    edges=nx.get_edge_attributes(graph,'procesada')\n",
    "\n",
    "    p = mp.plot( np.array([ graph.nodes[v]['posicion'] for v in graph.nodes]), shading={'point_size':0.1}, return_plot=True)\n",
    "\n",
    "    for arista in graph.edges:\n",
    "        p.add_lines( graph.nodes[arista[0]]['posicion'], graph.nodes[arista[1]]['posicion'])\n",
    "\n",
    "    return \n",
    "\n",
    "def traverse(root, tree):\n",
    "       \n",
    "        if root is not None:\n",
    "            traverse(root.left, tree)\n",
    "            tree.append((root.radius, root.data))\n",
    "            traverse(root.right, tree)\n",
    "            return tree\n",
    "\n",
    "def traverse_2(tree1, tree2, t_l):\n",
    "       \n",
    "        if tree1 is not None:\n",
    "            traverse_2(tree1.left, tree2.left, t_l)\n",
    "            if tree2:\n",
    "                t_l.append((tree1.radius, tree2.radius))\n",
    "                print((tree1.radius, tree2.radius))\n",
    "            else:\n",
    "                t_l.append(tree1.radius)\n",
    "                print((tree1.radius))\n",
    "            traverse_2(tree1.right, tree2, t_l)\n",
    "            return t_l\n",
    "            \n",
    "\n",
    "def traverse_conexiones(root, tree):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            traverse_conexiones(root.left, tree)\n",
    "            if root.right is not None:\n",
    "                tree.append((root.data, root.right.data))\n",
    "            if root.left is not None:\n",
    "                tree.append((root.data, root.left.data))\n",
    "            traverse_conexiones(root.right, tree)\n",
    "            return tree\n",
    "\n",
    "def arbolAGrafo (nodoRaiz):\n",
    "    \n",
    "    conexiones = []\n",
    "    lineas = traverse_conexiones(nodoRaiz, conexiones)\n",
    "    tree = []\n",
    "    tree = traverse(nodoRaiz, tree)\n",
    "\n",
    "    vertices = []\n",
    "    verticesCrudos = []\n",
    "    for node in tree:\n",
    "        vertice = node[0][0][:3]\n",
    "        rad = node[0][0][-1]\n",
    "        num = node[1]\n",
    "        \n",
    "        #vertices.append((num, {'posicion': Vec3( vertice[0], vertice[1], vertice[2]), 'radio': rad} ))\n",
    "        vertices.append((len(verticesCrudos),{'posicion': Vec3( vertice[0], vertice[1], vertice[2]), 'radio': rad}))\n",
    "        verticesCrudos.append(vertice)\n",
    "\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from( vertices )\n",
    "    G.add_edges_from( lineas )\n",
    "    \n",
    "    return G\n",
    "\n",
    "@count_fn\n",
    "def createNode(data, radius, position = None, left = None, right = None, cl_prob = None, ce = None, mse=None):\n",
    "        \"\"\"\n",
    "        Utility function to create a node.\n",
    "        \"\"\"\n",
    "        return Node(data, radius, position, left, right, cl_prob, ce, mse)\n",
    " \n",
    "def deserialize(data):\n",
    "    if  not data:\n",
    "        return \n",
    "    nodes = data.split(';')  \n",
    "    #print(\"node\",nodes[3])\n",
    "    def post_order(nodes):\n",
    "                \n",
    "        if nodes[-1] == '#':\n",
    "            nodes.pop()\n",
    "            return None\n",
    "        node = nodes.pop().split('_')\n",
    "        data = int(node[0])\n",
    "        #radius = float(node[1])\n",
    "        #print(\"node\", node)\n",
    "        #breakpoint()\n",
    "        radius = node[1]\n",
    "        #print(\"radius\", radius)\n",
    "        rad = radius.split(\",\")\n",
    "        rad [0] = rad[0].replace('[','')\n",
    "        rad [3] = rad[3].replace(']','')\n",
    "        r = []\n",
    "        for value in rad:\n",
    "            r.append(float(value))\n",
    "        #r =[float(num) for num in radius if num.isdigit()]\n",
    "        r = torch.tensor(r, device=device)\n",
    "        #breakpoint()\n",
    "        root = createNode(data, r)\n",
    "        root.right = post_order(nodes)\n",
    "        root.left = post_order(nodes)\n",
    "        \n",
    "        return root    \n",
    "    return post_order(nodes)    \n",
    "\n",
    "\n",
    "def read_tree(filename):\n",
    "    with open('./trees/' +filename, \"r\") as f:\n",
    "        byte = f.read() \n",
    "        return byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternalEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, feature_size: int, hidden_size: int):\n",
    "        super(InternalEncoder, self).__init__()\n",
    "\n",
    "        #print(\"init\")\n",
    "        # Encoders atributos\n",
    "        self.attribute_lin_encoder_1 = nn.Linear(input_size,hidden_size)\n",
    "        self.attribute_lin_encoder_2 = nn.Linear(hidden_size,feature_size)\n",
    "\n",
    "        # Encoders derecho e izquierdo\n",
    "        self.right_lin_encoder_1 = nn.Linear(feature_size,feature_size)\n",
    "        self.left_lin_encoder_1  = nn.Linear(feature_size,feature_size)\n",
    "\n",
    "        # Encoder final\n",
    "        self.final_lin_encoder_1 = nn.Linear(2*feature_size, feature_size)\n",
    "\n",
    "        # Funciones / Parametros utiles\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.feature_size = feature_size\n",
    "        #print(\"fin\")\n",
    "\n",
    "    def forward(self, input, right_input, left_input):\n",
    "        #print(\"estoy encodeando\")\n",
    "        #print(\"input\", input)\n",
    "        # Encodeo los atributos\n",
    "        attributes = self.attribute_lin_encoder_1(input)\n",
    "        attributes = self.tanh(attributes)\n",
    "        attributes = self.attribute_lin_encoder_2(attributes)\n",
    "        attributes = self.tanh(attributes)\n",
    "        #print(\"attributes\", attributes)\n",
    "\n",
    "        # Encodeo el derecho\n",
    "        if right_input is not None:\n",
    "            #print(\"right input\", right_input)\n",
    "            context = self.right_lin_encoder_1(right_input)\n",
    "            #print(\"context derecho\", context)\n",
    "            # Encodeo el izquierdo\n",
    "            #print(\"left input\", left_input)\n",
    "            if left_input is not None:\n",
    "                \n",
    "                context += self.left_lin_encoder_1(left_input)\n",
    "                #print(\"context izquierdo\", context.shape)\n",
    "        else:\n",
    "            context = torch.zeros(input.shape[0],self.feature_size, requires_grad=True, device=device)\n",
    "        \n",
    "\n",
    "        context = self.tanh(context)\n",
    "        #print(\"context shape\", context.shape)\n",
    "        #print(\"attributes shape\", attributes.shape)\n",
    "        #if len(attributes.shape) == 1:\n",
    "            #print(\"len(attributes.shape)\",len(attributes.shape))\n",
    "            #attributes = attributes.reshape(1, 128)\n",
    "        #print(\"attributes shape\", attributes.shape)\n",
    "\n",
    "        feature = torch.cat((attributes,context), 1)\n",
    "        #print(\"feature cat\", feature.shape)\n",
    "\n",
    "        feature = self.final_lin_encoder_1(feature)\n",
    "        feature = self.tanh(feature)\n",
    "        #print(\"output\", feature)\n",
    "        return feature\n",
    "\n",
    "        #print(\"radius\", radius.shape)\n",
    "        #if right_input is not None:\n",
    "        #    context = self.right(right_input)\n",
    "        #    #print(\"context\", context.shape)\n",
    "        #    if left_input is not None:\n",
    "        #        context += self.left(left_input)\n",
    "        #        #print(\"context2\", context.shape)\n",
    "        #    context = self.tanh(context)\n",
    "        #    #print(\"context3\", context.shape)\n",
    "        #    feature = torch.cat((radius,context), 1)\n",
    "        #    #print(\"feature\", feature.shape)\n",
    "        #    feature = self.encoder(feature)\n",
    "        #else:\n",
    "        #    feature = self.l3(radius)\n",
    "        #    feature = self.tanh(feature)\n",
    "        #    feature = self.leafencoder(radius)\n",
    "        #    #print(\"feature\", feature.shape)\n",
    "\n",
    "        #feature = self.tanh(feature)\n",
    "        #return feature\n",
    "    \n",
    "\n",
    "class GRASSEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, feature_size : int, hidden_size: int):\n",
    "        super(GRASSEncoder, self).__init__()\n",
    "        self.leaf_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        self.internal_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        self.bifurcation_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        \n",
    "    def leafEncoder(self, node, right=None, left = None):\n",
    "        return self.internal_encoder(node, right, left)\n",
    "    def internalEncoder(self, node, right, left = None):\n",
    "        return self.internal_encoder(node, right, left)\n",
    "    def bifurcationEncoder(self, node, right, left):\n",
    "        \n",
    "        return self.bifurcation_encoder(node, right, left)\n",
    "\n",
    "Grassencoder = GRASSEncoder(input_size = 4, feature_size=128, hidden_size=256)\n",
    "Grassencoder = Grassencoder.to(device)\n",
    "\n",
    "\n",
    "def encode_structure_fold(fold, root):\n",
    "    \n",
    "    \n",
    "    def encode_node(node):\n",
    "        \n",
    "        if node is None:\n",
    "            return\n",
    "        \n",
    "        if node.is_leaf():\n",
    "            return fold.add('leafEncoder', node.radius)\n",
    "        else:\n",
    "            left = encode_node(node.left)\n",
    "            right = encode_node(node.right)\n",
    "            if left is not None:\n",
    "             \n",
    "                return fold.add('bifurcationEncoder', node.radius, right, left)\n",
    "            else:\n",
    "                return fold.add('internalEncoder', node.radius, right)\n",
    "        \n",
    "\n",
    "    encoding = encode_node(root)\n",
    "    \n",
    "    return encoding\n",
    "  \n",
    "def encode_structure(root):\n",
    "    \n",
    "    def encode_node(node):\n",
    "          \n",
    "        if node is None:\n",
    "            return\n",
    "        if node.is_leaf():\n",
    "            return Grassencoder.leafEncoder(node.radius.reshape(-1,4))\n",
    "        else :\n",
    "            left = encode_node(node.left)\n",
    "            right = encode_node(node.right)\n",
    "            if left is not None:\n",
    "                return Grassencoder.bifurcationEncoder(node.radius.reshape(-1,4), right, left)\n",
    "            else:\n",
    "                return Grassencoder.internalEncoder(node.radius.reshape(-1,4), right)\n",
    "        \n",
    "\n",
    "    encoding = encode_node(root)\n",
    "   \n",
    "    return encoding\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerar_nodos(root, count):\n",
    "    if root is not None:\n",
    "        numerar_nodos(root.left, count)\n",
    "        root.data = len(count)\n",
    "        count.append(1)\n",
    "        numerar_nodos(root.right, count)\n",
    "        return \n",
    "\n",
    "\n",
    "def traversefeatures(root, features):\n",
    "       \n",
    "    if root is not None:\n",
    "        traversefeatures(root.left, features)\n",
    "        features.append(root.radius)\n",
    "        traversefeatures(root.right, features)\n",
    "        return features\n",
    "\n",
    "def norm(root, minx, miny, minz, minr, maxx, maxy, maxz, maxr):\n",
    "    \n",
    "    if root is not None:\n",
    "        mx = minx.clone().detach()\n",
    "        my = miny.clone().detach()\n",
    "        mz = minz.clone().detach()\n",
    "        mr = minr.clone().detach()\n",
    "        Mx = maxx.clone().detach()\n",
    "        My = maxy.clone().detach()\n",
    "        Mz = maxz.clone().detach()\n",
    "        Mr = maxr.clone().detach()\n",
    "       \n",
    "        root.radius[0] = (root.radius[0] - minx)/(maxx - minx)\n",
    "        root.radius[1] = (root.radius[1] - miny)/(maxy - miny)\n",
    "        root.radius[2] = (root.radius[2] - minz)/(maxz - minz)\n",
    "        root.radius[3] = (root.radius[3] - minr)/(maxr - minr)\n",
    "        \n",
    "        norm(root.left, mx, my, mz, mr, Mx, My, Mz, Mr)\n",
    "        norm(root.right, mx, my, mz, mr, Mx, My, Mz, Mr)\n",
    "        return \n",
    "\n",
    "def normalize_features(root):\n",
    "    features = []\n",
    "    features = traversefeatures(root, features)\n",
    "    \n",
    "    x = [tensor[0] for tensor in features]\n",
    "    y = [tensor[1] for tensor in features]\n",
    "    z = [tensor[2] for tensor in features]\n",
    "    r = [tensor[3] for tensor in features]\n",
    " \n",
    "    norm(root, min(x), min(y), min(z), min(r), max(x), max(y), max(z), max(r))\n",
    "\n",
    "    return \n",
    "\n",
    "def traversefeatures(root, features):\n",
    "       \n",
    "    if root is not None:\n",
    "        traversefeatures(root.left, features)\n",
    "        features.append(root.radius)\n",
    "        traversefeatures(root.right, features)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test2.dat']\n"
     ]
    }
   ],
   "source": [
    "def my_collate(batch):\n",
    "    return batch\n",
    "\n",
    "#t_list = ['ArteryObjAN1-7.dat','ArteryObjAN1-0.dat', 'ArteryObjAN1-17.dat',  'ArteryObjAN1-11.dat']\n",
    "\n",
    "#t_list = ['ArteryObjAN1-0.dat','ArteryObjAN1-7.dat', 'ArteryObjAN1-17.dat',  'ArteryObjAN1-11.dat', 'ArteryObjAN1-19.dat', 'ArteryObjAN2-4.dat', 'ArteryObjAN2-6.dat', \n",
    "#           'ArteryObjAN25-18.dat']\n",
    "#t_list = ['ArteryObjAN1-7.dat']\n",
    "t_list = ['test2.dat']\n",
    "\n",
    "#t_list = ['ArteryObjAN31-14.dat']\n",
    "#t_list = os.listdir(\"./trees\")[:20]\n",
    "print(t_list)\n",
    "class tDataset(Dataset):\n",
    "    def __init__(self, dir, transform=None):\n",
    "        self.names = dir\n",
    "        self.transform = transform\n",
    "        self.data = [] #lista con las strings de todos los arboles\n",
    "        for file in self.names:\n",
    "            self.data.append(read_tree(file))\n",
    "        self.trees = []\n",
    "        for tree in self.data:\n",
    "            deserial = deserialize(tree)\n",
    "            normalize_features(deserial)\n",
    "            self.trees.append(deserial)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #file = self.names[idx]\n",
    "        #string = read_tree(file)\n",
    "        tree = self.trees[idx]\n",
    "        return tree\n",
    "\n",
    "batch_size = 1\n",
    "dataset = tDataset(t_list)\n",
    "data_loader = DataLoader(dataset, batch_size = batch_size, shuffle=True, collate_fn=my_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb08da26c0b04c69b4228bfc90cc798f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.5, 0.5,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input = iter(data_loader).next()[0]\n",
    "plotTree(input, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED con batch [tensor([[ 3.5992e-03,  3.0452e-02,  1.1639e-01, -7.0448e-02,  2.3071e-03,\n",
      "          1.1119e-01, -3.1330e-02, -3.9756e-02, -1.1225e-01,  3.9869e-02,\n",
      "          7.9754e-02, -7.0361e-02, -4.0752e-02,  1.3252e-01, -1.0837e-02,\n",
      "          4.1937e-02, -5.1220e-02,  4.7678e-03, -1.5894e-01,  1.5764e-01,\n",
      "         -1.9491e-02,  4.6329e-02,  6.6132e-02,  6.7206e-02,  1.0342e-02,\n",
      "         -4.3742e-02, -5.9580e-02, -5.4157e-02, -9.6915e-02, -3.2758e-02,\n",
      "         -2.4366e-02,  1.6915e-01,  1.0665e-01, -1.5666e-01, -7.0915e-02,\n",
      "          1.1217e-01,  1.8687e-01,  4.4914e-03, -1.0712e-01,  5.1256e-02,\n",
      "         -9.2797e-02, -1.3917e-02,  5.8721e-02,  1.5763e-04,  1.0076e-01,\n",
      "         -9.0281e-02,  1.0091e-01, -5.6798e-02, -2.4207e-02, -7.0614e-02,\n",
      "         -5.5528e-02,  6.8203e-04, -1.6714e-02,  8.0128e-02, -2.7864e-02,\n",
      "          3.0889e-02,  8.3806e-03,  6.8090e-02,  7.3218e-03, -2.7974e-02,\n",
      "         -1.0691e-01,  1.0326e-01, -8.5506e-04, -1.6772e-03,  7.5442e-02,\n",
      "          3.1093e-02, -3.2103e-03,  3.4352e-02, -1.0159e-01,  1.5983e-01,\n",
      "         -9.4558e-02, -1.1204e-01,  1.1098e-01, -1.4517e-01, -2.5494e-02,\n",
      "         -2.9110e-02,  6.7254e-02, -2.6295e-02, -3.9450e-02,  2.4389e-03,\n",
      "         -9.1727e-02, -6.0843e-02, -5.3055e-02, -4.1518e-03, -5.8284e-02,\n",
      "          6.4615e-02, -1.0360e-01,  8.9451e-02,  3.9304e-02,  3.0748e-03,\n",
      "          1.0822e-02, -3.8858e-02, -9.0052e-02, -6.7759e-02,  7.8748e-02,\n",
      "          9.0189e-02,  7.9519e-02,  3.9419e-02,  3.5190e-02, -4.2483e-02,\n",
      "         -6.6232e-02, -1.9641e-01,  2.7919e-02, -3.1052e-02, -4.1626e-02,\n",
      "         -1.5476e-02,  1.1209e-02, -1.3403e-01, -7.4311e-03, -6.7120e-02,\n",
      "          2.1538e-02,  1.1102e-01,  9.2058e-02, -2.0955e-01,  4.3029e-02,\n",
      "         -1.8786e-01, -2.0657e-02, -9.4325e-02, -3.0276e-02, -6.6404e-02,\n",
      "          1.6267e-01,  4.0560e-02,  6.2460e-02, -2.1782e-02,  3.9165e-02,\n",
      "         -3.7387e-02,  5.8074e-02,  6.9672e-02]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "import torch_f\n",
    "enc_fold = torch_f.Fold(device)\n",
    "enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "batch = iter(data_loader).next()\n",
    "for example in batch:\n",
    "        #enc_fold.add('leafEncoder', example.radius)\n",
    "        #enc_fold_nodes.append(enc_fold.add('leafEncoder', example.radius))\n",
    "        enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "\n",
    "        #print(\"enc fold nodes\", enc_fold)\n",
    "enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "print(\"ENCODED con batch\",enc_fold_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encodeado sin batch tensor([[ 3.5992e-03,  3.0452e-02,  1.1639e-01, -7.0448e-02,  2.3071e-03,\n",
      "          1.1119e-01, -3.1330e-02, -3.9756e-02, -1.1225e-01,  3.9869e-02,\n",
      "          7.9754e-02, -7.0361e-02, -4.0752e-02,  1.3252e-01, -1.0837e-02,\n",
      "          4.1937e-02, -5.1220e-02,  4.7678e-03, -1.5894e-01,  1.5764e-01,\n",
      "         -1.9491e-02,  4.6329e-02,  6.6132e-02,  6.7206e-02,  1.0342e-02,\n",
      "         -4.3742e-02, -5.9580e-02, -5.4157e-02, -9.6915e-02, -3.2758e-02,\n",
      "         -2.4366e-02,  1.6915e-01,  1.0665e-01, -1.5666e-01, -7.0915e-02,\n",
      "          1.1217e-01,  1.8687e-01,  4.4914e-03, -1.0712e-01,  5.1256e-02,\n",
      "         -9.2797e-02, -1.3917e-02,  5.8721e-02,  1.5763e-04,  1.0076e-01,\n",
      "         -9.0281e-02,  1.0091e-01, -5.6798e-02, -2.4207e-02, -7.0614e-02,\n",
      "         -5.5528e-02,  6.8203e-04, -1.6714e-02,  8.0128e-02, -2.7864e-02,\n",
      "          3.0889e-02,  8.3806e-03,  6.8090e-02,  7.3218e-03, -2.7974e-02,\n",
      "         -1.0691e-01,  1.0326e-01, -8.5506e-04, -1.6772e-03,  7.5442e-02,\n",
      "          3.1093e-02, -3.2103e-03,  3.4352e-02, -1.0159e-01,  1.5983e-01,\n",
      "         -9.4558e-02, -1.1204e-01,  1.1098e-01, -1.4517e-01, -2.5494e-02,\n",
      "         -2.9110e-02,  6.7254e-02, -2.6295e-02, -3.9450e-02,  2.4389e-03,\n",
      "         -9.1727e-02, -6.0843e-02, -5.3055e-02, -4.1518e-03, -5.8284e-02,\n",
      "          6.4615e-02, -1.0360e-01,  8.9451e-02,  3.9304e-02,  3.0748e-03,\n",
      "          1.0822e-02, -3.8858e-02, -9.0052e-02, -6.7759e-02,  7.8748e-02,\n",
      "          9.0189e-02,  7.9519e-02,  3.9419e-02,  3.5190e-02, -4.2483e-02,\n",
      "         -6.6232e-02, -1.9641e-01,  2.7919e-02, -3.1052e-02, -4.1626e-02,\n",
      "         -1.5476e-02,  1.1209e-02, -1.3403e-01, -7.4311e-03, -6.7120e-02,\n",
      "          2.1538e-02,  1.1102e-01,  9.2058e-02, -2.0955e-01,  4.3029e-02,\n",
      "         -1.8786e-01, -2.0657e-02, -9.4325e-02, -3.0276e-02, -6.6404e-02,\n",
      "          1.6267e-01,  4.0560e-02,  6.2460e-02, -2.1782e-02,  3.9165e-02,\n",
      "         -3.7387e-02,  5.8074e-02,  6.9672e-02]], device='cuda:0',\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for data in data_loader:\n",
    "    data = data[0]\n",
    "\n",
    "    enc_f = encode_structure(data).to(device)\n",
    "\n",
    "print(\"encodeado sin batch\", enc_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_fold_nodes[0]-enc_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[4]\n",
      "4.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "n_no = []\n",
    "qzero = 0\n",
    "qOne = 0\n",
    "qtwo = 0\n",
    "\n",
    "for batch in data_loader:\n",
    "    for tree in batch:\n",
    "        count = []\n",
    "        n = tree.count_nodes(tree, count)\n",
    "        n_no.append(len(n))\n",
    "        li = []\n",
    "        tree.traverseInorderChilds(tree, li)\n",
    "        zero = [a for a in li if a == 0]\n",
    "        one = [a for a in li if a == 1]\n",
    "        two = [a for a in li if a == 2]\n",
    "        qzero += len(zero)\n",
    "        qOne += len(one)\n",
    "        qtwo += len(two)\n",
    "\n",
    "print(len(data_loader)*batch_size)\n",
    "print(n_no)\n",
    "nprom = np.mean(n_no)\n",
    "print(nprom)\n",
    "qzero /= len(data_loader)*batch_size\n",
    "qOne /= len(data_loader)*batch_size\n",
    "qtwo /= len(data_loader)*batch_size\n",
    "\n",
    "print(qzero)\n",
    "print(qOne)\n",
    "print(qtwo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en el loop creo un fold, mando este fold con cada uno de los arboles del batch a encode_structure_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.mlp1 = nn.Linear(latent_size, hidden_size)\n",
    "        self.mlp2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.mlp3 = nn.Linear(hidden_size, 3)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_feature):\n",
    "        #print(\"classifier input\", input_feature)\n",
    "        output = self.mlp1(input_feature)\n",
    "        output = self.tanh(output)\n",
    "        output = self.mlp2(output)\n",
    "        output = self.tanh(output)\n",
    "        output = self.mlp3(output)\n",
    "        #print(\"classifier output\", output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class InternalDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size: int):\\n        super(InternalDecoder, self).__init__()\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.lp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp_right = nn.Linear(latent_size, latent_size)\\n        self.tanh = nn.Tanh()\\n        self.mlp2 = nn.Linear(latent_size,4)\\n\\n    def forward(self, parent_feature):\\n        #print(\"internal decoder\")\\n        #print(\"input\", parent_feature.shape)\\n        vector = self.mlp(parent_feature)\\n        vector = self.tanh(vector)\\n        vector = self.lp2(vector)\\n        vector = self.tanh(vector)\\n        right_feature = self.mlp_right(vector)\\n        right_feature = self.tanh(right_feature)\\n        rad_feature = self.mlp2(vector)\\n\\n        return right_feature, rad_feature\\n\\nclass BifurcationDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size : int):\\n        super(BifurcationDecoder, self).__init__()\\n        #self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.lp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp_left = nn.Linear(latent_size, latent_size)\\n        self.mlp_right = nn.Linear(latent_size, latent_size)\\n        self.mlp2 = nn.Linear(latent_size,4)\\n        self.tanh = nn.Tanh()\\n\\n    def forward(self, parent_feature):\\n        #print(\"bifurcation decoder input\", parent_feature.shape)\\n        parent_feature = parent_feature.reshape(-1,128)\\n        #print(\"bifurcation decoder input\", parent_feature.shape)\\n        vector = self.mlp(parent_feature)\\n        #print(\"v1\", vector.shape)\\n        vector = self.tanh(vector)\\n        #print(\"v2\", vector.shape)\\n        vector = self.lp2(vector)\\n        #print(\"v3\", vector.shape)\\n        vector = self.tanh(vector)\\n        left_feature = self.mlp_left(vector)\\n        left_feature = self.tanh(left_feature)\\n        right_feature = self.mlp_right(vector)\\n        right_feature = self.tanh(right_feature)\\n        rad_feature = self.mlp2(vector)\\n        #print(\"exiting bif dec\")\\n        return left_feature, right_feature, rad_feature\\n\\n\\nclass featureDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size: int):\\n        super(featureDecoder, self).__init__()\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.mlp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp3 = nn.Linear(latent_size, latent_size)\\n        self.tanh = nn.Tanh()\\n        self.mlp4 = nn.Linear(latent_size,4)\\n\\n    def forward(self, parent_feature):\\n        #print(\"feature decoder input\", parent_feature.shape)\\n\\n        vector = self.mlp(parent_feature)\\n        vector = self.tanh(vector)\\n        vector = self.mlp2(vector)\\n        vector = self.tanh(vector)\\n        vector = self.mlp3(vector)\\n        vector = self.tanh(vector)\\n        vector = self.mlp4(vector)\\n       \\n        return vector\\n\\n\\n\\nclass GRASSDecoder(nn.Module):\\n    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\\n        super(GRASSDecoder, self).__init__()\\n        self.feature_decoder = featureDecoder(latent_size, hidden_size)\\n        self.internal_decoder = InternalDecoder(latent_size, hidden_size)\\n        self.bifurcation_decoder = BifurcationDecoder(latent_size, hidden_size)\\n        self.node_classifier = NodeClassifier(latent_size, hidden_size)\\n        self.mseLoss = nn.MSELoss()  # pytorch\\'s mean squared error loss\\n        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch\\'s cross entropy loss (NOTE: no softmax is needed before)\\n\\n    def featureDecoder(self, feature):\\n        return self.feature_decoder(feature)\\n\\n    def internalDecoder(self, feature):\\n        return self.internal_decoder(feature)\\n\\n    def bifurcationDecoder(self, feature):\\n        return self.bifurcation_decoder(feature)\\n\\n    def nodeClassifier(self, feature):\\n        return self.node_classifier(feature)\\n\\n    def calcularLossAtributo(self, nodo, radio):\\n        if nodo is None:\\n            return\\n        else:\\n            #print(\"radio\", radio)\\n            #print(\"nodo\", nodo)\\n            nodo = torch.stack(nodo)\\n            #print(\"nodo stack\", nodo)\\n            #radio = radio.reshape(-1,4)\\n        \\n            #return mse\\n            #return torch.cat([self.mseLoss(b, gt) for b, gt in zip(radio, nodo)], 0)\\n            z = zip(radio.reshape(-1,4), nodo.reshape(-1,4))\\n            \\n            #for b, gt in z:\\n            #    print(\"bgt\", b, gt)\\n                \\n            \\n            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\\n            #print(\"loss\", l)\\n            return l\\n\\n\\n    def classifyLossEstimator(self, label_vector, original):\\n        if original is None:\\n            return\\n        else:\\n           \\n            v = []\\n            for o in original:\\n                if o == 0:\\n                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\\n                if o == 1:\\n                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\\n                if o == 2:\\n                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\\n                v.append(vector)\\n\\n            v = torch.stack(v)\\n            z = zip(label_vector.reshape(-1,3), v.reshape(-1,3))   \\n            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\\n            \\n            return l\\n            #return c\\n       # return torch.cat([self.creLoss(l.unsqueeze(0), gt).mul(0.2) for l, gt in zip(label_vector, gt_label_vector)], 0)\\n\\n    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\\n        \\n        v = v1.add(v2)\\n        #print(\"v0\", v)\\n        if v3 is not None:\\n            v = v.add(v3)\\n        #print(\"v0\", v)\\n        if v4 is not None:\\n            v = v.add(v4)\\n       \\n        return v\\nif qzero == 0:\\n    qzero = 1\\nif qOne == 0:\\n    qOne = 1\\nif qtwo == 0:\\n    qtwo = 1\\nmult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\\nGrassdecoder = GRASSDecoder(latent_size=128, hidden_size=256, mult = mult)\\nGrassdecoder = Grassdecoder.to(device)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class InternalDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size: int):\n",
    "        super(InternalDecoder, self).__init__()\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, latent_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.mlp2 = nn.Linear(latent_size,4)\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"internal decoder\")\n",
    "        #print(\"input\", parent_feature.shape)\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.lp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        rad_feature = self.mlp2(vector)\n",
    "\n",
    "        return right_feature, rad_feature\n",
    "\n",
    "class BifurcationDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(BifurcationDecoder, self).__init__()\n",
    "        #self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp_left = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp2 = nn.Linear(latent_size,4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"bifurcation decoder input\", parent_feature.shape)\n",
    "        parent_feature = parent_feature.reshape(-1,128)\n",
    "        #print(\"bifurcation decoder input\", parent_feature.shape)\n",
    "        vector = self.mlp(parent_feature)\n",
    "        #print(\"v1\", vector.shape)\n",
    "        vector = self.tanh(vector)\n",
    "        #print(\"v2\", vector.shape)\n",
    "        vector = self.lp2(vector)\n",
    "        #print(\"v3\", vector.shape)\n",
    "        vector = self.tanh(vector)\n",
    "        left_feature = self.mlp_left(vector)\n",
    "        left_feature = self.tanh(left_feature)\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        rad_feature = self.mlp2(vector)\n",
    "        #print(\"exiting bif dec\")\n",
    "        return left_feature, right_feature, rad_feature\n",
    "\n",
    "\n",
    "class featureDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size: int):\n",
    "        super(featureDecoder, self).__init__()\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp3 = nn.Linear(latent_size, latent_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.mlp4 = nn.Linear(latent_size,4)\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"feature decoder input\", parent_feature.shape)\n",
    "\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp3(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp4(vector)\n",
    "       \n",
    "        return vector\n",
    "\n",
    "\n",
    "\n",
    "class GRASSDecoder(nn.Module):\n",
    "    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\n",
    "        super(GRASSDecoder, self).__init__()\n",
    "        self.feature_decoder = featureDecoder(latent_size, hidden_size)\n",
    "        self.internal_decoder = InternalDecoder(latent_size, hidden_size)\n",
    "        self.bifurcation_decoder = BifurcationDecoder(latent_size, hidden_size)\n",
    "        self.node_classifier = NodeClassifier(latent_size, hidden_size)\n",
    "        self.mseLoss = nn.MSELoss()  # pytorch's mean squared error loss\n",
    "        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch's cross entropy loss (NOTE: no softmax is needed before)\n",
    "\n",
    "    def featureDecoder(self, feature):\n",
    "        return self.feature_decoder(feature)\n",
    "\n",
    "    def internalDecoder(self, feature):\n",
    "        return self.internal_decoder(feature)\n",
    "\n",
    "    def bifurcationDecoder(self, feature):\n",
    "        return self.bifurcation_decoder(feature)\n",
    "\n",
    "    def nodeClassifier(self, feature):\n",
    "        return self.node_classifier(feature)\n",
    "\n",
    "    def calcularLossAtributo(self, nodo, radio):\n",
    "        if nodo is None:\n",
    "            return\n",
    "        else:\n",
    "            #print(\"radio\", radio)\n",
    "            #print(\"nodo\", nodo)\n",
    "            nodo = torch.stack(nodo)\n",
    "            #print(\"nodo stack\", nodo)\n",
    "            #radio = radio.reshape(-1,4)\n",
    "        \n",
    "            #return mse\n",
    "            #return torch.cat([self.mseLoss(b, gt) for b, gt in zip(radio, nodo)], 0)\n",
    "            z = zip(radio.reshape(-1,4), nodo.reshape(-1,4))\n",
    "            \n",
    "            #for b, gt in z:\n",
    "            #    print(\"bgt\", b, gt)\n",
    "                \n",
    "            \n",
    "            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\n",
    "            #print(\"loss\", l)\n",
    "            return l\n",
    "\n",
    "\n",
    "    def classifyLossEstimator(self, label_vector, original):\n",
    "        if original is None:\n",
    "            return\n",
    "        else:\n",
    "           \n",
    "            v = []\n",
    "            for o in original:\n",
    "                if o == 0:\n",
    "                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\n",
    "                if o == 1:\n",
    "                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\n",
    "                if o == 2:\n",
    "                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\n",
    "                v.append(vector)\n",
    "\n",
    "            v = torch.stack(v)\n",
    "            z = zip(label_vector.reshape(-1,3), v.reshape(-1,3))   \n",
    "            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\n",
    "            \n",
    "            return l\n",
    "            #return c\n",
    "       # return torch.cat([self.creLoss(l.unsqueeze(0), gt).mul(0.2) for l, gt in zip(label_vector, gt_label_vector)], 0)\n",
    "\n",
    "    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\n",
    "        \n",
    "        v = v1.add(v2)\n",
    "        #print(\"v0\", v)\n",
    "        if v3 is not None:\n",
    "            v = v.add(v3)\n",
    "        #print(\"v0\", v)\n",
    "        if v4 is not None:\n",
    "            v = v.add(v4)\n",
    "       \n",
    "        return v\n",
    "if qzero == 0:\n",
    "    qzero = 1\n",
    "if qOne == 0:\n",
    "    qOne = 1\n",
    "if qtwo == 0:\n",
    "    qtwo = 1\n",
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "Grassdecoder = GRASSDecoder(latent_size=128, hidden_size=256, mult = mult)\n",
    "Grassdecoder = Grassdecoder.to(device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(Decoder, self).__init__()\n",
    "        #self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp_left = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp3 = nn.Linear(latent_size,latent_size)\n",
    "        self.mlp2 = nn.Linear(latent_size,4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def common_branch(self, parent_feature):\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.lp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        return vector\n",
    "\n",
    "    def attr_branch(self, vector):\n",
    "        vector = self.mlp2(vector)        \n",
    "        return vector\n",
    "\n",
    "    def right_branch(self, vector):\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        return right_feature\n",
    "\n",
    "    def left_branch(self, vector):\n",
    "        left_feature = self.mlp_left(vector)\n",
    "        left_feature = self.tanh(left_feature)\n",
    "        return left_feature\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "      \n",
    "        vector      = self.common_branch(parent_feature)\n",
    "        attr_vector = self.attr_branch(vector)\n",
    "        return attr_vector \n",
    "\n",
    "    def forward1(self, parent_feature):\n",
    "    \n",
    "\n",
    "        vector       = self.common_branch(parent_feature)\n",
    "        attr_vector  = self.attr_branch(vector)\n",
    "        right_vector = self.right_branch(vector)\n",
    "        \n",
    "        #print(\"right vector\", right_vector)\n",
    "        #print(\"radius\", attr_vector)\n",
    "        return right_vector, attr_vector\n",
    "\n",
    "    def forward2(self, parent_feature):\n",
    "       \n",
    "\n",
    "        vector       = self.common_branch(parent_feature)\n",
    "        attr_vector  = self.attr_branch(vector)\n",
    "        right_vector = self.right_branch(vector)\n",
    "        left_vector  = self.left_branch(vector)\n",
    "        #print(\"left vector\", left_vector)\n",
    "        #print(\"right vector\", right_vector)\n",
    "        #print(\"radius\", attr_vector)\n",
    "        return left_vector, right_vector, attr_vector\n",
    "\n",
    "\n",
    "\n",
    "class GRASSDecoder(nn.Module):\n",
    "    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\n",
    "        super(GRASSDecoder, self).__init__()\n",
    "        self.decoder = Decoder(latent_size, hidden_size)\n",
    "        self.node_classifier = NodeClassifier(latent_size, hidden_size)\n",
    "        self.mseLoss = nn.MSELoss()  # pytorch's mean squared error loss\n",
    "        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch's cross entropy loss (NOTE: no softmax is needed before)\n",
    "\n",
    "\n",
    "    def featureDecoder(self, feature):\n",
    "        return self.decoder.forward(feature)\n",
    "\n",
    "    def internalDecoder(self, feature):\n",
    "        return self.decoder.forward1(feature)\n",
    "\n",
    "    def bifurcationDecoder(self, feature):\n",
    "        return self.decoder.forward2(feature)\n",
    "\n",
    "    def nodeClassifier(self, feature):\n",
    "        return self.node_classifier(feature)\n",
    "\n",
    "    def calcularLossAtributo(self, nodo, radio):\n",
    "        #print(\"nodo\", nodo)\n",
    "        #print(\"radio\", radio)\n",
    "        a, b = list(zip(*nodo))# a son los atributos, b los pesos\n",
    "        if nodo is None:\n",
    "            return\n",
    "        else:\n",
    "            nodo = torch.stack(list(a))\n",
    "        \n",
    "            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\n",
    "            #print(\"mse\", l)\n",
    "            return l\n",
    "\n",
    "\n",
    "    def classifyLossEstimator(self, label_vector, original):\n",
    "        if original is None:\n",
    "            return\n",
    "        else:\n",
    "           \n",
    "            v = []\n",
    "            for o in original:\n",
    "                if o == 0:\n",
    "                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\n",
    "                if o == 1:\n",
    "                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\n",
    "                if o == 2:\n",
    "                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\n",
    "                v.append(vector)\n",
    "            #print(\"////\")\n",
    "            #print(\"original\", v)\n",
    "            #print(\"prediction\", label_vector)\n",
    "            #print(\"////\")\n",
    "\n",
    "            v = torch.stack(v)\n",
    "            #for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3)):\n",
    "            #    print(\"...\")\n",
    "            #    print(\"b\", b)\n",
    "            #    print(\"gt\", gt)\n",
    "            #    print(\"...\")\n",
    "            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\n",
    "            #print(\"ce\", l)\n",
    "\n",
    "            return l\n",
    "            #return c\n",
    "       # return torch.cat([self.creLoss(l.unsqueeze(0), gt).mul(0.2) for l, gt in zip(label_vector, gt_label_vector)], 0)\n",
    "\n",
    "    '''\n",
    "    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\n",
    "        \n",
    "        print(\"loss estructura\", v1)\n",
    "        print(\"loss atributo\", v2)\n",
    "        print(\"right loss\", v3)\n",
    "        print(\"left loss\", v4)\n",
    "\n",
    "\n",
    "        v = v1.add(v2)\n",
    "        #print(\"v0\", v)\n",
    "        if v3 is not None:\n",
    "            v = v.add(v3)\n",
    "        #print(\"v0\", v)\n",
    "        if v4 is not None:\n",
    "            v = v.add(v4)\n",
    "       \n",
    "        return v\n",
    "\n",
    "    '''\n",
    "    def vectorAdder(self, v1, v2):\n",
    "        v = v1.add(v2)\n",
    "        return v\n",
    "\n",
    "if qzero == 0:\n",
    "    qzero = 1\n",
    "if qOne == 0:\n",
    "    qOne = 1\n",
    "if qtwo == 0:\n",
    "    qtwo = 1\n",
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "\n",
    "Grassdecoder = GRASSDecoder(latent_size=128, hidden_size=256, mult = mult)\n",
    "Grassdecoder = Grassdecoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5000, 1.0000, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "print(mult)\n",
    "def calcularLossEstructura(cl_p, original):\n",
    "    \n",
    "    mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "    ce = nn.CrossEntropyLoss(weight = mult)\n",
    "\n",
    "    if original.childs() == 0:\n",
    "        vector = [1, 0, 0] \n",
    "    if original.childs() == 1:\n",
    "        vector = [0, 1, 0]\n",
    "    if original.childs() == 2:\n",
    "        vector = [0, 0, 1] \n",
    "\n",
    "    #print(\"original\", vector)\n",
    "    #print(\"prediction\", cl_p)\n",
    "    c = ce(cl_p, torch.tensor(vector, device=device, dtype = torch.float).reshape(1, 3))\n",
    "    #print(\"ce\", 0.4*c)\n",
    "    return c\n",
    "\n",
    "\n",
    "def calcularLossAtributo(nodo, radio):\n",
    "    #print(\"nodo\", nodo)\n",
    "    #print(\"radio\", radio)\n",
    "\n",
    "    radio = radio.reshape(-1,4)\n",
    "    nodo = nodo.reshape(-1,4)\n",
    "    l2    = nn.MSELoss()\n",
    "   \n",
    "    mse = l2(radio, nodo)\n",
    "    #print(\"mse\", mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def height(root):\n",
    "    # Check if the binary tree is empty\n",
    "        if root is None:\n",
    "            return 0 \n",
    "        # Recursively call height of each node\n",
    "        leftAns = height(root.left)\n",
    "        rightAns = height(root.right)\n",
    "    \n",
    "        # Return max(leftHeight, rightHeight) at each iteration\n",
    "        return max(leftAns, rightAns) + 1\n",
    "\n",
    "def printCurrentLevel(root, level):\n",
    "        if root is None:\n",
    "            return\n",
    "        if level == 1:\n",
    "            print(root.data, end=\" \")\n",
    "        elif level > 1:\n",
    "            printCurrentLevel(root.left, level-1)\n",
    "            printCurrentLevel(root.right, level-1)\n",
    "\n",
    "def printLevelOrder(root):\n",
    "    h = height(root)\n",
    "    for i in range(1, h+1):\n",
    "        printCurrentLevel(root, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 5 4 "
     ]
    }
   ],
   "source": [
    "printLevelOrder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_structure_fold_grass(fold, v, root):\n",
    "    \n",
    "    def decode_node(fold, v, node):\n",
    "        \n",
    "        \n",
    "        if node.childs() == 0 : ##output del classifier\n",
    "            radio = fold.add('featureDecoder', v)\n",
    "            lossAtributo = fold.add('calcularLossAtributo', node, radio)\n",
    "            #lossAtributo = calcularLossAtributo( node, radio)\n",
    "            label = fold.add('nodeClassifier', v)\n",
    "            #lossEstructura = calcularLossEstructura()\n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)  \n",
    "            #nd = createNode(1,radio, ce = lossEstructura,  mse = lossAtributo)          \n",
    "            return fold.add('vectorAdder', lossEstructura, lossAtributo)\n",
    "\n",
    "            \n",
    "            \n",
    "        elif node.childs() == 1 :\n",
    "            \n",
    "            right, radius = fold.add('internalDecoder', v).split(2)\n",
    "            label = fold.add('nodeClassifier', v)\n",
    "            nodoSiguiente = node.right\n",
    "            if nodoSiguiente is not None:\n",
    "                right_loss = decode_node(fold, right, nodoSiguiente)\n",
    "\n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)\n",
    "            lossAtributo = fold.add('calcularLossAtributo', node, radius)\n",
    "            loss = fold.add('vectorAdder', lossEstructura, lossAtributo)\n",
    "            multipl = node.level/node.total_level\n",
    "            return multipl*fold.add('vectorAdder', loss, right_loss)\n",
    "            \n",
    "            \n",
    "\n",
    "        elif node.childs() == 2 :\n",
    "            left, right, radius = fold.add('bifurcationDecoder', v).split(3)\n",
    "            \n",
    "            label = fold.add('nodeClassifier', v)            \n",
    "            \n",
    "            nodoSiguienteRight = node.right\n",
    "            nodoSiguienteLeft = node.left\n",
    "\n",
    "\n",
    "            if nodoSiguienteRight is not None:\n",
    "                right_loss = decode_node(fold, right, nodoSiguienteRight)\n",
    "             \n",
    "            if nodoSiguienteLeft is not None:\n",
    "                left_loss  = decode_node(fold, left, nodoSiguienteLeft)\n",
    "\n",
    "                \n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)\n",
    "            lossAtributo   = fold.add('calcularLossAtributo', node, radius)\n",
    "            loss = fold.add('vectorAdder', lossEstructura, lossAtributo)\n",
    "            loss2 = fold.add('vectorAdder', loss, right_loss)\n",
    "            \n",
    "            return fold.add('vectorAdder', loss2, left_loss)\n",
    "            \n",
    "\n",
    "    dec = decode_node (fold, v, root)\n",
    "    return dec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_testing_grass(v, root, max, decoder):\n",
    "    def decode_node(v, node, max, decoder):\n",
    "        cl = decoder.nodeClassifier(v)\n",
    "        _, label = torch.max(cl, 1)\n",
    "        label = label.data\n",
    "        \n",
    "        \n",
    "        if label == 0 and createNode.count <= max: ##output del classifier\n",
    "           \n",
    "            #lossEstructura = Grassdecoder.classifyLossEstimator(cl, node)\n",
    "            radio = decoder.featureDecoder(v)\n",
    "            #print(\"radius\", radio)\n",
    "            #lossAtrs = Grassdecoder.calcularLossAtributo( node, radio )\n",
    "           \n",
    "            return createNode(1,radio)\n",
    "\n",
    "        elif label == 1 and createNode.count <= max:\n",
    "       \n",
    "            right, radius = decoder.internalDecoder(v)\n",
    "            #print(\"radius\", radius)\n",
    "            \n",
    "            d = createNode(1, radius) \n",
    "            #print(\"d\", d.radius)\n",
    "             \n",
    "            if not node is None:\n",
    "                if not node.right is None:\n",
    "                    nodoSiguiente = node.right\n",
    "                else:\n",
    "                    nodoSiguiente = None\n",
    "            else:\n",
    "                nodoSiguiente = None\n",
    "            \n",
    "            d.right = decode_node(right, nodoSiguiente, max, decoder)\n",
    "            \n",
    "\n",
    "            return d\n",
    "       \n",
    "        elif label == 2 and createNode.count <= max:\n",
    "            left, right, radius = decoder.bifurcationDecoder(v)\n",
    "            #print(\"radius\", radius)\n",
    "            \n",
    "            d = createNode(1, radius )\n",
    "  \n",
    "            if not node is None: #el nodo existe, me fijo si tiene hijo der/izq\n",
    "                if not node.right is None:\n",
    "                    nodoSiguienteRight = node.right\n",
    "                else:\n",
    "                    nodoSiguienteRight = None\n",
    "                if not node.left is None:\n",
    "                    nodoSiguienteLeft = node.left\n",
    "                else:\n",
    "                    nodoSiguienteLeft = None\n",
    "            else: #el nodo no existe\n",
    "                nodoSiguienteRight = None\n",
    "                nodoSiguienteLeft = None\n",
    "            \n",
    "            d.right = decode_node(right, nodoSiguienteRight, max, decoder)\n",
    "            d.left = decode_node(left, nodoSiguienteLeft, max, decoder)\n",
    "            \n",
    "           \n",
    "            return d\n",
    "            \n",
    "    createNode.count = 0\n",
    "    dec = decode_node (v, root, max, decoder)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, encoder, decoder, optimizer\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            #print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            #print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            #'classifier_state_dict': classifier.state_dict(),\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                \n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, 'outputs/best_model.pth')\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5000\n",
    "learning_rate = 1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaufeldman\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\wandb\\run-20221116_174125-2m2lh35b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/paufeldman/autoencoder3/runs/2m2lh35b\" target=\"_blank\">ruby-spaceship-59</a></strong> to <a href=\"https://wandb.ai/paufeldman/autoencoder3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/paufeldman/autoencoder3/runs/2m2lh35b?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2672d3aacb0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "config = {\n",
    "  \"learning_rate\": learning_rate,\n",
    "  \"epochs\": epochs,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"dataset\": t_list,\n",
    "  \"number of trees\": len(data_loader)*batch_size\n",
    "}\n",
    "wandb.init(project=\"autoencoder3\", entity=\"paufeldman\", config = config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_structure_fold_(v, root):\n",
    "    \n",
    "    def decode_node(v, node):\n",
    "        cl = Grassdecoder.nodeClassifier(v)\n",
    "        _, label = torch.max(cl, 1)\n",
    "        label = label.data\n",
    "\n",
    "        \n",
    "        if node.childs() == 0 : ##output del classifier\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            radio = Grassdecoder.featureDecoder(v)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radio )\n",
    "            nd = createNode(1,radio, ce = lossEstructura,  mse = lossAtrs)\n",
    "            return nd\n",
    "\n",
    "        elif node.childs() == 1 :\n",
    "        \n",
    "            right, radius = Grassdecoder.internalDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radius )\n",
    "            nd = createNode(1, radius, cl_prob = lossAtrs , ce = lossEstructura, mse = lossAtrs) \n",
    "            \n",
    "            nodoSiguiente = node.right\n",
    "           \n",
    "            if nodoSiguiente is not None:\n",
    "                nd.right = decode_node(right, nodoSiguiente)\n",
    "               \n",
    "            return nd\n",
    "\n",
    "        elif node.childs() == 2 :\n",
    "            left, right, radius = Grassdecoder.bifurcationDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radius )\n",
    "            nd = createNode(1, radius, cl_prob = lossAtrs, ce = lossEstructura, mse = lossAtrs)\n",
    "            \n",
    "            nodoSiguienteRight = node.right\n",
    "            nodoSiguienteLeft = node.left\n",
    "\n",
    "            \n",
    "            if nodoSiguienteRight is not None:\n",
    "                nd.right = decode_node(right, nodoSiguienteRight)\n",
    "             \n",
    "            if nodoSiguienteLeft is not None:\n",
    "                nd.left  = decode_node(left, nodoSiguienteLeft)\n",
    "            \n",
    "            return nd\n",
    "            \n",
    "    createNode.count = 0\n",
    "    dec = decode_node (v, root)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder con batch - decoder con batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Node' object has no attribute 'level'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 39\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         t_l\u001b[39m.\u001b[39mappend(t)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mfor\u001b[39;00m example, fnode \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch, t_l): \u001b[39m#example es el arbol y fnode el encodeado\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39m#print(\"example\", example)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39m#print(\"fnode\", fnode) \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39m#root_code, kl_div = torch.chunk(fnode, 2, 0)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     dec_fold_nodes\u001b[39m.\u001b[39mappend(decode_structure_fold_grass(dec_fold, fnode, example))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# Apply the computations on the decoder model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m total_loss \u001b[39m=\u001b[39m dec_fold\u001b[39m.\u001b[39mapply(Grassdecoder, [dec_fold_nodes])\u001b[39m#[0]\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 39\u001b[0m in \u001b[0;36mdecode_structure_fold_grass\u001b[1;34m(fold, v, root)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         loss2 \u001b[39m=\u001b[39m fold\u001b[39m.\u001b[39madd(\u001b[39m'\u001b[39m\u001b[39mvectorAdder\u001b[39m\u001b[39m'\u001b[39m, loss, right_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m fold\u001b[39m.\u001b[39madd(\u001b[39m'\u001b[39m\u001b[39mvectorAdder\u001b[39m\u001b[39m'\u001b[39m, loss2, left_loss)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m dec \u001b[39m=\u001b[39m decode_node (fold, v, root)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dec\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 39\u001b[0m in \u001b[0;36mdecode_structure_fold_grass.<locals>.decode_node\u001b[1;34m(fold, v, node)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     lossAtributo \u001b[39m=\u001b[39m fold\u001b[39m.\u001b[39madd(\u001b[39m'\u001b[39m\u001b[39mcalcularLossAtributo\u001b[39m\u001b[39m'\u001b[39m, node, radius)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     loss \u001b[39m=\u001b[39m fold\u001b[39m.\u001b[39madd(\u001b[39m'\u001b[39m\u001b[39mvectorAdder\u001b[39m\u001b[39m'\u001b[39m, lossEstructura, lossAtributo)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     multipl \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mlevel\u001b[39m/\u001b[39mnode\u001b[39m.\u001b[39mtotal_level\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m multipl\u001b[39m*\u001b[39mfold\u001b[39m.\u001b[39madd(\u001b[39m'\u001b[39m\u001b[39mvectorAdder\u001b[39m\u001b[39m'\u001b[39m, loss, right_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X53sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39melif\u001b[39;00m node\u001b[39m.\u001b[39mchilds() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m :\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Node' object has no attribute 'level'"
     ]
    }
   ],
   "source": [
    "\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "#opt = torch.optim.Adam(params, lr=learning_rate, weight_decay=0.0001) \n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[100], gamma=0.2)\n",
    "    \n",
    "\n",
    "    \n",
    "train_loss_avg = []\n",
    "\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        enc_fold = torch_f.Fold(device)\n",
    "        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "        # Collect computation nodes recursively from encoding process\n",
    "        n_nodes = []\n",
    "        for example in batch: #example es un arbolito\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            encode_structure_fold(enc_fold, example)\n",
    "            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "       \n",
    "        # Apply the computations on the encoder model\n",
    "       \n",
    "        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "        \n",
    "        \n",
    "        # Initialize torchfold for *decoding*\n",
    "        dec_fold = torch_f.Fold(device)\n",
    "        # Collect computation nodes recursively from decoding process\n",
    "        dec_fold_nodes = []\n",
    "        kld_fold_nodes = []\n",
    "\n",
    "        t_l = []\n",
    "        for f in enc_fold_nodes:\n",
    "            for t in f:\n",
    "                t_l.append(t)\n",
    "        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\n",
    "            #print(\"example\", example)\n",
    "            #print(\"fnode\", fnode) \n",
    "            #root_code, kl_div = torch.chunk(fnode, 2, 0)\n",
    "            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\n",
    "        # Apply the computations on the decoder model\n",
    "\n",
    "                       \n",
    "        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        #print(\"n\", n_nodes)\n",
    "        total_loss = torch.div(total_loss[0], n_nodes)\n",
    "        #print(\"div\", total_loss)\n",
    "        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder con batch - decoder sin batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        # Initialize torchfold for *encoding*\\n\\n        \\n        enc_fold = torch_f.Fold(device)\\n        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\\n        # Collect computation nodes recursively from encoding process\\n        n_nodes = []\\n        for example in batch: #example es un arbolito\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            encode_structure_fold(enc_fold, example)\\n            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\\n       \\n        # Apply the computations on the encoder model\\n       \\n        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\\n        encodeado_con_batch = enc_fold_nodes\\n        \\n        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\\n        #print(\"decoded\", decoded)\\n        l = []\\n        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\\n        l = []\\n        ce_loss_list = decoded.traverseInorderCE(decoded, l)\\n            \\n        mse_loss = sum(mse_loss_list) \\n        ce_loss  = sum(ce_loss_list)  \\n        total_loss = (0.5*ce_loss + mse_loss)\\n        #print(\"total_loss\", total_loss)\\n        total_loss = total_loss / len(mse_loss_list)\\n        \\n        \\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        enc_fold = torch_f.Fold(device)\n",
    "        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "        # Collect computation nodes recursively from encoding process\n",
    "        n_nodes = []\n",
    "        for example in batch: #example es un arbolito\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            encode_structure_fold(enc_fold, example)\n",
    "            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "       \n",
    "        # Apply the computations on the encoder model\n",
    "       \n",
    "        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "        encodeado_con_batch = enc_fold_nodes\n",
    "        \n",
    "        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\n",
    "        #print(\"decoded\", decoded)\n",
    "        l = []\n",
    "        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\n",
    "        l = []\n",
    "        ce_loss_list = decoded.traverseInorderCE(decoded, l)\n",
    "            \n",
    "        mse_loss = sum(mse_loss_list) \n",
    "        ce_loss  = sum(ce_loss_list)  \n",
    "        total_loss = (0.5*ce_loss + mse_loss)\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        total_loss = total_loss / len(mse_loss_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder sin batch - decoder con batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        # Initialize torchfold for *encoding*\\n\\n        \\n        \\n        enc_fold_nodes = []\\n        n_nodes = []\\n        for example in batch:\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            enc_fold = encode_structure(example).to(device)\\n        #print(\"encodeado sin batch\", enc_fold)\\n        enc_fold_nodes.append(enc_fold)\\n        encodeado_sin_batch = enc_fold\\n        # Split into a list of fold nodes per example\\n        #enc_fold_nodes = torch.split(enc_fold_nodes[0], 1, 0) #divide ele ncodeado en vectores de un elemento\\n        \\n        \\n        # Initialize torchfold for *decoding*\\n        dec_fold = torch_f.Fold(device)\\n        # Collect computation nodes recursively from decoding process\\n        dec_fold_nodes = []\\n        kld_fold_nodes = []\\n\\n        t_l = []\\n        for f in enc_fold_nodes:\\n            for t in f:\\n                t_l.append(t)\\n        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\\n            #print(\"example\", example)\\n            #print(\"fnode\", fnode) \\n            #root_code, kl_div = torch.chunk(fnode, 2, 0)\\n            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\\n        # Apply the computations on the decoder model\\n        #print(\"dec fold nodes\", dec_fold_nodes)\\n           \\n                       \\n        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\\n        #print(\"total_loss\", total_loss)\\n        n_nodes = torch.tensor(n_nodes, device = device)\\n        #print(\"n\", n_nodes)\\n        total_loss = torch.div(total_loss[0], n_nodes)\\n        #print(\"div\", total_loss)\\n        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\\n        #print(\"total_loss\", total_loss)\\n        \\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        mse_loss_avg[-1] += (mse_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        \n",
    "        enc_fold_nodes = []\n",
    "        n_nodes = []\n",
    "        for example in batch:\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            enc_fold = encode_structure(example).to(device)\n",
    "        #print(\"encodeado sin batch\", enc_fold)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "        encodeado_sin_batch = enc_fold\n",
    "        # Split into a list of fold nodes per example\n",
    "        #enc_fold_nodes = torch.split(enc_fold_nodes[0], 1, 0) #divide ele ncodeado en vectores de un elemento\n",
    "        \n",
    "        \n",
    "        # Initialize torchfold for *decoding*\n",
    "        dec_fold = torch_f.Fold(device)\n",
    "        # Collect computation nodes recursively from decoding process\n",
    "        dec_fold_nodes = []\n",
    "        kld_fold_nodes = []\n",
    "\n",
    "        t_l = []\n",
    "        for f in enc_fold_nodes:\n",
    "            for t in f:\n",
    "                t_l.append(t)\n",
    "        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\n",
    "            #print(\"example\", example)\n",
    "            #print(\"fnode\", fnode) \n",
    "            #root_code, kl_div = torch.chunk(fnode, 2, 0)\n",
    "            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\n",
    "        # Apply the computations on the decoder model\n",
    "        #print(\"dec fold nodes\", dec_fold_nodes)\n",
    "           \n",
    "                       \n",
    "        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        #print(\"n\", n_nodes)\n",
    "        total_loss = torch.div(total_loss[0], n_nodes)\n",
    "        #print(\"div\", total_loss)\n",
    "        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        mse_loss_avg[-1] += (mse_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder sin batch - decoder sin batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 5000] average reconstruction error: 0.054370 \n",
      "Epoch [11 / 5000] average reconstruction error: 0.051708 \n",
      "Epoch [21 / 5000] average reconstruction error: 0.046687 \n",
      "Epoch [31 / 5000] average reconstruction error: 0.037939 \n",
      "Epoch [41 / 5000] average reconstruction error: 0.026855 \n",
      "Epoch [51 / 5000] average reconstruction error: 0.024507 \n",
      "Epoch [61 / 5000] average reconstruction error: 0.023644 \n",
      "Epoch [71 / 5000] average reconstruction error: 0.023039 \n",
      "Epoch [81 / 5000] average reconstruction error: 0.022726 \n",
      "Epoch [91 / 5000] average reconstruction error: 0.022414 \n",
      "Epoch [101 / 5000] average reconstruction error: 0.022103 \n",
      "Epoch [111 / 5000] average reconstruction error: 0.021782 \n",
      "Epoch [121 / 5000] average reconstruction error: 0.021408 \n",
      "Epoch [131 / 5000] average reconstruction error: 0.020862 \n",
      "Epoch [141 / 5000] average reconstruction error: 0.019578 \n",
      "Epoch [151 / 5000] average reconstruction error: 0.016745 \n",
      "Epoch [161 / 5000] average reconstruction error: 0.015780 \n",
      "Epoch [171 / 5000] average reconstruction error: 0.010846 \n",
      "Epoch [181 / 5000] average reconstruction error: 0.006516 \n",
      "Epoch [191 / 5000] average reconstruction error: 0.004406 \n",
      "Epoch [201 / 5000] average reconstruction error: 0.009886 \n",
      "Epoch [211 / 5000] average reconstruction error: 0.011776 \n",
      "Epoch [221 / 5000] average reconstruction error: 0.008867 \n",
      "Epoch [231 / 5000] average reconstruction error: 0.004686 \n",
      "Epoch [241 / 5000] average reconstruction error: 0.003498 \n",
      "Epoch [251 / 5000] average reconstruction error: 0.018751 \n",
      "Epoch [261 / 5000] average reconstruction error: 0.017804 \n",
      "Epoch [271 / 5000] average reconstruction error: 0.017228 \n",
      "Epoch [281 / 5000] average reconstruction error: 0.015148 \n",
      "Epoch [291 / 5000] average reconstruction error: 0.012463 \n",
      "Epoch [301 / 5000] average reconstruction error: 0.010258 \n",
      "Epoch [311 / 5000] average reconstruction error: 0.007302 \n",
      "Epoch [321 / 5000] average reconstruction error: 0.004926 \n",
      "Epoch [331 / 5000] average reconstruction error: 0.003563 \n",
      "Epoch [341 / 5000] average reconstruction error: 0.002802 \n",
      "Epoch [351 / 5000] average reconstruction error: 0.002373 \n",
      "Epoch [361 / 5000] average reconstruction error: 0.002044 \n",
      "Epoch [371 / 5000] average reconstruction error: 0.001801 \n",
      "Epoch [381 / 5000] average reconstruction error: 0.001614 \n",
      "Epoch [391 / 5000] average reconstruction error: 0.001813 \n",
      "Epoch [401 / 5000] average reconstruction error: 0.001325 \n",
      "Epoch [411 / 5000] average reconstruction error: 0.020248 \n",
      "Epoch [421 / 5000] average reconstruction error: 0.019616 \n",
      "Epoch [431 / 5000] average reconstruction error: 0.019319 \n",
      "Epoch [441 / 5000] average reconstruction error: 0.016646 \n",
      "Epoch [451 / 5000] average reconstruction error: 0.015468 \n",
      "Epoch [461 / 5000] average reconstruction error: 0.015124 \n",
      "Epoch [471 / 5000] average reconstruction error: 0.014615 \n",
      "Epoch [481 / 5000] average reconstruction error: 0.013924 \n",
      "Epoch [491 / 5000] average reconstruction error: 0.013061 \n",
      "Epoch [501 / 5000] average reconstruction error: 0.011944 \n",
      "Epoch [511 / 5000] average reconstruction error: 0.010459 \n",
      "Epoch [521 / 5000] average reconstruction error: 0.008531 \n",
      "Epoch [531 / 5000] average reconstruction error: 0.006375 \n",
      "Epoch [541 / 5000] average reconstruction error: 0.004620 \n",
      "Epoch [551 / 5000] average reconstruction error: 0.003530 \n",
      "Epoch [561 / 5000] average reconstruction error: 0.002820 \n",
      "Epoch [571 / 5000] average reconstruction error: 0.002290 \n",
      "Epoch [581 / 5000] average reconstruction error: 0.001933 \n",
      "Epoch [591 / 5000] average reconstruction error: 0.001690 \n",
      "Epoch [601 / 5000] average reconstruction error: 0.001489 \n",
      "Epoch [611 / 5000] average reconstruction error: 0.001326 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Documents\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 42\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     n \u001b[39m=\u001b[39m example\u001b[39m.\u001b[39mcount_nodes(example, c)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     n_nodes\u001b[39m.\u001b[39mappend(\u001b[39mlen\u001b[39m(n))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     enc_fold \u001b[39m=\u001b[39m encode_structure(example)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m#print(\"encodeado sin batch\", enc_fold)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m enc_fold_nodes\u001b[39m.\u001b[39mappend(enc_fold)\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 42\u001b[0m in \u001b[0;36mencode_structure\u001b[1;34m(root)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m Grassencoder\u001b[39m.\u001b[39minternalEncoder(node\u001b[39m.\u001b[39mradius\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m), right)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m encoding \u001b[39m=\u001b[39m encode_node(root)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m \u001b[39mreturn\u001b[39;00m encoding\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 42\u001b[0m in \u001b[0;36mencode_structure.<locals>.encode_node\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m     left \u001b[39m=\u001b[39m encode_node(node\u001b[39m.\u001b[39mleft)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m     right \u001b[39m=\u001b[39m encode_node(node\u001b[39m.\u001b[39;49mright)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m     \u001b[39mif\u001b[39;00m left \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m Grassencoder\u001b[39m.\u001b[39mbifurcationEncoder(node\u001b[39m.\u001b[39mradius\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m), right, left)\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 42\u001b[0m in \u001b[0;36mencode_structure.<locals>.encode_node\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m     left \u001b[39m=\u001b[39m encode_node(node\u001b[39m.\u001b[39mleft)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m     right \u001b[39m=\u001b[39m encode_node(node\u001b[39m.\u001b[39;49mright)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m     \u001b[39mif\u001b[39;00m left \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m Grassencoder\u001b[39m.\u001b[39mbifurcationEncoder(node\u001b[39m.\u001b[39mradius\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m), right, left)\n",
      "    \u001b[1;31m[... skipping similar frames: encode_structure.<locals>.encode_node at line 140 (5 times)]\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 42\u001b[0m in \u001b[0;36mencode_structure.<locals>.encode_node\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m     left \u001b[39m=\u001b[39m encode_node(node\u001b[39m.\u001b[39mleft)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m     right \u001b[39m=\u001b[39m encode_node(node\u001b[39m.\u001b[39;49mright)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m     \u001b[39mif\u001b[39;00m left \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m Grassencoder\u001b[39m.\u001b[39mbifurcationEncoder(node\u001b[39m.\u001b[39mradius\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m), right, left)\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 42\u001b[0m in \u001b[0;36mencode_structure.<locals>.encode_node\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Grassencoder\u001b[39m.\u001b[39mbifurcationEncoder(node\u001b[39m.\u001b[39mradius\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m), right, left)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Grassencoder\u001b[39m.\u001b[39;49minternalEncoder(node\u001b[39m.\u001b[39;49mradius\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m4\u001b[39;49m), right)\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 42\u001b[0m in \u001b[0;36mGRASSEncoder.internalEncoder\u001b[1;34m(self, node, right, left)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minternalEncoder\u001b[39m(\u001b[39mself\u001b[39m, node, right, left \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minternal_encoder(node, right, left)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py_torc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 42\u001b[0m in \u001b[0;36mInternalEncoder.forward\u001b[1;34m(self, input, right_input, left_input)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#print(\"attributes\", attributes)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Encodeo el derecho\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mif\u001b[39;00m right_input \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39m#print(\"right input\", right_input)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mright_lin_encoder_1(right_input)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39m#print(\"context derecho\", context)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m# Encodeo el izquierdo\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m#print(\"left input\", left_input)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Intra/autoencoder/autoencoder_fold.ipynb#X56sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mif\u001b[39;00m left_input \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py_torc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py_torc\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "    ce_avg.append(0)\n",
    "    mse_avg.append(0)\n",
    "\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "        enc_fold_nodes = []\n",
    "        n_nodes = []\n",
    "        for example in batch:\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            enc_fold = encode_structure(example).to(device)\n",
    "        #print(\"encodeado sin batch\", enc_fold)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "        encodeado_sin_batch = enc_fold\n",
    "        \n",
    "        \n",
    "        \n",
    "        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\n",
    "        #print(\"decoded\", decoded)\n",
    "        l = []\n",
    "        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\n",
    "        l = []\n",
    "        ce_loss_list = decoded.traverseInorderCE(decoded, l)\n",
    "            \n",
    "        mse_loss = sum(mse_loss_list) \n",
    "        ce_loss  = sum(ce_loss_list)  \n",
    "       \n",
    "        ce = [0.4*a for a in ce_loss_list]\n",
    "\n",
    "\n",
    "        total_loss = (0.4*ce_loss + mse_loss)\n",
    "        total_loss = total_loss / len(mse_loss_list)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        mse_avg[-1] += (mse_loss.item())\n",
    "        ce_avg[-1] += (ce_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss, 'mse loss': mse_loss, 'ce loss': ce_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 638\n"
     ]
    }
   ],
   "source": [
    "encoder = GRASSEncoder(input_size = 4, feature_size=128, hidden_size=256).to(device)\n",
    "decoder = GRASSDecoder(latent_size=128, hidden_size=256, mult = mult).to(device)\n",
    "\n",
    "checkpoint = torch.load(\"outputs/best_model.pth\")\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "print(\"epoch\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[0.4001, 0.4003, 0.4002, 0.4003]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "1 tensor([[0.9999, 1.0000, 0.9999, 1.0000]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "2 tensor([[ 1.1957e-04, -1.0518e-04,  1.1977e-04, -4.2841e-05]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "3 tensor([[0.8002, 0.8002, 0.8002, 0.8002]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "4 tensor([[0.2002, 0.2003, 0.2004, 0.2003]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "5 tensor([[0.6002, 0.6003, 0.6002, 0.6002]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = iter(data_loader).next()[0]\n",
    "enc_fold = torch_f.Fold(device)\n",
    "enc_fold_nodes = []\n",
    "enc_fold_nodes.append(encode_structure_fold(enc_fold, input))\n",
    "enc_fold_nodes = enc_fold.apply(encoder, [enc_fold_nodes])\n",
    "encoded = enc_fold_nodes[0]\n",
    "decoded = decode_testing_grass(encoded, input, 100, decoder)\n",
    "\n",
    "count = []\n",
    "numerar_nodos(decoded, count)\n",
    "decoded.traverseInorder(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6524c45caa4159baa8d59f101bd290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.5, 0.5,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTree(input, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f422e7539764a0bb250d532b3b26d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.5000062…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTree(decoded, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b812b61a364bd599d89407fc316e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.4894804…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTree(decoded, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotTree(decoded, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "n_nodes = input.count_nodes(input,c)\n",
    "len(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "n_nodes = decoded.count_nodes(decoded,c)\n",
    "len(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 56 2\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "decoded.traverseInorderChilds(decoded, li)\n",
    "zero = [a for a in li if a == 0]\n",
    "one = [a for a in li if a == 1]\n",
    "two = [a for a in li if a == 2]\n",
    "qzero = len(zero)\n",
    "qOne = len(one)\n",
    "qtwo = len(two)\n",
    "print(qzero, qOne, qtwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 56 2\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "input.traverseInorderChilds(input, li)\n",
    "zero = [a for a in li if a == 0]\n",
    "one = [a for a in li if a == 1]\n",
    "two = [a for a in li if a == 2]\n",
    "qzero = len(zero)\n",
    "qOne = len(one)\n",
    "qtwo = len(two)\n",
    "print(qzero, qOne, qtwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (20,10))\n",
    "plt.plot(train_loss_avg) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverseleaf(root):\n",
    "    if root is not None:\n",
    "        traverseleaf(root.left)\n",
    "        if root.is_leaf():\n",
    "            print(root.radius)\n",
    "        traverseleaf(root.right)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traversebif(root):\n",
    "    if root is not None:\n",
    "        traversebif(root.left)\n",
    "        if root.is_two_child():\n",
    "            print(root.radius)\n",
    "        traversebif(root.right)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1027, 0.6636, 0.3965, 1.0007]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[7.8643e-02, 7.4366e-01, 2.9832e-01, 9.0882e-05]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "////\n",
      "tensor([0.0969, 0.6578, 0.3956, 1.0000], device='cuda:0')\n",
      "tensor([0.0787, 0.7437, 0.2985, 0.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "traversebif(decoded)\n",
    "print(\"////\")\n",
    "traversebif(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9788, 0.9875, 0.3199, 0.6484]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2574, 0.0027, 0.9734, 0.5381]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8007, 0.9801, 0.0298, 0.3686]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "traverseleaf(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.9987, 0.3278, 0.6864], device='cuda:0')\n",
      "tensor([0.2583, 0.0000, 1.0000, 0.5396], device='cuda:0')\n",
      "tensor([0.8033, 1.0000, 0.0000, 0.4182], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "traverseleaf(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py_torc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f3e717cd274da89498094fde320e6eab1bf0f52911d27cf47473187acb3fe8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
