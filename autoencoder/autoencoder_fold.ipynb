{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import raiseExceptions\n",
    "from tokenize import Double\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from vec3 import Vec3\n",
    "import meshplot as mp\n",
    "import torch\n",
    "torch.manual_seed(125)\n",
    "import random\n",
    "random.seed(125)\n",
    "import torch_f as torch_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_fn(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        wrapper.count += 1\n",
    "        return f(*args, **kwargs)\n",
    "    wrapper.count = 0\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase nodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Class Node\n",
    "    \"\"\"\n",
    "    def __init__(self, value, radius, left = None, right = None, position = None, cl_prob= None, ce = None, mse = None, level = None, treelevel = None):\n",
    "        self.left = left\n",
    "        self.data = value\n",
    "        self.radius = radius\n",
    "        self.position = position\n",
    "        self.right = right\n",
    "        self.prob = cl_prob\n",
    "        self.mse = mse\n",
    "        self.ce = ce\n",
    "        self.children = [self.left, self.right]\n",
    "        self.level = level\n",
    "        self.treelevel = treelevel\n",
    "    \n",
    "    def agregarHijo(self, children):\n",
    "\n",
    "        if self.right is None:\n",
    "            self.right = children\n",
    "        elif self.left is None:\n",
    "            self.left = children\n",
    "\n",
    "        else:\n",
    "            raise ValueError (\"solo arbol binario \")\n",
    "\n",
    "\n",
    "    def is_leaf(self):\n",
    "        if self.right is None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_two_child(self):\n",
    "        if self.right is not None and self.left is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_one_child(self):\n",
    "        if self.is_two_child():\n",
    "            return False\n",
    "        elif self.is_leaf():\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def childs(self):\n",
    "        if self.is_leaf():\n",
    "            return 0\n",
    "        if self.is_one_child():\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    \n",
    "    \n",
    "    def traverseInorder(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorder(root.left)\n",
    "            print (root.data, root.radius)\n",
    "            self.traverseInorder(root.right)\n",
    "\n",
    "    def traverseInorderwl(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderwl(root.left)\n",
    "            print (root.data, root.radius, root.level, root.treelevel)\n",
    "            self.traverseInorderwl(root.right)\n",
    "\n",
    "    def get_tree_level(self, root, c):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.get_tree_level(root.left, c)\n",
    "            c.append(root.level)\n",
    "            self.get_tree_level(root.right, c)\n",
    "\n",
    "    def set_tree_level(self, root, c):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.set_tree_level(root.left, c)\n",
    "            root.treelevel = c\n",
    "            self.set_tree_level(root.right, c)\n",
    "\n",
    "    def traverseInorderLoss(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderLoss(root.left, loss)\n",
    "            loss.append(root.prob)\n",
    "            self.traverseInorderLoss(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderMSE(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderMSE(root.left, loss)\n",
    "            loss.append(root.mse)\n",
    "            self.traverseInorderMSE(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderCE(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderCE(root.left, loss)\n",
    "            loss.append(root.ce)\n",
    "            self.traverseInorderCE(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderChilds(self, root, l):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderChilds(root.left, l)\n",
    "            l.append(root.childs())\n",
    "            self.traverseInorderChilds(root.right, l)\n",
    "            return l\n",
    "\n",
    "    def preorder(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            print (root.data, root.radius)\n",
    "            self.preorder(root.left)\n",
    "            self.preorder(root.right)\n",
    "\n",
    "    def cloneBinaryTree(self, root):\n",
    "     \n",
    "        # base case\n",
    "        if root is None:\n",
    "            return None\n",
    "    \n",
    "        # create a new node with the same data as the root node\n",
    "        root_copy = Node(root.data, root.radius)\n",
    "    \n",
    "        # clone the left and right subtree\n",
    "        root_copy.left = self.cloneBinaryTree(root.left)\n",
    "        root_copy.right = self.cloneBinaryTree(root.right)\n",
    "    \n",
    "        # return cloned root node\n",
    "        return root_copy\n",
    "\n",
    "    def height(self, root):\n",
    "    # Check if the binary tree is empty\n",
    "        if root is None:\n",
    "            return 0 \n",
    "        # Recursively call height of each node\n",
    "        leftAns = self.height(root.left)\n",
    "        rightAns = self.height(root.right)\n",
    "    \n",
    "        # Return max(leftHeight, rightHeight) at each iteration\n",
    "        return max(leftAns, rightAns) + 1\n",
    "\n",
    "    # Print nodes at a current level\n",
    "    def printCurrentLevel(self, root, level):\n",
    "        if root is None:\n",
    "            return\n",
    "        if level == 1:\n",
    "            print(root.data, end=\" \")\n",
    "        elif level > 1:\n",
    "            self.printCurrentLevel(root.left, level-1)\n",
    "            self.printCurrentLevel(root.right, level-1)\n",
    "\n",
    "    def printLevelOrder(self, root):\n",
    "        h = self.height(root)\n",
    "        for i in range(1, h+1):\n",
    "            self.printCurrentLevel(root, i)\n",
    "\n",
    "\n",
    "    \n",
    "    def count_nodes(self, root, counter):\n",
    "        if   root is not None:\n",
    "            self.count_nodes(root.left, counter)\n",
    "            counter.append(root.data)\n",
    "            self.count_nodes(root.right, counter)\n",
    "            return counter\n",
    "\n",
    "    \n",
    "    def serialize(self, root):\n",
    "        def post_order(root):\n",
    "            if root:\n",
    "                post_order(root.left)\n",
    "                post_order(root.right)\n",
    "                ret[0] += str(root.data)+'_'+ str(root.radius) +';'\n",
    "                \n",
    "            else:\n",
    "                ret[0] += '#;'           \n",
    "\n",
    "        ret = ['']\n",
    "        post_order(root)\n",
    "        return ret[0][:-1]  # remove last ,\n",
    "\n",
    "    def toGraph( self, graph, index, dec, proc=True):\n",
    "        \n",
    "        \n",
    "        radius = self.radius.cpu().detach().numpy()\n",
    "        if dec:\n",
    "            radius= radius[0]\n",
    "        #print(\"posicion\", self.data, radius)\n",
    "        #print(\"right\", self.right)\n",
    "        \n",
    "        #graph.add_nodes_from( [ (index, {'posicion': radius[0:3], 'radio': radius[3] } ) ])\n",
    "        graph.add_nodes_from( [ (self.data, {'posicion': radius[0:3], 'radio': radius[3] } ) ])\n",
    "        \n",
    "\n",
    "        if self.right is not None:\n",
    "            #leftIndex = self.right.toGraph( graph, index + 1, dec)#\n",
    "            self.right.toGraph( graph, index + 1, dec)#\n",
    "            \n",
    "            #graph.add_edge( index, index + 1 )\n",
    "            graph.add_edge( self.data, self.right.data )\n",
    "            #if proc:\n",
    "            #    nx.set_edge_attributes( graph, {(index, index+1) : {'procesada':False}})\n",
    "        \n",
    "        if self.left is not None:\n",
    "            #retIndex = self.left.toGraph( graph, leftIndex, dec )#\n",
    "            self.left.toGraph( graph, 0, dec )#\n",
    "\n",
    "            #graph.add_edge( index, leftIndex)\n",
    "            graph.add_edge( self.data, self.left.data)\n",
    "            #if proc:\n",
    "            #    nx.set_edge_attributes( graph, {(index, leftIndex) : {'procesada':False}})\n",
    "\n",
    "        else:\n",
    "            #return index + 1\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTree( root, dec ):\n",
    "    graph = nx.Graph()\n",
    "    root.toGraph( graph, 0, dec)\n",
    "    edges=nx.get_edge_attributes(graph,'procesada')\n",
    "\n",
    "    p = mp.plot( np.array([ graph.nodes[v]['posicion'] for v in graph.nodes]), shading={'point_size':0.1}, return_plot=True)\n",
    "\n",
    "    for arista in graph.edges:\n",
    "        p.add_lines( graph.nodes[arista[0]]['posicion'], graph.nodes[arista[1]]['posicion'])\n",
    "\n",
    "    return \n",
    "\n",
    "def traverse(root, tree):\n",
    "       \n",
    "        if root is not None:\n",
    "            traverse(root.left, tree)\n",
    "            tree.append((root.radius, root.data))\n",
    "            traverse(root.right, tree)\n",
    "            return tree\n",
    "\n",
    "def traverse_2(tree1, tree2, t_l):\n",
    "       \n",
    "        if tree1 is not None:\n",
    "            traverse_2(tree1.left, tree2.left, t_l)\n",
    "            if tree2:\n",
    "                t_l.append((tree1.radius, tree2.radius))\n",
    "                print((tree1.radius, tree2.radius))\n",
    "            else:\n",
    "                t_l.append(tree1.radius)\n",
    "                print((tree1.radius))\n",
    "            traverse_2(tree1.right, tree2, t_l)\n",
    "            return t_l\n",
    "            \n",
    "\n",
    "def traverse_conexiones(root, tree):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            traverse_conexiones(root.left, tree)\n",
    "            if root.right is not None:\n",
    "                tree.append((root.data, root.right.data))\n",
    "            if root.left is not None:\n",
    "                tree.append((root.data, root.left.data))\n",
    "            traverse_conexiones(root.right, tree)\n",
    "            return tree\n",
    "\n",
    "def arbolAGrafo (nodoRaiz):\n",
    "    \n",
    "    conexiones = []\n",
    "    lineas = traverse_conexiones(nodoRaiz, conexiones)\n",
    "    tree = []\n",
    "    tree = traverse(nodoRaiz, tree)\n",
    "\n",
    "    vertices = []\n",
    "    verticesCrudos = []\n",
    "    for node in tree:\n",
    "        vertice = node[0][0][:3]\n",
    "        rad = node[0][0][-1]\n",
    "        num = node[1]\n",
    "        \n",
    "        #vertices.append((num, {'posicion': Vec3( vertice[0], vertice[1], vertice[2]), 'radio': rad} ))\n",
    "        vertices.append((len(verticesCrudos),{'posicion': Vec3( vertice[0], vertice[1], vertice[2]), 'radio': rad}))\n",
    "        verticesCrudos.append(vertice)\n",
    "\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from( vertices )\n",
    "    G.add_edges_from( lineas )\n",
    "    \n",
    "    return G\n",
    "\n",
    "@count_fn\n",
    "def createNode(data, radius, position = None, left = None, right = None, cl_prob = None, ce = None, mse=None):\n",
    "        \"\"\"\n",
    "        Utility function to create a node.\n",
    "        \"\"\"\n",
    "        return Node(data, radius, position, left, right, cl_prob, ce, mse)\n",
    " \n",
    "def deserialize(data):\n",
    "    if  not data:\n",
    "        return \n",
    "    nodes = data.split(';')  \n",
    "    #print(\"node\",nodes[3])\n",
    "    def post_order(nodes):\n",
    "                \n",
    "        if nodes[-1] == '#':\n",
    "            nodes.pop()\n",
    "            return None\n",
    "        node = nodes.pop().split('_')\n",
    "        data = int(node[0])\n",
    "        #radius = float(node[1])\n",
    "        #print(\"node\", node)\n",
    "        #breakpoint()\n",
    "        radius = node[1]\n",
    "        #print(\"radius\", radius)\n",
    "        rad = radius.split(\",\")\n",
    "        rad [0] = rad[0].replace('[','')\n",
    "        rad [3] = rad[3].replace(']','')\n",
    "        r = []\n",
    "        for value in rad:\n",
    "            r.append(float(value))\n",
    "        #r =[float(num) for num in radius if num.isdigit()]\n",
    "        r = torch.tensor(r, device=device)\n",
    "        #breakpoint()\n",
    "        root = createNode(data, r)\n",
    "        root.right = post_order(nodes)\n",
    "        root.left = post_order(nodes)\n",
    "        \n",
    "        return root    \n",
    "    return post_order(nodes)    \n",
    "\n",
    "\n",
    "def read_tree(filename):\n",
    "    with open('./trees/' +'prof3/' +filename, \"r\") as f:\n",
    "        byte = f.read() \n",
    "        return byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternalEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, feature_size: int, hidden_size: int):\n",
    "        super(InternalEncoder, self).__init__()\n",
    "\n",
    "        #print(\"init\")\n",
    "        # Encoders atributos\n",
    "        self.attribute_lin_encoder_1 = nn.Linear(input_size,hidden_size)\n",
    "        self.attribute_lin_encoder_2 = nn.Linear(hidden_size,hidden_size*2)\n",
    "        self.attribute_lin_encoder_3 = nn.Linear(hidden_size*2,feature_size)\n",
    "\n",
    "        # Encoders derecho e izquierdo\n",
    "        self.right_lin_encoder_1 = nn.Linear(feature_size,hidden_size)\n",
    "        self.right_lin_encoder_2 = nn.Linear(hidden_size,feature_size)\n",
    "\n",
    "        self.left_lin_encoder_1  = nn.Linear(feature_size,hidden_size)\n",
    "        self.left_lin_encoder_2  = nn.Linear(hidden_size,feature_size)\n",
    "\n",
    "        # Encoder final\n",
    "        self.final_lin_encoder_1 = nn.Linear(2*feature_size, feature_size)\n",
    "\n",
    "        # Funciones / Parametros utiles\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.feature_size = feature_size\n",
    "        #print(\"fin\")\n",
    "\n",
    "    def forward(self, input, right_input, left_input):\n",
    "        # Encodeo los atributos\n",
    "        attributes = self.attribute_lin_encoder_1(input)\n",
    "        attributes = self.tanh(attributes)\n",
    "        attributes = self.attribute_lin_encoder_2(attributes)\n",
    "        attributes = self.tanh(attributes)\n",
    "        attributes = self.attribute_lin_encoder_3(attributes)\n",
    "        attributes = self.tanh(attributes)\n",
    "        #print(\"attributes\", attributes)\n",
    "\n",
    "        # Encodeo el derecho\n",
    "        if right_input is not None:\n",
    "            #print(\"right input\", right_input)\n",
    "            context = self.right_lin_encoder_1(right_input)\n",
    "            context = self.tanh(context)\n",
    "            context = self.right_lin_encoder_2(context)\n",
    "            #print(\"context derecho\", context)\n",
    "            # Encodeo el izquierdo\n",
    "            #print(\"left input\", left_input)\n",
    "            if left_input is not None:\n",
    "                left = self.left_lin_encoder_1(left_input)\n",
    "                #print(\"izquierdo\", left.shape)\n",
    "                left = self.tanh(left)\n",
    "                context += self.left_lin_encoder_2(left)\n",
    "                #print(\"context izquierdo\", context.shape)\n",
    "        else:\n",
    "            context = torch.zeros(input.shape[0],self.feature_size, requires_grad=True, device=device)\n",
    "        \n",
    "\n",
    "        context = self.tanh(context)\n",
    "        #print(\"context shape\", context.shape)\n",
    "        #print(\"attributes shape\", attributes.shape)\n",
    "        #if len(attributes.shape) == 1:\n",
    "            #print(\"len(attributes.shape)\",len(attributes.shape))\n",
    "            #attributes = attributes.reshape(1, 128)\n",
    "        #print(\"attributes shape\", attributes.shape)\n",
    "\n",
    "        feature = torch.cat((attributes,context), 1)\n",
    "        #print(\"feature cat\", feature.shape)\n",
    "\n",
    "        feature = self.final_lin_encoder_1(feature)\n",
    "        feature = self.tanh(feature)\n",
    "        #print(\"output\", feature)\n",
    "        return feature\n",
    "\n",
    "       \n",
    "    \n",
    "\n",
    "class GRASSEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, feature_size : int, hidden_size: int):\n",
    "        super(GRASSEncoder, self).__init__()\n",
    "        self.leaf_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        self.internal_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        self.bifurcation_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        \n",
    "    def leafEncoder(self, node, right=None, left = None):\n",
    "        return self.internal_encoder(node, right, left)\n",
    "    def internalEncoder(self, node, right, left = None):\n",
    "        return self.internal_encoder(node, right, left)\n",
    "    def bifurcationEncoder(self, node, right, left):\n",
    "        \n",
    "        return self.bifurcation_encoder(node, right, left)\n",
    "\n",
    "Grassencoder = GRASSEncoder(input_size = 4, feature_size=256, hidden_size=512)\n",
    "Grassencoder = Grassencoder.to(device)\n",
    "\n",
    "\n",
    "def encode_structure_fold(fold, root):\n",
    "    \n",
    "    \n",
    "    def encode_node(node):\n",
    "        \n",
    "        if node is None:\n",
    "            return\n",
    "        \n",
    "        if node.is_leaf():\n",
    "            return fold.add('leafEncoder', node.radius)\n",
    "        else:\n",
    "            left = encode_node(node.left)\n",
    "            right = encode_node(node.right)\n",
    "            if left is not None:\n",
    "             \n",
    "                return fold.add('bifurcationEncoder', node.radius, right, left)\n",
    "            else:\n",
    "                return fold.add('internalEncoder', node.radius, right)\n",
    "        \n",
    "\n",
    "    encoding = encode_node(root)\n",
    "    \n",
    "    return encoding\n",
    "  \n",
    "def encode_structure(root):\n",
    "    \n",
    "    def encode_node(node):\n",
    "          \n",
    "        if node is None:\n",
    "            return\n",
    "        if node.is_leaf():\n",
    "            return Grassencoder.leafEncoder(node.radius.reshape(-1,4))\n",
    "        else :\n",
    "            left = encode_node(node.left)\n",
    "            right = encode_node(node.right)\n",
    "            if left is not None:\n",
    "                return Grassencoder.bifurcationEncoder(node.radius.reshape(-1,4), right, left)\n",
    "            else:\n",
    "                return Grassencoder.internalEncoder(node.radius.reshape(-1,4), right)\n",
    "        \n",
    "\n",
    "    encoding = encode_node(root)\n",
    "   \n",
    "    return encoding\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerar_nodos(root, count):\n",
    "    if root is not None:\n",
    "        numerar_nodos(root.left, count)\n",
    "        root.data = len(count)\n",
    "        count.append(1)\n",
    "        numerar_nodos(root.right, count)\n",
    "        return \n",
    "\n",
    "\n",
    "def traversefeatures(root, features):\n",
    "       \n",
    "    if root is not None:\n",
    "        traversefeatures(root.left, features)\n",
    "        features.append(root.radius)\n",
    "        traversefeatures(root.right, features)\n",
    "        return features\n",
    "\n",
    "def norm(root, minx, miny, minz, minr, maxx, maxy, maxz, maxr):\n",
    "    \n",
    "    if root is not None:\n",
    "        mx = minx.clone().detach()\n",
    "        my = miny.clone().detach()\n",
    "        mz = minz.clone().detach()\n",
    "        mr = minr.clone().detach()\n",
    "        Mx = maxx.clone().detach()\n",
    "        My = maxy.clone().detach()\n",
    "        Mz = maxz.clone().detach()\n",
    "        Mr = maxr.clone().detach()\n",
    "       \n",
    "        root.radius[0] = (root.radius[0] - minx)/(maxx - minx)\n",
    "        root.radius[1] = (root.radius[1] - miny)/(maxy - miny)\n",
    "        root.radius[2] = (root.radius[2] - minz)/(maxz - minz)\n",
    "        root.radius[3] = (root.radius[3] - minr)/(maxr - minr)\n",
    "        \n",
    "        norm(root.left, mx, my, mz, mr, Mx, My, Mz, Mr)\n",
    "        norm(root.right, mx, my, mz, mr, Mx, My, Mz, Mr)\n",
    "        return \n",
    "\n",
    "def normalize_features(root):\n",
    "    features = []\n",
    "    features = traversefeatures(root, features)\n",
    "    \n",
    "    x = [tensor[0] for tensor in features]\n",
    "    y = [tensor[1] for tensor in features]\n",
    "    z = [tensor[2] for tensor in features]\n",
    "    r = [tensor[3] for tensor in features]\n",
    " \n",
    "    norm(root, min(x), min(y), min(z), min(r), max(x), max(y), max(z), max(r))\n",
    "\n",
    "    return \n",
    "\n",
    "def traversefeatures(root, features):\n",
    "       \n",
    "    if root is not None:\n",
    "        traversefeatures(root.left, features)\n",
    "        features.append(root.radius)\n",
    "        traversefeatures(root.right, features)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tree0.dat', 'tree1.dat', 'tree10.dat', 'tree11.dat', 'tree12.dat', 'tree13.dat', 'tree14.dat', 'tree15.dat', 'tree16.dat', 'tree17.dat']\n"
     ]
    }
   ],
   "source": [
    "def my_collate(batch):\n",
    "    return batch\n",
    "\n",
    "#t_list = ['ArteryObjAN1-7.dat','ArteryObjAN1-0.dat', 'ArteryObjAN1-17.dat',  'ArteryObjAN1-11.dat']\n",
    "\n",
    "#t_list = ['ArteryObjAN1-0.dat','ArteryObjAN1-7.dat', 'ArteryObjAN1-17.dat',  'ArteryObjAN1-11.dat', 'ArteryObjAN1-19.dat', 'ArteryObjAN2-4.dat', 'ArteryObjAN2-6.dat', \n",
    "#           'ArteryObjAN25-18.dat']\n",
    "#t_list = ['ArteryObjAN1-17-55.dat', 'ArteryObjAN1-17-22.dat', \"ArteryObjAN1-17-12.dat\", \"ArteryObjAN1-17-9.dat\",'ArteryObjAN1-17-42.dat', 'ArteryObjAN1-17-64.dat', \"ArteryObjAN1-17-70.dat\", \"ArteryObjAN1-17-1.dat\"]\n",
    "#t_list = ['ArteryObjAN1-17.dat']\n",
    "#t_list = ['ArteryObjAN1-11.dat']\n",
    "\n",
    "\n",
    "#t_list = ['test2.dat']\n",
    "\n",
    "#t_list = ['ArteryObjAN31-14.dat']\n",
    "#t_list = os.listdir(\"./trees\")[:20]\n",
    "t_list = os.listdir(\"./trees/prof3\")[:10]\n",
    "print(t_list)\n",
    "class tDataset(Dataset):\n",
    "    def __init__(self, dir, transform=None):\n",
    "        self.names = dir\n",
    "        self.transform = transform\n",
    "        self.data = [] #lista con las strings de todos los arboles\n",
    "        for file in self.names:\n",
    "            self.data.append(read_tree(file))\n",
    "        self.trees = []\n",
    "        for tree in self.data:\n",
    "            deserial = deserialize(tree)\n",
    "            normalize_features(deserial)\n",
    "            self.trees.append(deserial)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #file = self.names[idx]\n",
    "        #string = read_tree(file)\n",
    "        tree = self.trees[idx]\n",
    "        return tree\n",
    "\n",
    "batch_size = 10\n",
    "dataset = tDataset(t_list)\n",
    "data_loader = DataLoader(dataset, batch_size = batch_size, shuffle=True, collate_fn=my_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a8abdfa4f44631b1537c1612459bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.5, 0.5,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 tensor([0.4444, 0.2000, 0.2222, 0.5000], device='cuda:0')\n",
      "0 tensor([0., 0., 0., 0.], device='cuda:0')\n",
      "3 tensor([0.8889, 0.4000, 0.4444, 1.0000], device='cuda:0')\n",
      "1 tensor([0.5556, 0.8000, 0.7778, 0.5000], device='cuda:0')\n",
      "2 tensor([1., 1., 1., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input = iter(data_loader).next()[0]\n",
    "plotTree(input, False)\n",
    "input.traverseInorder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED con batch [tensor([[-4.0991e-02,  3.7937e-02,  1.4489e-02, -3.8284e-02,  7.1172e-03,\n",
      "         -2.0350e-02, -3.0817e-02, -2.4126e-02, -3.0974e-02,  3.1869e-02,\n",
      "          1.0023e-01, -6.5026e-02,  4.3110e-02,  5.5201e-02, -4.1470e-02,\n",
      "         -3.6200e-02, -6.1063e-02, -2.9893e-02,  7.9525e-02,  8.6626e-03,\n",
      "          4.1159e-02,  1.3158e-02, -4.6662e-02, -1.9157e-02,  3.4661e-02,\n",
      "         -2.6606e-02, -7.4691e-04, -8.4020e-02,  4.9588e-02,  2.7434e-02,\n",
      "          4.8398e-03,  1.6214e-01,  7.0365e-03, -1.3736e-02,  1.0991e-02,\n",
      "         -1.9707e-02,  4.0069e-02,  4.5624e-02,  2.7545e-02, -5.0406e-02,\n",
      "         -9.0993e-03,  3.6015e-02, -4.7160e-02, -5.7786e-02, -5.0426e-02,\n",
      "         -1.2140e-02,  3.8423e-02, -1.1976e-01,  4.8500e-02, -1.8128e-02,\n",
      "          3.5993e-02,  6.0112e-02, -2.5072e-03, -4.3994e-02,  6.7696e-02,\n",
      "         -2.4995e-02,  3.2436e-02, -3.2388e-02, -6.6341e-02,  4.3642e-02,\n",
      "          2.0474e-02,  6.4475e-02,  3.1930e-02,  1.7211e-02, -9.2123e-02,\n",
      "         -6.4129e-02, -9.7807e-02, -7.6972e-02, -1.8392e-02, -1.0128e-02,\n",
      "         -3.7456e-02,  3.9576e-02, -7.0936e-02, -1.7699e-02, -5.5313e-02,\n",
      "         -3.1649e-02,  6.7232e-02, -3.0667e-02, -1.0057e-01, -4.4925e-02,\n",
      "          3.5655e-02, -3.4010e-03, -2.2830e-02,  3.5333e-02,  1.2381e-02,\n",
      "          2.4938e-02, -2.8901e-03,  3.9998e-02,  5.7208e-02, -2.8850e-02,\n",
      "         -3.2980e-02,  7.9462e-02,  3.9873e-02,  1.6460e-02, -4.7983e-02,\n",
      "         -6.8089e-02,  9.6959e-02,  1.0674e-02, -9.3176e-03, -3.3424e-02,\n",
      "          4.5521e-03,  5.1346e-02,  5.9213e-03,  4.0374e-02, -2.9789e-02,\n",
      "          5.7047e-02,  1.0019e-01,  8.2287e-03, -1.1538e-02,  2.4584e-02,\n",
      "          1.6860e-02, -7.2491e-03,  2.1069e-02, -4.5971e-02, -4.0325e-02,\n",
      "          1.1139e-02, -8.1311e-03, -4.2215e-02,  7.0027e-02,  2.0266e-02,\n",
      "         -6.0310e-02, -4.0623e-02,  4.2017e-02,  6.1442e-02, -3.7315e-02,\n",
      "          6.0347e-02,  1.4632e-02, -4.7135e-02, -4.4575e-03,  3.1267e-03,\n",
      "          3.6098e-02,  3.9859e-02, -1.4667e-02,  9.5069e-02, -1.1496e-02,\n",
      "         -3.8210e-02, -7.4318e-02,  5.5405e-02,  2.0209e-02, -9.6943e-03,\n",
      "         -1.0258e-02, -6.7500e-02, -9.5348e-02,  1.7238e-02,  8.3567e-02,\n",
      "         -5.6493e-03, -7.0300e-02, -2.1610e-02,  2.0530e-02, -1.6642e-02,\n",
      "          1.8445e-02,  3.4540e-02,  5.6944e-02, -1.0514e-02,  9.2861e-02,\n",
      "          1.8151e-02, -3.8257e-02, -6.1283e-02, -5.0922e-02,  7.9364e-03,\n",
      "         -8.8523e-04, -3.1381e-02,  3.8148e-02, -8.5206e-02, -2.8745e-02,\n",
      "          2.3105e-03,  6.7792e-03,  5.1362e-03,  1.1512e-01, -7.6736e-02,\n",
      "          7.6384e-02, -1.6170e-02, -1.0845e-02, -7.7648e-03, -4.5775e-02,\n",
      "          3.2780e-02,  6.2386e-02,  5.3332e-02, -6.5480e-02,  5.5829e-02,\n",
      "          1.0756e-01, -6.5982e-02, -4.4404e-02,  3.2520e-02, -7.8263e-02,\n",
      "         -6.9352e-02, -2.9011e-03,  2.1719e-02, -9.3692e-03, -4.7216e-02,\n",
      "          2.3775e-03, -2.7144e-02,  6.1181e-02, -1.8585e-02, -1.1485e-02,\n",
      "          1.0653e-01,  6.8118e-03,  1.3732e-02, -2.2261e-02, -7.3196e-02,\n",
      "         -7.0448e-03, -5.1206e-02, -1.1798e-01, -4.3975e-02,  1.2591e-02,\n",
      "          6.2312e-02, -1.3957e-03,  4.3554e-02, -9.3234e-03,  2.8553e-02,\n",
      "          5.9159e-03,  6.1009e-02,  1.4957e-02, -2.0489e-02,  2.8336e-02,\n",
      "          7.1567e-03,  6.0218e-02,  2.0232e-02,  3.1792e-02, -1.7034e-02,\n",
      "          8.0261e-02,  1.0636e-02, -1.9130e-02,  7.8478e-02,  1.8794e-02,\n",
      "          1.2587e-04,  2.6902e-02,  8.9716e-02, -8.0689e-02,  3.5461e-02,\n",
      "          3.3584e-02,  9.7787e-03,  1.2262e-01, -4.2529e-02,  6.2667e-02,\n",
      "         -4.3892e-02, -3.5458e-02, -7.1566e-03, -5.6024e-02,  1.1585e-01,\n",
      "         -3.4796e-02, -2.2761e-02, -6.4432e-02,  1.7597e-02,  2.7631e-03,\n",
      "          3.8358e-02, -1.3601e-03, -7.9101e-02,  1.1918e-02, -3.4571e-02,\n",
      "         -6.6246e-02,  8.6170e-04,  5.9728e-02, -8.0681e-02,  3.0763e-02,\n",
      "          1.2666e-02]], device='cuda:0', grad_fn=<StackBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "import torch_f\n",
    "enc_fold = torch_f.Fold(device)\n",
    "enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "batch = iter(data_loader).next()\n",
    "#for example in batch:\n",
    "        #enc_fold.add('leafEncoder', example.radius)\n",
    "        #enc_fold_nodes.append(enc_fold.add('leafEncoder', example.radius))\n",
    "        #enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "\n",
    "        #print(\"enc fold nodes\", enc_fold)\n",
    "for example in data_loader:\n",
    "        example = example[0]\n",
    "        enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "print(\"ENCODED con batch\",enc_fold_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encodeado sin batch tensor([[-4.0991e-02,  3.7937e-02,  1.4489e-02, -3.8284e-02,  7.1172e-03,\n",
      "         -2.0350e-02, -3.0817e-02, -2.4126e-02, -3.0974e-02,  3.1869e-02,\n",
      "          1.0023e-01, -6.5026e-02,  4.3110e-02,  5.5201e-02, -4.1470e-02,\n",
      "         -3.6200e-02, -6.1063e-02, -2.9893e-02,  7.9525e-02,  8.6626e-03,\n",
      "          4.1159e-02,  1.3158e-02, -4.6662e-02, -1.9157e-02,  3.4661e-02,\n",
      "         -2.6606e-02, -7.4691e-04, -8.4020e-02,  4.9588e-02,  2.7434e-02,\n",
      "          4.8398e-03,  1.6214e-01,  7.0365e-03, -1.3736e-02,  1.0991e-02,\n",
      "         -1.9707e-02,  4.0069e-02,  4.5624e-02,  2.7545e-02, -5.0406e-02,\n",
      "         -9.0993e-03,  3.6015e-02, -4.7160e-02, -5.7786e-02, -5.0426e-02,\n",
      "         -1.2140e-02,  3.8423e-02, -1.1976e-01,  4.8500e-02, -1.8128e-02,\n",
      "          3.5993e-02,  6.0112e-02, -2.5072e-03, -4.3994e-02,  6.7696e-02,\n",
      "         -2.4995e-02,  3.2436e-02, -3.2388e-02, -6.6341e-02,  4.3642e-02,\n",
      "          2.0474e-02,  6.4475e-02,  3.1930e-02,  1.7211e-02, -9.2123e-02,\n",
      "         -6.4129e-02, -9.7807e-02, -7.6972e-02, -1.8392e-02, -1.0128e-02,\n",
      "         -3.7456e-02,  3.9576e-02, -7.0936e-02, -1.7699e-02, -5.5313e-02,\n",
      "         -3.1649e-02,  6.7232e-02, -3.0667e-02, -1.0057e-01, -4.4925e-02,\n",
      "          3.5655e-02, -3.4010e-03, -2.2830e-02,  3.5333e-02,  1.2381e-02,\n",
      "          2.4938e-02, -2.8901e-03,  3.9998e-02,  5.7208e-02, -2.8850e-02,\n",
      "         -3.2980e-02,  7.9462e-02,  3.9873e-02,  1.6460e-02, -4.7983e-02,\n",
      "         -6.8089e-02,  9.6959e-02,  1.0674e-02, -9.3176e-03, -3.3424e-02,\n",
      "          4.5521e-03,  5.1346e-02,  5.9213e-03,  4.0374e-02, -2.9789e-02,\n",
      "          5.7047e-02,  1.0019e-01,  8.2287e-03, -1.1538e-02,  2.4584e-02,\n",
      "          1.6860e-02, -7.2491e-03,  2.1069e-02, -4.5971e-02, -4.0325e-02,\n",
      "          1.1139e-02, -8.1311e-03, -4.2215e-02,  7.0027e-02,  2.0266e-02,\n",
      "         -6.0310e-02, -4.0623e-02,  4.2017e-02,  6.1442e-02, -3.7315e-02,\n",
      "          6.0347e-02,  1.4632e-02, -4.7135e-02, -4.4575e-03,  3.1267e-03,\n",
      "          3.6098e-02,  3.9859e-02, -1.4667e-02,  9.5069e-02, -1.1496e-02,\n",
      "         -3.8210e-02, -7.4318e-02,  5.5405e-02,  2.0209e-02, -9.6943e-03,\n",
      "         -1.0258e-02, -6.7500e-02, -9.5348e-02,  1.7238e-02,  8.3567e-02,\n",
      "         -5.6493e-03, -7.0300e-02, -2.1610e-02,  2.0530e-02, -1.6642e-02,\n",
      "          1.8445e-02,  3.4540e-02,  5.6944e-02, -1.0514e-02,  9.2861e-02,\n",
      "          1.8151e-02, -3.8257e-02, -6.1283e-02, -5.0922e-02,  7.9364e-03,\n",
      "         -8.8523e-04, -3.1381e-02,  3.8148e-02, -8.5206e-02, -2.8745e-02,\n",
      "          2.3105e-03,  6.7792e-03,  5.1362e-03,  1.1512e-01, -7.6736e-02,\n",
      "          7.6384e-02, -1.6170e-02, -1.0845e-02, -7.7648e-03, -4.5775e-02,\n",
      "          3.2780e-02,  6.2386e-02,  5.3332e-02, -6.5480e-02,  5.5829e-02,\n",
      "          1.0756e-01, -6.5982e-02, -4.4404e-02,  3.2520e-02, -7.8263e-02,\n",
      "         -6.9352e-02, -2.9011e-03,  2.1719e-02, -9.3692e-03, -4.7216e-02,\n",
      "          2.3775e-03, -2.7144e-02,  6.1181e-02, -1.8585e-02, -1.1485e-02,\n",
      "          1.0653e-01,  6.8118e-03,  1.3732e-02, -2.2261e-02, -7.3196e-02,\n",
      "         -7.0448e-03, -5.1206e-02, -1.1798e-01, -4.3975e-02,  1.2591e-02,\n",
      "          6.2312e-02, -1.3957e-03,  4.3554e-02, -9.3234e-03,  2.8553e-02,\n",
      "          5.9159e-03,  6.1009e-02,  1.4957e-02, -2.0489e-02,  2.8336e-02,\n",
      "          7.1567e-03,  6.0218e-02,  2.0232e-02,  3.1792e-02, -1.7034e-02,\n",
      "          8.0261e-02,  1.0636e-02, -1.9130e-02,  7.8478e-02,  1.8794e-02,\n",
      "          1.2587e-04,  2.6902e-02,  8.9716e-02, -8.0689e-02,  3.5461e-02,\n",
      "          3.3584e-02,  9.7787e-03,  1.2262e-01, -4.2529e-02,  6.2667e-02,\n",
      "         -4.3892e-02, -3.5458e-02, -7.1566e-03, -5.6024e-02,  1.1585e-01,\n",
      "         -3.4796e-02, -2.2761e-02, -6.4432e-02,  1.7597e-02,  2.7631e-03,\n",
      "          3.8358e-02, -1.3601e-03, -7.9101e-02,  1.1918e-02, -3.4571e-02,\n",
      "         -6.6246e-02,  8.6170e-04,  5.9728e-02, -8.0681e-02,  3.0763e-02,\n",
      "          1.2666e-02]], device='cuda:0', grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for data in data_loader:\n",
    "    data = data[0]\n",
    "\n",
    "enc_f = encode_structure(example).to(device)\n",
    "\n",
    "print(\"encodeado sin batch\", enc_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_fold_nodes[0]-enc_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[3, 5, 5, 5, 5, 6, 3, 5, 6, 5]\n",
      "4.8\n",
      "3.0\n",
      "0.6\n",
      "1.2\n"
     ]
    }
   ],
   "source": [
    "n_no = []\n",
    "qzero = 0\n",
    "qOne = 0\n",
    "qtwo = 0\n",
    "\n",
    "for batch in data_loader:\n",
    "    for tree in batch:\n",
    "        count = []\n",
    "        n = tree.count_nodes(tree, count)\n",
    "        n_no.append(len(n))\n",
    "        li = []\n",
    "        tree.traverseInorderChilds(tree, li)\n",
    "        zero = [a for a in li if a == 0]\n",
    "        one = [a for a in li if a == 1]\n",
    "        two = [a for a in li if a == 2]\n",
    "        qzero += len(zero)\n",
    "        qOne += len(one)\n",
    "        qtwo += len(two)\n",
    "\n",
    "print(len(data_loader)*batch_size)\n",
    "print(n_no)\n",
    "nprom = np.mean(n_no)\n",
    "print(nprom)\n",
    "qzero /= len(data_loader)*batch_size\n",
    "qOne /= len(data_loader)*batch_size\n",
    "qtwo /= len(data_loader)*batch_size\n",
    "\n",
    "print(qzero)\n",
    "print(qOne)\n",
    "print(qtwo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en el loop creo un fold, mando este fold con cada uno de los arboles del batch a encode_structure_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.mlp1 = nn.Linear(latent_size, hidden_size)\n",
    "        self.mlp2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.mlp3 = nn.Linear(hidden_size, 3)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_feature):\n",
    "        #print(\"classifier input\", input_feature)\n",
    "        output = self.mlp1(input_feature)\n",
    "        output = self.tanh(output)\n",
    "        output = self.mlp2(output)\n",
    "        output = self.tanh(output)\n",
    "        output = self.mlp3(output)\n",
    "        #print(\"classifier output\", output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class InternalDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size: int):\\n        super(InternalDecoder, self).__init__()\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.lp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp_right = nn.Linear(latent_size, latent_size)\\n        self.tanh = nn.Tanh()\\n        self.mlp2 = nn.Linear(latent_size,4)\\n\\n    def forward(self, parent_feature):\\n        #print(\"internal decoder\")\\n        #print(\"input\", parent_feature.shape)\\n        vector = self.mlp(parent_feature)\\n        vector = self.tanh(vector)\\n        vector = self.lp2(vector)\\n        vector = self.tanh(vector)\\n        right_feature = self.mlp_right(vector)\\n        right_feature = self.tanh(right_feature)\\n        rad_feature = self.mlp2(vector)\\n\\n        return right_feature, rad_feature\\n\\nclass BifurcationDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size : int):\\n        super(BifurcationDecoder, self).__init__()\\n        #self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.lp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp_left = nn.Linear(latent_size, latent_size)\\n        self.mlp_right = nn.Linear(latent_size, latent_size)\\n        self.mlp2 = nn.Linear(latent_size,4)\\n        self.tanh = nn.Tanh()\\n\\n    def forward(self, parent_feature):\\n        #print(\"bifurcation decoder input\", parent_feature.shape)\\n        parent_feature = parent_feature.reshape(-1,128)\\n        #print(\"bifurcation decoder input\", parent_feature.shape)\\n        vector = self.mlp(parent_feature)\\n        #print(\"v1\", vector.shape)\\n        vector = self.tanh(vector)\\n        #print(\"v2\", vector.shape)\\n        vector = self.lp2(vector)\\n        #print(\"v3\", vector.shape)\\n        vector = self.tanh(vector)\\n        left_feature = self.mlp_left(vector)\\n        left_feature = self.tanh(left_feature)\\n        right_feature = self.mlp_right(vector)\\n        right_feature = self.tanh(right_feature)\\n        rad_feature = self.mlp2(vector)\\n        #print(\"exiting bif dec\")\\n        return left_feature, right_feature, rad_feature\\n\\n\\nclass featureDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size: int):\\n        super(featureDecoder, self).__init__()\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.mlp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp3 = nn.Linear(latent_size, latent_size)\\n        self.tanh = nn.Tanh()\\n        self.mlp4 = nn.Linear(latent_size,4)\\n\\n    def forward(self, parent_feature):\\n        #print(\"feature decoder input\", parent_feature.shape)\\n\\n        vector = self.mlp(parent_feature)\\n        vector = self.tanh(vector)\\n        vector = self.mlp2(vector)\\n        vector = self.tanh(vector)\\n        vector = self.mlp3(vector)\\n        vector = self.tanh(vector)\\n        vector = self.mlp4(vector)\\n       \\n        return vector\\n\\n\\n\\nclass GRASSDecoder(nn.Module):\\n    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\\n        super(GRASSDecoder, self).__init__()\\n        self.feature_decoder = featureDecoder(latent_size, hidden_size)\\n        self.internal_decoder = InternalDecoder(latent_size, hidden_size)\\n        self.bifurcation_decoder = BifurcationDecoder(latent_size, hidden_size)\\n        self.node_classifier = NodeClassifier(latent_size, hidden_size)\\n        self.mseLoss = nn.MSELoss()  # pytorch\\'s mean squared error loss\\n        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch\\'s cross entropy loss (NOTE: no softmax is needed before)\\n\\n    def featureDecoder(self, feature):\\n        return self.feature_decoder(feature)\\n\\n    def internalDecoder(self, feature):\\n        return self.internal_decoder(feature)\\n\\n    def bifurcationDecoder(self, feature):\\n        return self.bifurcation_decoder(feature)\\n\\n    def nodeClassifier(self, feature):\\n        return self.node_classifier(feature)\\n\\n    def calcularLossAtributo(self, nodo, radio):\\n        if nodo is None:\\n            return\\n        else:\\n            #print(\"radio\", radio)\\n            #print(\"nodo\", nodo)\\n            nodo = torch.stack(nodo)\\n            #print(\"nodo stack\", nodo)\\n            #radio = radio.reshape(-1,4)\\n        \\n            #return mse\\n            #return torch.cat([self.mseLoss(b, gt) for b, gt in zip(radio, nodo)], 0)\\n            z = zip(radio.reshape(-1,4), nodo.reshape(-1,4))\\n            \\n            #for b, gt in z:\\n            #    print(\"bgt\", b, gt)\\n                \\n            \\n            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\\n            #print(\"loss\", l)\\n            return l\\n\\n\\n    def classifyLossEstimator(self, label_vector, original):\\n        if original is None:\\n            return\\n        else:\\n           \\n            v = []\\n            for o in original:\\n                if o == 0:\\n                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\\n                if o == 1:\\n                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\\n                if o == 2:\\n                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\\n                v.append(vector)\\n\\n            v = torch.stack(v)\\n            z = zip(label_vector.reshape(-1,3), v.reshape(-1,3))   \\n            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\\n            \\n            return l\\n            #return c\\n       # return torch.cat([self.creLoss(l.unsqueeze(0), gt).mul(0.2) for l, gt in zip(label_vector, gt_label_vector)], 0)\\n\\n    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\\n        \\n        v = v1.add(v2)\\n        #print(\"v0\", v)\\n        if v3 is not None:\\n            v = v.add(v3)\\n        #print(\"v0\", v)\\n        if v4 is not None:\\n            v = v.add(v4)\\n       \\n        return v\\nif qzero == 0:\\n    qzero = 1\\nif qOne == 0:\\n    qOne = 1\\nif qtwo == 0:\\n    qtwo = 1\\nmult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\\nGrassdecoder = GRASSDecoder(latent_size=128, hidden_size=256, mult = mult)\\nGrassdecoder = Grassdecoder.to(device)\\n'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class InternalDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size: int):\n",
    "        super(InternalDecoder, self).__init__()\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, latent_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.mlp2 = nn.Linear(latent_size,4)\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"internal decoder\")\n",
    "        #print(\"input\", parent_feature.shape)\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.lp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        rad_feature = self.mlp2(vector)\n",
    "\n",
    "        return right_feature, rad_feature\n",
    "\n",
    "class BifurcationDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(BifurcationDecoder, self).__init__()\n",
    "        #self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp_left = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp2 = nn.Linear(latent_size,4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"bifurcation decoder input\", parent_feature.shape)\n",
    "        parent_feature = parent_feature.reshape(-1,128)\n",
    "        #print(\"bifurcation decoder input\", parent_feature.shape)\n",
    "        vector = self.mlp(parent_feature)\n",
    "        #print(\"v1\", vector.shape)\n",
    "        vector = self.tanh(vector)\n",
    "        #print(\"v2\", vector.shape)\n",
    "        vector = self.lp2(vector)\n",
    "        #print(\"v3\", vector.shape)\n",
    "        vector = self.tanh(vector)\n",
    "        left_feature = self.mlp_left(vector)\n",
    "        left_feature = self.tanh(left_feature)\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        rad_feature = self.mlp2(vector)\n",
    "        #print(\"exiting bif dec\")\n",
    "        return left_feature, right_feature, rad_feature\n",
    "\n",
    "\n",
    "class featureDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size: int):\n",
    "        super(featureDecoder, self).__init__()\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp3 = nn.Linear(latent_size, latent_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.mlp4 = nn.Linear(latent_size,4)\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"feature decoder input\", parent_feature.shape)\n",
    "\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp3(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp4(vector)\n",
    "       \n",
    "        return vector\n",
    "\n",
    "\n",
    "\n",
    "class GRASSDecoder(nn.Module):\n",
    "    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\n",
    "        super(GRASSDecoder, self).__init__()\n",
    "        self.feature_decoder = featureDecoder(latent_size, hidden_size)\n",
    "        self.internal_decoder = InternalDecoder(latent_size, hidden_size)\n",
    "        self.bifurcation_decoder = BifurcationDecoder(latent_size, hidden_size)\n",
    "        self.node_classifier = NodeClassifier(latent_size, hidden_size)\n",
    "        self.mseLoss = nn.MSELoss()  # pytorch's mean squared error loss\n",
    "        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch's cross entropy loss (NOTE: no softmax is needed before)\n",
    "\n",
    "    def featureDecoder(self, feature):\n",
    "        return self.feature_decoder(feature)\n",
    "\n",
    "    def internalDecoder(self, feature):\n",
    "        return self.internal_decoder(feature)\n",
    "\n",
    "    def bifurcationDecoder(self, feature):\n",
    "        return self.bifurcation_decoder(feature)\n",
    "\n",
    "    def nodeClassifier(self, feature):\n",
    "        return self.node_classifier(feature)\n",
    "\n",
    "    def calcularLossAtributo(self, nodo, radio):\n",
    "        if nodo is None:\n",
    "            return\n",
    "        else:\n",
    "            #print(\"radio\", radio)\n",
    "            #print(\"nodo\", nodo)\n",
    "            nodo = torch.stack(nodo)\n",
    "            #print(\"nodo stack\", nodo)\n",
    "            #radio = radio.reshape(-1,4)\n",
    "        \n",
    "            #return mse\n",
    "            #return torch.cat([self.mseLoss(b, gt) for b, gt in zip(radio, nodo)], 0)\n",
    "            z = zip(radio.reshape(-1,4), nodo.reshape(-1,4))\n",
    "            \n",
    "            #for b, gt in z:\n",
    "            #    print(\"bgt\", b, gt)\n",
    "                \n",
    "            \n",
    "            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\n",
    "            #print(\"loss\", l)\n",
    "            return l\n",
    "\n",
    "\n",
    "    def classifyLossEstimator(self, label_vector, original):\n",
    "        if original is None:\n",
    "            return\n",
    "        else:\n",
    "           \n",
    "            v = []\n",
    "            for o in original:\n",
    "                if o == 0:\n",
    "                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\n",
    "                if o == 1:\n",
    "                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\n",
    "                if o == 2:\n",
    "                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\n",
    "                v.append(vector)\n",
    "\n",
    "            v = torch.stack(v)\n",
    "            z = zip(label_vector.reshape(-1,3), v.reshape(-1,3))   \n",
    "            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\n",
    "            \n",
    "            return l\n",
    "            #return c\n",
    "       # return torch.cat([self.creLoss(l.unsqueeze(0), gt).mul(0.2) for l, gt in zip(label_vector, gt_label_vector)], 0)\n",
    "\n",
    "    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\n",
    "        \n",
    "        v = v1.add(v2)\n",
    "        #print(\"v0\", v)\n",
    "        if v3 is not None:\n",
    "            v = v.add(v3)\n",
    "        #print(\"v0\", v)\n",
    "        if v4 is not None:\n",
    "            v = v.add(v4)\n",
    "       \n",
    "        return v\n",
    "if qzero == 0:\n",
    "    qzero = 1\n",
    "if qOne == 0:\n",
    "    qOne = 1\n",
    "if qtwo == 0:\n",
    "    qtwo = 1\n",
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "Grassdecoder = GRASSDecoder(latent_size=128, hidden_size=256, mult = mult)\n",
    "Grassdecoder = Grassdecoder.to(device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(Decoder, self).__init__()\n",
    "        #self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.lp3 = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "        self.mlp_left = nn.Linear(latent_size, hidden_size)\n",
    "        self.mlp_left2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, hidden_size)\n",
    "        self.mlp_right2 = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "        self.mlp2 = nn.Linear(latent_size,latent_size)\n",
    "        self.mlp3 = nn.Linear(latent_size,4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def common_branch(self, parent_feature):\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.lp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.lp3(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        return vector\n",
    "\n",
    "    def attr_branch(self, vector):\n",
    "        vector = self.mlp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp3(vector)        \n",
    "        return vector\n",
    "\n",
    "    def right_branch(self, vector):\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        right_feature = self.mlp_right2(right_feature)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        return right_feature\n",
    "\n",
    "    def left_branch(self, vector):\n",
    "        left_feature = self.mlp_left(vector)\n",
    "        left_feature = self.tanh(left_feature)\n",
    "        left_feature = self.mlp_left2(left_feature)\n",
    "        left_feature = self.tanh(left_feature)\n",
    "        return left_feature\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "      \n",
    "        vector      = self.common_branch(parent_feature)\n",
    "        attr_vector = self.attr_branch(vector)\n",
    "        return attr_vector \n",
    "\n",
    "    def forward1(self, parent_feature):\n",
    "    \n",
    "\n",
    "        vector       = self.common_branch(parent_feature)\n",
    "        attr_vector  = self.attr_branch(vector)\n",
    "        right_vector = self.right_branch(vector)\n",
    "        \n",
    "        #print(\"right vector\", right_vector)\n",
    "        #print(\"radius\", attr_vector)\n",
    "        return right_vector, attr_vector\n",
    "\n",
    "    def forward2(self, parent_feature):\n",
    "       \n",
    "\n",
    "        vector       = self.common_branch(parent_feature)\n",
    "        attr_vector  = self.attr_branch(vector)\n",
    "        right_vector = self.right_branch(vector)\n",
    "        left_vector  = self.left_branch(vector)\n",
    "        #print(\"left vector\", left_vector)\n",
    "        #print(\"right vector\", right_vector)\n",
    "        #print(\"radius\", attr_vector)\n",
    "        return left_vector, right_vector, attr_vector\n",
    "\n",
    "\n",
    "\n",
    "class GRASSDecoder(nn.Module):\n",
    "    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\n",
    "        super(GRASSDecoder, self).__init__()\n",
    "        self.decoder = Decoder(latent_size, hidden_size)\n",
    "        self.node_classifier = NodeClassifier(latent_size, hidden_size)\n",
    "        self.mseLoss = nn.MSELoss()  # pytorch's mean squared error loss\n",
    "        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch's cross entropy loss (NOTE: no softmax is needed before)\n",
    "        \n",
    "\n",
    "\n",
    "    def featureDecoder(self, feature):\n",
    "        return self.decoder.forward(feature)\n",
    "\n",
    "    def internalDecoder(self, feature):\n",
    "        return self.decoder.forward1(feature)\n",
    "\n",
    "    def bifurcationDecoder(self, feature):\n",
    "        return self.decoder.forward2(feature)\n",
    "\n",
    "    def nodeClassifier(self, feature):\n",
    "        return self.node_classifier(feature)\n",
    "\n",
    "    def calcularLossAtributo(self, nodo, radio):\n",
    "        #print(\"nodo\", nodo)\n",
    "        #print(\"radio\", radio)\n",
    "        a, b = list(zip(*nodo))# a son los atributos, b los pesos\n",
    "        if nodo is None:\n",
    "            return\n",
    "        else:\n",
    "            nodo = torch.stack(list(a))\n",
    "        \n",
    "            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\n",
    "            #print(\"mse\", l)\n",
    "            return l\n",
    "\n",
    "\n",
    "    def classifyLossEstimator(self, label_vector, original):\n",
    "        if original is None:\n",
    "            return\n",
    "        else:\n",
    "           \n",
    "            v = []\n",
    "            for o in original:\n",
    "                if o == 0:\n",
    "                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\n",
    "                if o == 1:\n",
    "                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\n",
    "                if o == 2:\n",
    "                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\n",
    "                v.append(vector)\n",
    "            \n",
    "\n",
    "            v = torch.stack(v)\n",
    "            \n",
    "            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\n",
    "         \n",
    "\n",
    "            return l\n",
    "            #return c\n",
    "\n",
    "    '''\n",
    "    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\n",
    "        \n",
    "        print(\"loss estructura\", v1)\n",
    "        print(\"loss atributo\", v2)\n",
    "        print(\"right loss\", v3)\n",
    "        print(\"left loss\", v4)\n",
    "\n",
    "\n",
    "        v = v1.add(v2)\n",
    "        #print(\"v0\", v)\n",
    "        if v3 is not None:\n",
    "            v = v.add(v3)\n",
    "        #print(\"v0\", v)\n",
    "        if v4 is not None:\n",
    "            v = v.add(v4)\n",
    "       \n",
    "        return v\n",
    "\n",
    "    '''\n",
    "    def vectorAdder(self, v1, v2):\n",
    "        v = v1.add(v2)\n",
    "        return v\n",
    "\n",
    "    def vectorMult(self, m, v):\n",
    "        #print(\"v\", v)\n",
    "        #print(\"m\", m)\n",
    "        z = zip(v, m)\n",
    "        r = []\n",
    "        for c, d in z:\n",
    "            #print(\"v\", c)\n",
    "            #print(\"m\", d)\n",
    "            r.append(torch.mul(c, d))\n",
    "        #res = [torch.mul(v, m) for v, m in zip(v, m)]\n",
    "        #print(\"res\", r)\n",
    "        return r\n",
    "\n",
    "if round(qzero) == 0:\n",
    "    qzero = 1\n",
    "if round(qOne) == 0:\n",
    "    qOne = 1\n",
    "if round(qtwo) == 0:\n",
    "    qtwo = 1\n",
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "\n",
    "Grassdecoder = GRASSDecoder(latent_size=256, hidden_size=512, mult = mult)\n",
    "Grassdecoder = Grassdecoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3333, 1.0000, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "print(mult)\n",
    "def calcularLossEstructura(cl_p, original):\n",
    "    \n",
    "    if original is None:\n",
    "        return\n",
    "    mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "    ce = nn.CrossEntropyLoss(weight = mult)\n",
    "\n",
    "    if original.childs() == 0:\n",
    "        vector = [1, 0, 0] \n",
    "    if original.childs() == 1:\n",
    "        vector = [0, 1, 0]\n",
    "    if original.childs() == 2:\n",
    "        vector = [0, 0, 1] \n",
    "\n",
    "    #print(\"original\", vector)\n",
    "    #print(\"prediction\", cl_p)\n",
    "    c = ce(cl_p, torch.tensor(vector, device=device, dtype = torch.float).reshape(1, 3))\n",
    "    #print(\"ce\", 0.4*c)\n",
    "    return c\n",
    "\n",
    "\n",
    "def calcularLossAtributo(nodo, radio):\n",
    "    if nodo is None:\n",
    "        return\n",
    "    #print(\"nodo\", nodo)\n",
    "    #print(\"radio\", radio)\n",
    "\n",
    "    radio = radio.reshape(-1,4)\n",
    "    nodo = nodo.radius.reshape(-1,4)\n",
    "    l2    = nn.MSELoss()\n",
    "   \n",
    "    mse = l2(radio, nodo)\n",
    "    #print(\"mse\", mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchNode(node, key):\n",
    "     \n",
    "    if (node == None):\n",
    "        return False\n",
    " \n",
    "    if (node.data == key):\n",
    "        return node\n",
    "        \n",
    " \n",
    "    \"\"\" then recur on left subtree \"\"\"\n",
    "    res1 = searchNode(node.left, key)\n",
    "    # node found, no need to look further\n",
    "    if res1:\n",
    "        return res1\n",
    " \n",
    "    \"\"\" node is not found in left,\n",
    "    so recur on right subtree \"\"\"\n",
    "    res2 = searchNode(node.right, key)\n",
    "    return res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_nodes 5\n"
     ]
    }
   ],
   "source": [
    "def getLevelUtil(node, data, level):\n",
    "    if (node == None):\n",
    "        return 0\n",
    " \n",
    "    if (node.data == data):\n",
    "        return level\n",
    " \n",
    "    downlevel = getLevelUtil(node.left, data, level + 1)\n",
    "\n",
    "    if (downlevel != 0):\n",
    "        return downlevel\n",
    " \n",
    "    downlevel = getLevelUtil(node.right, data, level + 1)\n",
    "    return downlevel\n",
    " \n",
    "# Returns level of given data value\n",
    " \n",
    " \n",
    "def getLevel(node, data):\n",
    "    return getLevelUtil(node, data, 1)\n",
    " \n",
    "\n",
    "c = []\n",
    "n_nodes = input.count_nodes(input, c)\n",
    "print(\"n_nodes\", len(n_nodes))\n",
    "for x in range(0, len(n_nodes)):\n",
    "        level = getLevel(input, x)\n",
    "        if (level):\n",
    "            #print(\"Level of\", x, \"is\", getLevel(input, x))\n",
    "            node = searchNode(input, x)\n",
    "            node.level = getLevel(input, x)\n",
    "        else:\n",
    "            print(x, \"is not present in tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Node object at 0x000002485B4A3910>\n",
      "tree level 11\n",
      "<__main__.Node object at 0x00000247841935E0>\n",
      "tree level 11\n",
      "<__main__.Node object at 0x000002485B328E50>\n",
      "tree level 11\n",
      "<__main__.Node object at 0x000002485B4A1FF0>\n",
      "tree level 11\n",
      "<__main__.Node object at 0x000002485B32A2C0>\n",
      "tree level 6\n",
      "<__main__.Node object at 0x000002485B4A3790>\n",
      "tree level 11\n",
      "<__main__.Node object at 0x000002485B4A22C0>\n",
      "tree level 14\n",
      "<__main__.Node object at 0x000002485B328AF0>\n",
      "tree level 6\n",
      "<__main__.Node object at 0x000002485B329150>\n",
      "tree level 11\n",
      "<__main__.Node object at 0x000002485B4A0160>\n",
      "tree level 14\n"
     ]
    }
   ],
   "source": [
    "for d in data_loader:\n",
    "    for data in d:\n",
    "        print(data)\n",
    "        count = []\n",
    "        numerar_nodos(data, count)\n",
    "        c = []\n",
    "        n_nodes = data.count_nodes(data, c)\n",
    "        for x in range(0, len(n_nodes)):\n",
    "            level = getLevel(data, x)\n",
    "            if (level):\n",
    "                #print(\"Level of\", x, \"is\", getLevel(input, x))\n",
    "                node = searchNode(data, x)\n",
    "                node.level = getLevel(data, x)\n",
    "            else:\n",
    "                print(x, \"is not present in tree\")\n",
    "        tree_level = []\n",
    "        data.get_tree_level(data, tree_level)\n",
    "        print(\"tree level\", sum(tree_level))\n",
    "        data.set_tree_level(data, sum(tree_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_structure_fold_grass(fold, v, root):\n",
    "   \n",
    "    def decode_node(fold, v, node):\n",
    "        \n",
    "        \n",
    "        if node.childs() == 0 : \n",
    "\n",
    "            radio = fold.add('featureDecoder', v)\n",
    "            lossAtributo = fold.add('calcularLossAtributo', node, radio)\n",
    "\n",
    "            label = fold.add('nodeClassifier', v)\n",
    "            \n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)  \n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            \n",
    "            loss =  fold.add('vectorAdder', losse, lossAtributo)       \n",
    "            return loss\n",
    "\n",
    "            \n",
    "            \n",
    "        elif node.childs() == 1 :\n",
    "            right, radius = fold.add('internalDecoder', v).split(2)\n",
    "            label = fold.add('nodeClassifier', v)\n",
    "            nodoSiguiente = node.right\n",
    "            if nodoSiguiente is not None:\n",
    "                right_loss = decode_node(fold, right, nodoSiguiente)\n",
    "\n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)\n",
    "            lossAtributo = fold.add('calcularLossAtributo', node, radius)\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            loss = fold.add('vectorAdder', losse, lossAtributo)\n",
    "            \n",
    "        \n",
    "            loss2 = fold.add('vectorAdder', loss, right_loss)\n",
    "            return loss2\n",
    "            \n",
    "            \n",
    "\n",
    "        elif node.childs() == 2 :\n",
    "            left, right, radius = fold.add('bifurcationDecoder', v).split(3)\n",
    "            \n",
    "            label = fold.add('nodeClassifier', v)            \n",
    "            \n",
    "            nodoSiguienteRight = node.right\n",
    "            nodoSiguienteLeft = node.left\n",
    "\n",
    "\n",
    "            if nodoSiguienteRight is not None:\n",
    "                right_loss = decode_node(fold, right, nodoSiguienteRight)\n",
    "             \n",
    "            if nodoSiguienteLeft is not None:\n",
    "                left_loss  = decode_node(fold, left, nodoSiguienteLeft)\n",
    "\n",
    "            multipl = node.level/node.treelevel\n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            lossAtributo   = fold.add('calcularLossAtributo', node, radius)\n",
    "            loss = fold.add('vectorAdder', losse, lossAtributo)\n",
    "            loss2 = fold.add('vectorAdder', loss, right_loss)\n",
    "            loss3 = fold.add('vectorAdder', loss2, left_loss)\n",
    "            \n",
    "            return loss3\n",
    "            \n",
    "\n",
    "    dec = decode_node (fold, v, root)\n",
    "    return dec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_testing_grass(v, root, max, decoder):\n",
    "    def decode_node(v, node, max, decoder):\n",
    "        cl = decoder.nodeClassifier(v)\n",
    "        _, label = torch.max(cl, 1)\n",
    "        label = label.data\n",
    "        \n",
    "        \n",
    "        if label == 0 and createNode.count <= max: ##output del classifier\n",
    "           \n",
    "            radio = decoder.featureDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node, radio )\n",
    "            if lossEstructura is not None:\n",
    "                multipl = node.level/node.treelevel\n",
    "                lossEstructura = multipl*lossEstructura\n",
    "            \n",
    "            return createNode(1,radio, ce = lossEstructura,  mse = lossAtrs)\n",
    "\n",
    "        elif label == 1 and createNode.count <= max:\n",
    "       \n",
    "            right, radius = decoder.internalDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node, radius )\n",
    "            if lossEstructura is not None:\n",
    "                multipl = node.level/node.treelevel\n",
    "                lossEstructura = multipl*lossEstructura\n",
    "            d = createNode(1, radius, ce = lossEstructura,  mse = lossAtrs) \n",
    "           \n",
    "            if not node is None:\n",
    "                if not node.right is None:\n",
    "                    nodoSiguiente = node.right\n",
    "                else:\n",
    "                    nodoSiguiente = None\n",
    "            else:\n",
    "                nodoSiguiente = None\n",
    "            \n",
    "            d.right = decode_node(right, nodoSiguiente, max, decoder)\n",
    "            \n",
    "\n",
    "            return d\n",
    "       \n",
    "        elif label == 2 and createNode.count <= max:\n",
    "            left, right, radius = decoder.bifurcationDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node, radius )\n",
    "            if lossEstructura is not None:\n",
    "                multipl = node.level/node.treelevel\n",
    "                lossEstructura = multipl*lossEstructura\n",
    "            \n",
    "            d = createNode(1, radius, ce = lossEstructura,  mse = lossAtrs )\n",
    "  \n",
    "            if not node is None: #el nodo existe, me fijo si tiene hijo der/izq\n",
    "                if not node.right is None:\n",
    "                    nodoSiguienteRight = node.right\n",
    "                else:\n",
    "                    nodoSiguienteRight = None\n",
    "                if not node.left is None:\n",
    "                    nodoSiguienteLeft = node.left\n",
    "                else:\n",
    "                    nodoSiguienteLeft = None\n",
    "            else: #el nodo no existe\n",
    "                nodoSiguienteRight = None\n",
    "                nodoSiguienteLeft = None\n",
    "            \n",
    "            d.right = decode_node(right, nodoSiguienteRight, max, decoder)\n",
    "            d.left = decode_node(left, nodoSiguienteLeft, max, decoder)\n",
    "            \n",
    "           \n",
    "            return d\n",
    "            \n",
    "    createNode.count = 0\n",
    "    dec = decode_node (v, root, max, decoder)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, encoder, decoder, optimizer\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            #print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            #print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            #'classifier_state_dict': classifier.state_dict(),\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                \n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, 'outputs/best_model.pth')\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_structure_fold_(v, root):\n",
    "    \n",
    "    def decode_node(v, node):\n",
    "        cl = Grassdecoder.nodeClassifier(v)\n",
    "        _, label = torch.max(cl, 1)\n",
    "        label = label.data\n",
    "\n",
    "        \n",
    "        if node.childs() == 0 : ##output del classifier\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            radio = Grassdecoder.featureDecoder(v)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radio )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "            nd = createNode(1,radio, ce = losse,  mse = lossAtrs)\n",
    "            return nd\n",
    "\n",
    "        elif node.childs() == 1 :\n",
    "        \n",
    "            right, radius = Grassdecoder.internalDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radius )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "            nd = createNode(1, radius, cl_prob = lossAtrs , ce = losse, mse = lossAtrs) \n",
    "            \n",
    "            nodoSiguiente = node.right\n",
    "           \n",
    "            if nodoSiguiente is not None:\n",
    "                nd.right = decode_node(right, nodoSiguiente)\n",
    "               \n",
    "            return nd\n",
    "\n",
    "        elif node.childs() == 2 :\n",
    "            left, right, radius = Grassdecoder.bifurcationDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radius )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "            nd = createNode(1, radius, cl_prob = lossAtrs, ce = losse, mse = lossAtrs)\n",
    "            \n",
    "            nodoSiguienteRight = node.right\n",
    "            nodoSiguienteLeft = node.left\n",
    "\n",
    "            \n",
    "            if nodoSiguienteRight is not None:\n",
    "                nd.right = decode_node(right, nodoSiguienteRight)\n",
    "             \n",
    "            if nodoSiguienteLeft is not None:\n",
    "                nd.left  = decode_node(left, nodoSiguienteLeft)\n",
    "            \n",
    "            return nd\n",
    "            \n",
    "    createNode.count = 0\n",
    "    dec = decode_node (v, root)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder con batch - decoder con batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1onuttkc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e45f25e1d640d8886f89c28c7ba09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.013 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.099969…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">firm-vortex-2</strong>: <a href=\"https://wandb.ai/paufeldman/autoencoder4/runs/1onuttkc\" target=\"_blank\">https://wandb.ai/paufeldman/autoencoder4/runs/1onuttkc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221129_160955-1onuttkc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1onuttkc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681a9095fcd94a5fa444b8cc566367fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\wandb\\run-20221129_161105-1004sk1x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/paufeldman/autoencoder4/runs/1004sk1x\" target=\"_blank\">logical-capybara-3</a></strong> to <a href=\"https://wandb.ai/paufeldman/autoencoder4\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/paufeldman/autoencoder4/runs/1004sk1x?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2485b323970>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 8000\n",
    "learning_rate = 1e-5\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "#opt = torch.optim.Adam(params, lr=learning_rate, weight_decay=0.0001) \n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "#opt = torch.optim.SGD(params, lr=learning_rate, momentum = 0.96) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[100], gamma=0.2)\n",
    "import wandb\n",
    "config = {\n",
    "  \"learning_rate\": learning_rate,\n",
    "  \"epochs\": epochs,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"dataset\": t_list,\n",
    "  \"number of trees\": len(data_loader)*batch_size,\n",
    "  \"optim\": opt\n",
    "}\n",
    "wandb.init(project=\"autoencoder4\", entity=\"paufeldman\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 8000] average reconstruction error: 0.32504568 \n",
      "Epoch [11 / 8000] average reconstruction error: 0.31498209 \n",
      "Epoch [21 / 8000] average reconstruction error: 0.30499563 \n",
      "Epoch [31 / 8000] average reconstruction error: 0.29470032 \n",
      "Epoch [41 / 8000] average reconstruction error: 0.28363237 \n",
      "Epoch [51 / 8000] average reconstruction error: 0.27124572 \n",
      "Epoch [61 / 8000] average reconstruction error: 0.25684214 \n",
      "Epoch [71 / 8000] average reconstruction error: 0.23949979 \n",
      "Epoch [81 / 8000] average reconstruction error: 0.21784483 \n",
      "Epoch [91 / 8000] average reconstruction error: 0.18999055 \n",
      "Epoch [101 / 8000] average reconstruction error: 0.15421091 \n",
      "Epoch [111 / 8000] average reconstruction error: 0.11239133 \n",
      "Epoch [121 / 8000] average reconstruction error: 0.07491596 \n",
      "Epoch [131 / 8000] average reconstruction error: 0.05398503 \n",
      "Epoch [141 / 8000] average reconstruction error: 0.04587218 \n",
      "Epoch [151 / 8000] average reconstruction error: 0.04145901 \n",
      "Epoch [161 / 8000] average reconstruction error: 0.03859184 \n",
      "Epoch [171 / 8000] average reconstruction error: 0.03662423 \n",
      "Epoch [181 / 8000] average reconstruction error: 0.03506552 \n",
      "Epoch [191 / 8000] average reconstruction error: 0.03374450 \n",
      "Epoch [201 / 8000] average reconstruction error: 0.03255309 \n",
      "Epoch [211 / 8000] average reconstruction error: 0.03142260 \n",
      "Epoch [221 / 8000] average reconstruction error: 0.03031224 \n",
      "Epoch [231 / 8000] average reconstruction error: 0.02920876 \n",
      "Epoch [241 / 8000] average reconstruction error: 0.02811119 \n",
      "Epoch [251 / 8000] average reconstruction error: 0.02703707 \n",
      "Epoch [261 / 8000] average reconstruction error: 0.02601311 \n",
      "Epoch [271 / 8000] average reconstruction error: 0.02506701 \n",
      "Epoch [281 / 8000] average reconstruction error: 0.02421653 \n",
      "Epoch [291 / 8000] average reconstruction error: 0.02347375 \n",
      "Epoch [301 / 8000] average reconstruction error: 0.02283272 \n",
      "Epoch [311 / 8000] average reconstruction error: 0.02228381 \n",
      "Epoch [321 / 8000] average reconstruction error: 0.02180881 \n",
      "Epoch [331 / 8000] average reconstruction error: 0.02139016 \n",
      "Epoch [341 / 8000] average reconstruction error: 0.02101242 \n",
      "Epoch [351 / 8000] average reconstruction error: 0.02065794 \n",
      "Epoch [361 / 8000] average reconstruction error: 0.02031991 \n",
      "Epoch [371 / 8000] average reconstruction error: 0.01998282 \n",
      "Epoch [381 / 8000] average reconstruction error: 0.01963805 \n",
      "Epoch [391 / 8000] average reconstruction error: 0.01927583 \n",
      "Epoch [401 / 8000] average reconstruction error: 0.01888832 \n",
      "Epoch [411 / 8000] average reconstruction error: 0.01846539 \n",
      "Epoch [421 / 8000] average reconstruction error: 0.01799644 \n",
      "Epoch [431 / 8000] average reconstruction error: 0.01747170 \n",
      "Epoch [441 / 8000] average reconstruction error: 0.01688744 \n",
      "Epoch [451 / 8000] average reconstruction error: 0.01623731 \n",
      "Epoch [461 / 8000] average reconstruction error: 0.01552540 \n",
      "Epoch [471 / 8000] average reconstruction error: 0.01476385 \n",
      "Epoch [481 / 8000] average reconstruction error: 0.01397310 \n",
      "Epoch [491 / 8000] average reconstruction error: 0.01317738 \n",
      "Epoch [501 / 8000] average reconstruction error: 0.01239517 \n",
      "Epoch [511 / 8000] average reconstruction error: 0.01163989 \n",
      "Epoch [521 / 8000] average reconstruction error: 0.01091423 \n",
      "Epoch [531 / 8000] average reconstruction error: 0.01021793 \n",
      "Epoch [541 / 8000] average reconstruction error: 0.00955129 \n",
      "Epoch [551 / 8000] average reconstruction error: 0.00891479 \n",
      "Epoch [561 / 8000] average reconstruction error: 0.00831134 \n",
      "Epoch [571 / 8000] average reconstruction error: 0.00774715 \n",
      "Epoch [581 / 8000] average reconstruction error: 0.00722600 \n",
      "Epoch [591 / 8000] average reconstruction error: 0.00675425 \n",
      "Epoch [601 / 8000] average reconstruction error: 0.00632863 \n",
      "Epoch [611 / 8000] average reconstruction error: 0.00594783 \n",
      "Epoch [621 / 8000] average reconstruction error: 0.00560319 \n",
      "Epoch [631 / 8000] average reconstruction error: 0.00528957 \n",
      "Epoch [641 / 8000] average reconstruction error: 0.00499774 \n",
      "Epoch [651 / 8000] average reconstruction error: 0.00472487 \n",
      "Epoch [661 / 8000] average reconstruction error: 0.00446830 \n",
      "Epoch [671 / 8000] average reconstruction error: 0.00422379 \n",
      "Epoch [681 / 8000] average reconstruction error: 0.00399218 \n",
      "Epoch [691 / 8000] average reconstruction error: 0.00377138 \n",
      "Epoch [701 / 8000] average reconstruction error: 0.00356066 \n",
      "Epoch [711 / 8000] average reconstruction error: 0.00335958 \n",
      "Epoch [721 / 8000] average reconstruction error: 0.00316874 \n",
      "Epoch [731 / 8000] average reconstruction error: 0.00298752 \n",
      "Epoch [741 / 8000] average reconstruction error: 0.00281651 \n",
      "Epoch [751 / 8000] average reconstruction error: 0.00265451 \n",
      "Epoch [761 / 8000] average reconstruction error: 0.00250300 \n",
      "Epoch [771 / 8000] average reconstruction error: 0.00236182 \n",
      "Epoch [781 / 8000] average reconstruction error: 0.00223100 \n",
      "Epoch [791 / 8000] average reconstruction error: 0.00211205 \n",
      "Epoch [801 / 8000] average reconstruction error: 0.00200336 \n",
      "Epoch [811 / 8000] average reconstruction error: 0.00190682 \n",
      "Epoch [821 / 8000] average reconstruction error: 0.00182090 \n",
      "Epoch [831 / 8000] average reconstruction error: 0.00174582 \n",
      "Epoch [841 / 8000] average reconstruction error: 0.00167956 \n",
      "Epoch [851 / 8000] average reconstruction error: 0.00162185 \n",
      "Epoch [861 / 8000] average reconstruction error: 0.00157084 \n",
      "Epoch [871 / 8000] average reconstruction error: 0.00152588 \n",
      "Epoch [881 / 8000] average reconstruction error: 0.00148537 \n",
      "Epoch [891 / 8000] average reconstruction error: 0.00144946 \n",
      "Epoch [901 / 8000] average reconstruction error: 0.00141663 \n",
      "Epoch [911 / 8000] average reconstruction error: 0.00138686 \n",
      "Epoch [921 / 8000] average reconstruction error: 0.00135846 \n",
      "Epoch [931 / 8000] average reconstruction error: 0.00133181 \n",
      "Epoch [941 / 8000] average reconstruction error: 0.00130829 \n",
      "Epoch [951 / 8000] average reconstruction error: 0.00128515 \n",
      "Epoch [961 / 8000] average reconstruction error: 0.00126394 \n",
      "Epoch [971 / 8000] average reconstruction error: 0.00124328 \n",
      "Epoch [981 / 8000] average reconstruction error: 0.00122321 \n",
      "Epoch [991 / 8000] average reconstruction error: 0.00120441 \n",
      "Epoch [1001 / 8000] average reconstruction error: 0.00118627 \n",
      "Epoch [1011 / 8000] average reconstruction error: 0.00116910 \n",
      "Epoch [1021 / 8000] average reconstruction error: 0.00115184 \n",
      "Epoch [1031 / 8000] average reconstruction error: 0.00113558 \n",
      "Epoch [1041 / 8000] average reconstruction error: 0.00112011 \n",
      "Epoch [1051 / 8000] average reconstruction error: 0.00110480 \n",
      "Epoch [1061 / 8000] average reconstruction error: 0.00108956 \n",
      "Epoch [1071 / 8000] average reconstruction error: 0.00107525 \n",
      "Epoch [1081 / 8000] average reconstruction error: 0.00106085 \n",
      "Epoch [1091 / 8000] average reconstruction error: 0.00104712 \n",
      "Epoch [1101 / 8000] average reconstruction error: 0.00103375 \n",
      "Epoch [1111 / 8000] average reconstruction error: 0.00102072 \n",
      "Epoch [1121 / 8000] average reconstruction error: 0.00100793 \n",
      "Epoch [1131 / 8000] average reconstruction error: 0.00099563 \n",
      "Epoch [1141 / 8000] average reconstruction error: 0.00098278 \n",
      "Epoch [1151 / 8000] average reconstruction error: 0.00097124 \n",
      "Epoch [1161 / 8000] average reconstruction error: 0.00096018 \n",
      "Epoch [1171 / 8000] average reconstruction error: 0.00094896 \n",
      "Epoch [1181 / 8000] average reconstruction error: 0.00093850 \n",
      "Epoch [1191 / 8000] average reconstruction error: 0.00092775 \n",
      "Epoch [1201 / 8000] average reconstruction error: 0.00091736 \n",
      "Epoch [1211 / 8000] average reconstruction error: 0.00090756 \n",
      "Epoch [1221 / 8000] average reconstruction error: 0.00089818 \n",
      "Epoch [1231 / 8000] average reconstruction error: 0.00088910 \n",
      "Epoch [1241 / 8000] average reconstruction error: 0.00087996 \n",
      "Epoch [1251 / 8000] average reconstruction error: 0.00087090 \n",
      "Epoch [1261 / 8000] average reconstruction error: 0.00086210 \n",
      "Epoch [1271 / 8000] average reconstruction error: 0.00085354 \n",
      "Epoch [1281 / 8000] average reconstruction error: 0.00084544 \n",
      "Epoch [1291 / 8000] average reconstruction error: 0.00083710 \n",
      "Epoch [1301 / 8000] average reconstruction error: 0.00082851 \n",
      "Epoch [1311 / 8000] average reconstruction error: 0.00082048 \n",
      "Epoch [1321 / 8000] average reconstruction error: 0.00081237 \n",
      "Epoch [1331 / 8000] average reconstruction error: 0.00080450 \n",
      "Epoch [1341 / 8000] average reconstruction error: 0.00079623 \n",
      "Epoch [1351 / 8000] average reconstruction error: 0.00078797 \n",
      "Epoch [1361 / 8000] average reconstruction error: 0.00078040 \n",
      "Epoch [1371 / 8000] average reconstruction error: 0.00077222 \n",
      "Epoch [1381 / 8000] average reconstruction error: 0.00076387 \n",
      "Epoch [1391 / 8000] average reconstruction error: 0.00075600 \n",
      "Epoch [1401 / 8000] average reconstruction error: 0.00074732 \n",
      "Epoch [1411 / 8000] average reconstruction error: 0.00073879 \n",
      "Epoch [1421 / 8000] average reconstruction error: 0.00073074 \n",
      "Epoch [1431 / 8000] average reconstruction error: 0.00072223 \n",
      "Epoch [1441 / 8000] average reconstruction error: 0.00071329 \n",
      "Epoch [1451 / 8000] average reconstruction error: 0.00070458 \n",
      "Epoch [1461 / 8000] average reconstruction error: 0.00069575 \n",
      "Epoch [1471 / 8000] average reconstruction error: 0.00068637 \n",
      "Epoch [1481 / 8000] average reconstruction error: 0.00067763 \n",
      "Epoch [1491 / 8000] average reconstruction error: 0.00066831 \n",
      "Epoch [1501 / 8000] average reconstruction error: 0.00065923 \n",
      "Epoch [1511 / 8000] average reconstruction error: 0.00064989 \n",
      "Epoch [1521 / 8000] average reconstruction error: 0.00064053 \n",
      "Epoch [1531 / 8000] average reconstruction error: 0.00063117 \n",
      "Epoch [1541 / 8000] average reconstruction error: 0.00062178 \n",
      "Epoch [1551 / 8000] average reconstruction error: 0.00061226 \n",
      "Epoch [1561 / 8000] average reconstruction error: 0.00060312 \n",
      "Epoch [1571 / 8000] average reconstruction error: 0.00059405 \n",
      "Epoch [1581 / 8000] average reconstruction error: 0.00058465 \n",
      "Epoch [1591 / 8000] average reconstruction error: 0.00057569 \n",
      "Epoch [1601 / 8000] average reconstruction error: 0.00056641 \n",
      "Epoch [1611 / 8000] average reconstruction error: 0.00055772 \n",
      "Epoch [1621 / 8000] average reconstruction error: 0.00054848 \n",
      "Epoch [1631 / 8000] average reconstruction error: 0.00053938 \n",
      "Epoch [1641 / 8000] average reconstruction error: 0.00053037 \n",
      "Epoch [1651 / 8000] average reconstruction error: 0.00052134 \n",
      "Epoch [1661 / 8000] average reconstruction error: 0.00051238 \n",
      "Epoch [1671 / 8000] average reconstruction error: 0.00050287 \n",
      "Epoch [1681 / 8000] average reconstruction error: 0.00049346 \n",
      "Epoch [1691 / 8000] average reconstruction error: 0.00048400 \n",
      "Epoch [1701 / 8000] average reconstruction error: 0.00047444 \n",
      "Epoch [1711 / 8000] average reconstruction error: 0.00046450 \n",
      "Epoch [1721 / 8000] average reconstruction error: 0.00045476 \n",
      "Epoch [1731 / 8000] average reconstruction error: 0.00044451 \n",
      "Epoch [1741 / 8000] average reconstruction error: 0.00043386 \n",
      "Epoch [1751 / 8000] average reconstruction error: 0.00042338 \n",
      "Epoch [1761 / 8000] average reconstruction error: 0.00041260 \n",
      "Epoch [1771 / 8000] average reconstruction error: 0.00040179 \n",
      "Epoch [1781 / 8000] average reconstruction error: 0.00039005 \n",
      "Epoch [1791 / 8000] average reconstruction error: 0.00037869 \n",
      "Epoch [1801 / 8000] average reconstruction error: 0.00036717 \n",
      "Epoch [1811 / 8000] average reconstruction error: 0.00035506 \n",
      "Epoch [1821 / 8000] average reconstruction error: 0.00034303 \n",
      "Epoch [1831 / 8000] average reconstruction error: 0.00033100 \n",
      "Epoch [1841 / 8000] average reconstruction error: 0.00031852 \n",
      "Epoch [1851 / 8000] average reconstruction error: 0.00030632 \n",
      "Epoch [1861 / 8000] average reconstruction error: 0.00029410 \n",
      "Epoch [1871 / 8000] average reconstruction error: 0.00028173 \n",
      "Epoch [1881 / 8000] average reconstruction error: 0.00026941 \n",
      "Epoch [1891 / 8000] average reconstruction error: 0.00025737 \n",
      "Epoch [1901 / 8000] average reconstruction error: 0.00024558 \n",
      "Epoch [1911 / 8000] average reconstruction error: 0.00023399 \n",
      "Epoch [1921 / 8000] average reconstruction error: 0.00022290 \n",
      "Epoch [1931 / 8000] average reconstruction error: 0.00021231 \n",
      "Epoch [1941 / 8000] average reconstruction error: 0.00020191 \n",
      "Epoch [1951 / 8000] average reconstruction error: 0.00019207 \n",
      "Epoch [1961 / 8000] average reconstruction error: 0.00018282 \n",
      "Epoch [1971 / 8000] average reconstruction error: 0.00017388 \n",
      "Epoch [1981 / 8000] average reconstruction error: 0.00016553 \n",
      "Epoch [1991 / 8000] average reconstruction error: 0.00015778 \n",
      "Epoch [2001 / 8000] average reconstruction error: 0.00015043 \n",
      "Epoch [2011 / 8000] average reconstruction error: 0.00014349 \n",
      "Epoch [2021 / 8000] average reconstruction error: 0.00013710 \n",
      "Epoch [2031 / 8000] average reconstruction error: 0.00013110 \n",
      "Epoch [2041 / 8000] average reconstruction error: 0.00012531 \n",
      "Epoch [2051 / 8000] average reconstruction error: 0.00012008 \n",
      "Epoch [2061 / 8000] average reconstruction error: 0.00011507 \n",
      "Epoch [2071 / 8000] average reconstruction error: 0.00011025 \n",
      "Epoch [2081 / 8000] average reconstruction error: 0.00010583 \n",
      "Epoch [2091 / 8000] average reconstruction error: 0.00010161 \n",
      "Epoch [2101 / 8000] average reconstruction error: 0.00009775 \n",
      "Epoch [2111 / 8000] average reconstruction error: 0.00009387 \n",
      "Epoch [2121 / 8000] average reconstruction error: 0.00009055 \n",
      "Epoch [2131 / 8000] average reconstruction error: 0.00008718 \n",
      "Epoch [2141 / 8000] average reconstruction error: 0.00008406 \n",
      "Epoch [2151 / 8000] average reconstruction error: 0.00008109 \n",
      "Epoch [2161 / 8000] average reconstruction error: 0.00007837 \n",
      "Epoch [2171 / 8000] average reconstruction error: 0.00007568 \n",
      "Epoch [2181 / 8000] average reconstruction error: 0.00007329 \n",
      "Epoch [2191 / 8000] average reconstruction error: 0.00007102 \n",
      "Epoch [2201 / 8000] average reconstruction error: 0.00006887 \n",
      "Epoch [2211 / 8000] average reconstruction error: 0.00006680 \n",
      "Epoch [2221 / 8000] average reconstruction error: 0.00006488 \n",
      "Epoch [2231 / 8000] average reconstruction error: 0.00006309 \n",
      "Epoch [2241 / 8000] average reconstruction error: 0.00006151 \n",
      "Epoch [2251 / 8000] average reconstruction error: 0.00005984 \n",
      "Epoch [2261 / 8000] average reconstruction error: 0.00005840 \n",
      "Epoch [2271 / 8000] average reconstruction error: 0.00005708 \n",
      "Epoch [2281 / 8000] average reconstruction error: 0.00005577 \n",
      "Epoch [2291 / 8000] average reconstruction error: 0.00005454 \n",
      "Epoch [2301 / 8000] average reconstruction error: 0.00005343 \n",
      "Epoch [2311 / 8000] average reconstruction error: 0.00005236 \n",
      "Epoch [2321 / 8000] average reconstruction error: 0.00005124 \n",
      "Epoch [2331 / 8000] average reconstruction error: 0.00005032 \n",
      "Epoch [2341 / 8000] average reconstruction error: 0.00004950 \n",
      "Epoch [2351 / 8000] average reconstruction error: 0.00004865 \n",
      "Epoch [2361 / 8000] average reconstruction error: 0.00004780 \n",
      "Epoch [2371 / 8000] average reconstruction error: 0.00004703 \n",
      "Epoch [2381 / 8000] average reconstruction error: 0.00004630 \n",
      "Epoch [2391 / 8000] average reconstruction error: 0.00004557 \n",
      "Epoch [2401 / 8000] average reconstruction error: 0.00004494 \n",
      "Epoch [2411 / 8000] average reconstruction error: 0.00004428 \n",
      "Epoch [2421 / 8000] average reconstruction error: 0.00004365 \n",
      "Epoch [2431 / 8000] average reconstruction error: 0.00004307 \n",
      "Epoch [2441 / 8000] average reconstruction error: 0.00004252 \n",
      "Epoch [2451 / 8000] average reconstruction error: 0.00004190 \n",
      "Epoch [2461 / 8000] average reconstruction error: 0.00004138 \n",
      "Epoch [2471 / 8000] average reconstruction error: 0.00004085 \n",
      "Epoch [2481 / 8000] average reconstruction error: 0.00004035 \n",
      "Epoch [2491 / 8000] average reconstruction error: 0.00003977 \n",
      "Epoch [2501 / 8000] average reconstruction error: 0.00003934 \n",
      "Epoch [2511 / 8000] average reconstruction error: 0.00003890 \n",
      "Epoch [2521 / 8000] average reconstruction error: 0.00003845 \n",
      "Epoch [2531 / 8000] average reconstruction error: 0.00003793 \n",
      "Epoch [2541 / 8000] average reconstruction error: 0.00003753 \n",
      "Epoch [2551 / 8000] average reconstruction error: 0.00003709 \n",
      "Epoch [2561 / 8000] average reconstruction error: 0.00003669 \n",
      "Epoch [2571 / 8000] average reconstruction error: 0.00003622 \n",
      "Epoch [2581 / 8000] average reconstruction error: 0.00003577 \n",
      "Epoch [2591 / 8000] average reconstruction error: 0.00003544 \n",
      "Epoch [2601 / 8000] average reconstruction error: 0.00003498 \n",
      "Epoch [2611 / 8000] average reconstruction error: 0.00003464 \n",
      "Epoch [2621 / 8000] average reconstruction error: 0.00003427 \n",
      "Epoch [2631 / 8000] average reconstruction error: 0.00003390 \n",
      "Epoch [2641 / 8000] average reconstruction error: 0.00003355 \n",
      "Epoch [2651 / 8000] average reconstruction error: 0.00003318 \n",
      "Epoch [2661 / 8000] average reconstruction error: 0.00003285 \n",
      "Epoch [2671 / 8000] average reconstruction error: 0.00003241 \n",
      "Epoch [2681 / 8000] average reconstruction error: 0.00003212 \n",
      "Epoch [2691 / 8000] average reconstruction error: 0.00003173 \n",
      "Epoch [2701 / 8000] average reconstruction error: 0.00003141 \n",
      "Epoch [2711 / 8000] average reconstruction error: 0.00003113 \n",
      "Epoch [2721 / 8000] average reconstruction error: 0.00003078 \n",
      "Epoch [2731 / 8000] average reconstruction error: 0.00003041 \n",
      "Epoch [2741 / 8000] average reconstruction error: 0.00003016 \n",
      "Epoch [2751 / 8000] average reconstruction error: 0.00002980 \n",
      "Epoch [2761 / 8000] average reconstruction error: 0.00002954 \n",
      "Epoch [2771 / 8000] average reconstruction error: 0.00002916 \n",
      "Epoch [2781 / 8000] average reconstruction error: 0.00002886 \n",
      "Epoch [2791 / 8000] average reconstruction error: 0.00002861 \n",
      "Epoch [2801 / 8000] average reconstruction error: 0.00002833 \n",
      "Epoch [2811 / 8000] average reconstruction error: 0.00002805 \n",
      "Epoch [2821 / 8000] average reconstruction error: 0.00002771 \n",
      "Epoch [2831 / 8000] average reconstruction error: 0.00002738 \n",
      "Epoch [2841 / 8000] average reconstruction error: 0.00002716 \n",
      "Epoch [2851 / 8000] average reconstruction error: 0.00002684 \n",
      "Epoch [2861 / 8000] average reconstruction error: 0.00002658 \n",
      "Epoch [2871 / 8000] average reconstruction error: 0.00002629 \n",
      "Epoch [2881 / 8000] average reconstruction error: 0.00002601 \n",
      "Epoch [2891 / 8000] average reconstruction error: 0.00002580 \n",
      "Epoch [2901 / 8000] average reconstruction error: 0.00002552 \n",
      "Epoch [2911 / 8000] average reconstruction error: 0.00002526 \n",
      "Epoch [2921 / 8000] average reconstruction error: 0.00002500 \n",
      "Epoch [2931 / 8000] average reconstruction error: 0.00002474 \n",
      "Epoch [2941 / 8000] average reconstruction error: 0.00002451 \n",
      "Epoch [2951 / 8000] average reconstruction error: 0.00002426 \n",
      "Epoch [2961 / 8000] average reconstruction error: 0.00002399 \n",
      "Epoch [2971 / 8000] average reconstruction error: 0.00002376 \n",
      "Epoch [2981 / 8000] average reconstruction error: 0.00002352 \n",
      "Epoch [2991 / 8000] average reconstruction error: 0.00002329 \n",
      "Epoch [3001 / 8000] average reconstruction error: 0.00002303 \n",
      "Epoch [3011 / 8000] average reconstruction error: 0.00002282 \n",
      "Epoch [3021 / 8000] average reconstruction error: 0.00002259 \n",
      "Epoch [3031 / 8000] average reconstruction error: 0.00002238 \n",
      "Epoch [3041 / 8000] average reconstruction error: 0.00002212 \n",
      "Epoch [3051 / 8000] average reconstruction error: 0.00002192 \n",
      "Epoch [3061 / 8000] average reconstruction error: 0.00002165 \n",
      "Epoch [3071 / 8000] average reconstruction error: 0.00002149 \n",
      "Epoch [3081 / 8000] average reconstruction error: 0.00002124 \n",
      "Epoch [3091 / 8000] average reconstruction error: 0.00002103 \n",
      "Epoch [3101 / 8000] average reconstruction error: 0.00002081 \n",
      "Epoch [3111 / 8000] average reconstruction error: 0.00002061 \n",
      "Epoch [3121 / 8000] average reconstruction error: 0.00002044 \n",
      "Epoch [3131 / 8000] average reconstruction error: 0.00002018 \n",
      "Epoch [3141 / 8000] average reconstruction error: 0.00002000 \n",
      "Epoch [3151 / 8000] average reconstruction error: 0.00001984 \n",
      "Epoch [3161 / 8000] average reconstruction error: 0.00001962 \n",
      "Epoch [3171 / 8000] average reconstruction error: 0.00001945 \n",
      "Epoch [3181 / 8000] average reconstruction error: 0.00001927 \n",
      "Epoch [3191 / 8000] average reconstruction error: 0.00001907 \n",
      "Epoch [3201 / 8000] average reconstruction error: 0.00001887 \n",
      "Epoch [3211 / 8000] average reconstruction error: 0.00001869 \n",
      "Epoch [3221 / 8000] average reconstruction error: 0.00001853 \n",
      "Epoch [3231 / 8000] average reconstruction error: 0.00001831 \n",
      "Epoch [3241 / 8000] average reconstruction error: 0.00001814 \n",
      "Epoch [3251 / 8000] average reconstruction error: 0.00001798 \n",
      "Epoch [3261 / 8000] average reconstruction error: 0.00001779 \n",
      "Epoch [3271 / 8000] average reconstruction error: 0.00001762 \n",
      "Epoch [3281 / 8000] average reconstruction error: 0.00001743 \n",
      "Epoch [3291 / 8000] average reconstruction error: 0.00001729 \n",
      "Epoch [3301 / 8000] average reconstruction error: 0.00001713 \n",
      "Epoch [3311 / 8000] average reconstruction error: 0.00001694 \n",
      "Epoch [3321 / 8000] average reconstruction error: 0.00001678 \n",
      "Epoch [3331 / 8000] average reconstruction error: 0.00001666 \n",
      "Epoch [3341 / 8000] average reconstruction error: 0.00001648 \n",
      "Epoch [3351 / 8000] average reconstruction error: 0.00001633 \n",
      "Epoch [3361 / 8000] average reconstruction error: 0.00001625 \n",
      "Epoch [3371 / 8000] average reconstruction error: 0.00001606 \n",
      "Epoch [3381 / 8000] average reconstruction error: 0.00001588 \n",
      "Epoch [3391 / 8000] average reconstruction error: 0.00001577 \n",
      "Epoch [3401 / 8000] average reconstruction error: 0.00001561 \n",
      "Epoch [3411 / 8000] average reconstruction error: 0.00001550 \n",
      "Epoch [3421 / 8000] average reconstruction error: 0.00001532 \n",
      "Epoch [3431 / 8000] average reconstruction error: 0.00001524 \n",
      "Epoch [3441 / 8000] average reconstruction error: 0.00001507 \n",
      "Epoch [3451 / 8000] average reconstruction error: 0.00001494 \n",
      "Epoch [3461 / 8000] average reconstruction error: 0.00001483 \n",
      "Epoch [3471 / 8000] average reconstruction error: 0.00001466 \n",
      "Epoch [3481 / 8000] average reconstruction error: 0.00001454 \n",
      "Epoch [3491 / 8000] average reconstruction error: 0.00001444 \n",
      "Epoch [3501 / 8000] average reconstruction error: 0.00001432 \n",
      "Epoch [3511 / 8000] average reconstruction error: 0.00001427 \n",
      "Epoch [3521 / 8000] average reconstruction error: 0.00001409 \n",
      "Epoch [3531 / 8000] average reconstruction error: 0.00001394 \n",
      "Epoch [3541 / 8000] average reconstruction error: 0.00001389 \n",
      "Epoch [3551 / 8000] average reconstruction error: 0.00001384 \n",
      "Epoch [3561 / 8000] average reconstruction error: 0.00001360 \n",
      "Epoch [3571 / 8000] average reconstruction error: 0.00001350 \n",
      "Epoch [3581 / 8000] average reconstruction error: 0.00001338 \n",
      "Epoch [3591 / 8000] average reconstruction error: 0.00001328 \n",
      "Epoch [3601 / 8000] average reconstruction error: 0.00001315 \n",
      "Epoch [3611 / 8000] average reconstruction error: 0.00001305 \n",
      "Epoch [3621 / 8000] average reconstruction error: 0.00001296 \n",
      "Epoch [3631 / 8000] average reconstruction error: 0.00001285 \n",
      "Epoch [3641 / 8000] average reconstruction error: 0.00001279 \n",
      "Epoch [3651 / 8000] average reconstruction error: 0.00001270 \n",
      "Epoch [3661 / 8000] average reconstruction error: 0.00001254 \n",
      "Epoch [3671 / 8000] average reconstruction error: 0.00001249 \n",
      "Epoch [3681 / 8000] average reconstruction error: 0.00001243 \n",
      "Epoch [3691 / 8000] average reconstruction error: 0.00001228 \n",
      "Epoch [3701 / 8000] average reconstruction error: 0.00001228 \n",
      "Epoch [3711 / 8000] average reconstruction error: 0.00001211 \n",
      "Epoch [3721 / 8000] average reconstruction error: 0.00001207 \n",
      "Epoch [3731 / 8000] average reconstruction error: 0.00001192 \n",
      "Epoch [3741 / 8000] average reconstruction error: 0.00001186 \n",
      "Epoch [3751 / 8000] average reconstruction error: 0.00001175 \n",
      "Epoch [3761 / 8000] average reconstruction error: 0.00001169 \n",
      "Epoch [3771 / 8000] average reconstruction error: 0.00001166 \n",
      "Epoch [3781 / 8000] average reconstruction error: 0.00001154 \n",
      "Epoch [3791 / 8000] average reconstruction error: 0.00001144 \n",
      "Epoch [3801 / 8000] average reconstruction error: 0.00001137 \n",
      "Epoch [3811 / 8000] average reconstruction error: 0.00001129 \n",
      "Epoch [3821 / 8000] average reconstruction error: 0.00001122 \n",
      "Epoch [3831 / 8000] average reconstruction error: 0.00001116 \n",
      "Epoch [3841 / 8000] average reconstruction error: 0.00001118 \n",
      "Epoch [3851 / 8000] average reconstruction error: 0.00001100 \n",
      "Epoch [3861 / 8000] average reconstruction error: 0.00001096 \n",
      "Epoch [3871 / 8000] average reconstruction error: 0.00001122 \n",
      "Epoch [3881 / 8000] average reconstruction error: 0.00001093 \n",
      "Epoch [3891 / 8000] average reconstruction error: 0.00001079 \n",
      "Epoch [3901 / 8000] average reconstruction error: 0.00001066 \n",
      "Epoch [3911 / 8000] average reconstruction error: 0.00001066 \n",
      "Epoch [3921 / 8000] average reconstruction error: 0.00001055 \n",
      "Epoch [3931 / 8000] average reconstruction error: 0.00001049 \n",
      "Epoch [3941 / 8000] average reconstruction error: 0.00001041 \n",
      "Epoch [3951 / 8000] average reconstruction error: 0.00001037 \n",
      "Epoch [3961 / 8000] average reconstruction error: 0.00001030 \n",
      "Epoch [3971 / 8000] average reconstruction error: 0.00001024 \n",
      "Epoch [3981 / 8000] average reconstruction error: 0.00001020 \n",
      "Epoch [3991 / 8000] average reconstruction error: 0.00001013 \n",
      "Epoch [4001 / 8000] average reconstruction error: 0.00001005 \n",
      "Epoch [4011 / 8000] average reconstruction error: 0.00001000 \n",
      "Epoch [4021 / 8000] average reconstruction error: 0.00000995 \n",
      "Epoch [4031 / 8000] average reconstruction error: 0.00000988 \n",
      "Epoch [4041 / 8000] average reconstruction error: 0.00000984 \n",
      "Epoch [4051 / 8000] average reconstruction error: 0.00000984 \n",
      "Epoch [4061 / 8000] average reconstruction error: 0.00000973 \n",
      "Epoch [4071 / 8000] average reconstruction error: 0.00000974 \n",
      "Epoch [4081 / 8000] average reconstruction error: 0.00000960 \n",
      "Epoch [4091 / 8000] average reconstruction error: 0.00000965 \n",
      "Epoch [4101 / 8000] average reconstruction error: 0.00000954 \n",
      "Epoch [4111 / 8000] average reconstruction error: 0.00000959 \n",
      "Epoch [4121 / 8000] average reconstruction error: 0.00000943 \n",
      "Epoch [4131 / 8000] average reconstruction error: 0.00000948 \n",
      "Epoch [4141 / 8000] average reconstruction error: 0.00000930 \n",
      "Epoch [4151 / 8000] average reconstruction error: 0.00000929 \n",
      "Epoch [4161 / 8000] average reconstruction error: 0.00000921 \n",
      "Epoch [4171 / 8000] average reconstruction error: 0.00000916 \n",
      "Epoch [4181 / 8000] average reconstruction error: 0.00000912 \n",
      "Epoch [4191 / 8000] average reconstruction error: 0.00000908 \n",
      "Epoch [4201 / 8000] average reconstruction error: 0.00000907 \n",
      "Epoch [4211 / 8000] average reconstruction error: 0.00000909 \n",
      "Epoch [4221 / 8000] average reconstruction error: 0.00000895 \n",
      "Epoch [4231 / 8000] average reconstruction error: 0.00000897 \n",
      "Epoch [4241 / 8000] average reconstruction error: 0.00000893 \n",
      "Epoch [4251 / 8000] average reconstruction error: 0.00000883 \n",
      "Epoch [4261 / 8000] average reconstruction error: 0.00000877 \n",
      "Epoch [4271 / 8000] average reconstruction error: 0.00000878 \n",
      "Epoch [4281 / 8000] average reconstruction error: 0.00000873 \n",
      "Epoch [4291 / 8000] average reconstruction error: 0.00000869 \n",
      "Epoch [4301 / 8000] average reconstruction error: 0.00000862 \n",
      "Epoch [4311 / 8000] average reconstruction error: 0.00000862 \n",
      "Epoch [4321 / 8000] average reconstruction error: 0.00000867 \n",
      "Epoch [4331 / 8000] average reconstruction error: 0.00000857 \n",
      "Epoch [4341 / 8000] average reconstruction error: 0.00000846 \n",
      "Epoch [4351 / 8000] average reconstruction error: 0.00000849 \n",
      "Epoch [4361 / 8000] average reconstruction error: 0.00000839 \n",
      "Epoch [4371 / 8000] average reconstruction error: 0.00000833 \n",
      "Epoch [4381 / 8000] average reconstruction error: 0.00000828 \n",
      "Epoch [4391 / 8000] average reconstruction error: 0.00000827 \n",
      "Epoch [4401 / 8000] average reconstruction error: 0.00000825 \n",
      "Epoch [4411 / 8000] average reconstruction error: 0.00000832 \n",
      "Epoch [4421 / 8000] average reconstruction error: 0.00000840 \n",
      "Epoch [4431 / 8000] average reconstruction error: 0.00000812 \n",
      "Epoch [4441 / 8000] average reconstruction error: 0.00000806 \n",
      "Epoch [4451 / 8000] average reconstruction error: 0.00000808 \n",
      "Epoch [4461 / 8000] average reconstruction error: 0.00000800 \n",
      "Epoch [4471 / 8000] average reconstruction error: 0.00000800 \n",
      "Epoch [4481 / 8000] average reconstruction error: 0.00000807 \n",
      "Epoch [4491 / 8000] average reconstruction error: 0.00000795 \n",
      "Epoch [4501 / 8000] average reconstruction error: 0.00000785 \n",
      "Epoch [4511 / 8000] average reconstruction error: 0.00000789 \n",
      "Epoch [4521 / 8000] average reconstruction error: 0.00000778 \n",
      "Epoch [4531 / 8000] average reconstruction error: 0.00000778 \n",
      "Epoch [4541 / 8000] average reconstruction error: 0.00000771 \n",
      "Epoch [4551 / 8000] average reconstruction error: 0.00000778 \n",
      "Epoch [4561 / 8000] average reconstruction error: 0.00000764 \n",
      "Epoch [4571 / 8000] average reconstruction error: 0.00000772 \n",
      "Epoch [4581 / 8000] average reconstruction error: 0.00000760 \n",
      "Epoch [4591 / 8000] average reconstruction error: 0.00000760 \n",
      "Epoch [4601 / 8000] average reconstruction error: 0.00000757 \n",
      "Epoch [4611 / 8000] average reconstruction error: 0.00000767 \n",
      "Epoch [4621 / 8000] average reconstruction error: 0.00000746 \n",
      "Epoch [4631 / 8000] average reconstruction error: 0.00000741 \n",
      "Epoch [4641 / 8000] average reconstruction error: 0.00000747 \n",
      "Epoch [4651 / 8000] average reconstruction error: 0.00000739 \n",
      "Epoch [4661 / 8000] average reconstruction error: 0.00000759 \n",
      "Epoch [4671 / 8000] average reconstruction error: 0.00000728 \n",
      "Epoch [4681 / 8000] average reconstruction error: 0.00000725 \n",
      "Epoch [4691 / 8000] average reconstruction error: 0.00000721 \n",
      "Epoch [4701 / 8000] average reconstruction error: 0.00000728 \n",
      "Epoch [4711 / 8000] average reconstruction error: 0.00000718 \n",
      "Epoch [4721 / 8000] average reconstruction error: 0.00000711 \n",
      "Epoch [4731 / 8000] average reconstruction error: 0.00000712 \n",
      "Epoch [4741 / 8000] average reconstruction error: 0.00000707 \n",
      "Epoch [4751 / 8000] average reconstruction error: 0.00000708 \n",
      "Epoch [4761 / 8000] average reconstruction error: 0.00000701 \n",
      "Epoch [4771 / 8000] average reconstruction error: 0.00000695 \n",
      "Epoch [4781 / 8000] average reconstruction error: 0.00000696 \n",
      "Epoch [4791 / 8000] average reconstruction error: 0.00000704 \n",
      "Epoch [4801 / 8000] average reconstruction error: 0.00000712 \n",
      "Epoch [4811 / 8000] average reconstruction error: 0.00000692 \n",
      "Epoch [4821 / 8000] average reconstruction error: 0.00000683 \n",
      "Epoch [4831 / 8000] average reconstruction error: 0.00000690 \n",
      "Epoch [4841 / 8000] average reconstruction error: 0.00000693 \n",
      "Epoch [4851 / 8000] average reconstruction error: 0.00000690 \n",
      "Epoch [4861 / 8000] average reconstruction error: 0.00000668 \n",
      "Epoch [4871 / 8000] average reconstruction error: 0.00000674 \n",
      "Epoch [4881 / 8000] average reconstruction error: 0.00000665 \n",
      "Epoch [4891 / 8000] average reconstruction error: 0.00000663 \n",
      "Epoch [4901 / 8000] average reconstruction error: 0.00000658 \n",
      "Epoch [4911 / 8000] average reconstruction error: 0.00000658 \n",
      "Epoch [4921 / 8000] average reconstruction error: 0.00000653 \n",
      "Epoch [4931 / 8000] average reconstruction error: 0.00000651 \n",
      "Epoch [4941 / 8000] average reconstruction error: 0.00000645 \n",
      "Epoch [4951 / 8000] average reconstruction error: 0.00000646 \n",
      "Epoch [4961 / 8000] average reconstruction error: 0.00000640 \n",
      "Epoch [4971 / 8000] average reconstruction error: 0.00000664 \n",
      "Epoch [4981 / 8000] average reconstruction error: 0.00000641 \n",
      "Epoch [4991 / 8000] average reconstruction error: 0.00000633 \n",
      "Epoch [5001 / 8000] average reconstruction error: 0.00000632 \n",
      "Epoch [5011 / 8000] average reconstruction error: 0.00000632 \n",
      "Epoch [5021 / 8000] average reconstruction error: 0.00000624 \n",
      "Epoch [5031 / 8000] average reconstruction error: 0.00000636 \n",
      "Epoch [5041 / 8000] average reconstruction error: 0.00000627 \n",
      "Epoch [5051 / 8000] average reconstruction error: 0.00000613 \n",
      "Epoch [5061 / 8000] average reconstruction error: 0.00000610 \n",
      "Epoch [5071 / 8000] average reconstruction error: 0.00000610 \n",
      "Epoch [5081 / 8000] average reconstruction error: 0.00000605 \n",
      "Epoch [5091 / 8000] average reconstruction error: 0.00000601 \n",
      "Epoch [5101 / 8000] average reconstruction error: 0.00000602 \n",
      "Epoch [5111 / 8000] average reconstruction error: 0.00000608 \n",
      "Epoch [5121 / 8000] average reconstruction error: 0.00000596 \n",
      "Epoch [5131 / 8000] average reconstruction error: 0.00000593 \n",
      "Epoch [5141 / 8000] average reconstruction error: 0.00000589 \n",
      "Epoch [5151 / 8000] average reconstruction error: 0.00000590 \n",
      "Epoch [5161 / 8000] average reconstruction error: 0.00000588 \n",
      "Epoch [5171 / 8000] average reconstruction error: 0.00000588 \n",
      "Epoch [5181 / 8000] average reconstruction error: 0.00000590 \n",
      "Epoch [5191 / 8000] average reconstruction error: 0.00000575 \n",
      "Epoch [5201 / 8000] average reconstruction error: 0.00000573 \n",
      "Epoch [5211 / 8000] average reconstruction error: 0.00000569 \n",
      "Epoch [5221 / 8000] average reconstruction error: 0.00000573 \n",
      "Epoch [5231 / 8000] average reconstruction error: 0.00000563 \n",
      "Epoch [5241 / 8000] average reconstruction error: 0.00000592 \n",
      "Epoch [5251 / 8000] average reconstruction error: 0.00000561 \n",
      "Epoch [5261 / 8000] average reconstruction error: 0.00000559 \n",
      "Epoch [5271 / 8000] average reconstruction error: 0.00000563 \n",
      "Epoch [5281 / 8000] average reconstruction error: 0.00000584 \n",
      "Epoch [5291 / 8000] average reconstruction error: 0.00000549 \n",
      "Epoch [5301 / 8000] average reconstruction error: 0.00000550 \n",
      "Epoch [5311 / 8000] average reconstruction error: 0.00000556 \n",
      "Epoch [5321 / 8000] average reconstruction error: 0.00000538 \n",
      "Epoch [5331 / 8000] average reconstruction error: 0.00000539 \n",
      "Epoch [5341 / 8000] average reconstruction error: 0.00000537 \n",
      "Epoch [5351 / 8000] average reconstruction error: 0.00000540 \n",
      "Epoch [5361 / 8000] average reconstruction error: 0.00000528 \n",
      "Epoch [5371 / 8000] average reconstruction error: 0.00000523 \n",
      "Epoch [5381 / 8000] average reconstruction error: 0.00000532 \n",
      "Epoch [5391 / 8000] average reconstruction error: 0.00000528 \n",
      "Epoch [5401 / 8000] average reconstruction error: 0.00000519 \n",
      "Epoch [5411 / 8000] average reconstruction error: 0.00000531 \n",
      "Epoch [5421 / 8000] average reconstruction error: 0.00000511 \n",
      "Epoch [5431 / 8000] average reconstruction error: 0.00000520 \n",
      "Epoch [5441 / 8000] average reconstruction error: 0.00000516 \n",
      "Epoch [5451 / 8000] average reconstruction error: 0.00000503 \n",
      "Epoch [5461 / 8000] average reconstruction error: 0.00000501 \n",
      "Epoch [5471 / 8000] average reconstruction error: 0.00000512 \n",
      "Epoch [5481 / 8000] average reconstruction error: 0.00000503 \n",
      "Epoch [5491 / 8000] average reconstruction error: 0.00000532 \n",
      "Epoch [5501 / 8000] average reconstruction error: 0.00000499 \n",
      "Epoch [5511 / 8000] average reconstruction error: 0.00000499 \n",
      "Epoch [5521 / 8000] average reconstruction error: 0.00000505 \n",
      "Epoch [5531 / 8000] average reconstruction error: 0.00000483 \n",
      "Epoch [5541 / 8000] average reconstruction error: 0.00000490 \n",
      "Epoch [5551 / 8000] average reconstruction error: 0.00000480 \n",
      "Epoch [5561 / 8000] average reconstruction error: 0.00000481 \n",
      "Epoch [5571 / 8000] average reconstruction error: 0.00000473 \n",
      "Epoch [5581 / 8000] average reconstruction error: 0.00000481 \n",
      "Epoch [5591 / 8000] average reconstruction error: 0.00000477 \n",
      "Epoch [5601 / 8000] average reconstruction error: 0.00000462 \n",
      "Epoch [5611 / 8000] average reconstruction error: 0.00000458 \n",
      "Epoch [5621 / 8000] average reconstruction error: 0.00000456 \n",
      "Epoch [5631 / 8000] average reconstruction error: 0.00000458 \n",
      "Epoch [5641 / 8000] average reconstruction error: 0.00000469 \n",
      "Epoch [5651 / 8000] average reconstruction error: 0.00000447 \n",
      "Epoch [5661 / 8000] average reconstruction error: 0.00000450 \n",
      "Epoch [5671 / 8000] average reconstruction error: 0.00000448 \n",
      "Epoch [5681 / 8000] average reconstruction error: 0.00000456 \n",
      "Epoch [5691 / 8000] average reconstruction error: 0.00000438 \n",
      "Epoch [5701 / 8000] average reconstruction error: 0.00000441 \n",
      "Epoch [5711 / 8000] average reconstruction error: 0.00000459 \n",
      "Epoch [5721 / 8000] average reconstruction error: 0.00000437 \n",
      "Epoch [5731 / 8000] average reconstruction error: 0.00000433 \n",
      "Epoch [5741 / 8000] average reconstruction error: 0.00000425 \n",
      "Epoch [5751 / 8000] average reconstruction error: 0.00000430 \n",
      "Epoch [5761 / 8000] average reconstruction error: 0.00000420 \n",
      "Epoch [5771 / 8000] average reconstruction error: 0.00000422 \n",
      "Epoch [5781 / 8000] average reconstruction error: 0.00000422 \n",
      "Epoch [5791 / 8000] average reconstruction error: 0.00000414 \n",
      "Epoch [5801 / 8000] average reconstruction error: 0.00000413 \n",
      "Epoch [5811 / 8000] average reconstruction error: 0.00000404 \n",
      "Epoch [5821 / 8000] average reconstruction error: 0.00000408 \n",
      "Epoch [5831 / 8000] average reconstruction error: 0.00000401 \n",
      "Epoch [5841 / 8000] average reconstruction error: 0.00000406 \n",
      "Epoch [5851 / 8000] average reconstruction error: 0.00000420 \n",
      "Epoch [5861 / 8000] average reconstruction error: 0.00000411 \n",
      "Epoch [5871 / 8000] average reconstruction error: 0.00000399 \n",
      "Epoch [5881 / 8000] average reconstruction error: 0.00000392 \n",
      "Epoch [5891 / 8000] average reconstruction error: 0.00000389 \n",
      "Epoch [5901 / 8000] average reconstruction error: 0.00000385 \n",
      "Epoch [5911 / 8000] average reconstruction error: 0.00000383 \n",
      "Epoch [5921 / 8000] average reconstruction error: 0.00000379 \n",
      "Epoch [5931 / 8000] average reconstruction error: 0.00000386 \n",
      "Epoch [5941 / 8000] average reconstruction error: 0.00000393 \n",
      "Epoch [5951 / 8000] average reconstruction error: 0.00000375 \n",
      "Epoch [5961 / 8000] average reconstruction error: 0.00000369 \n",
      "Epoch [5971 / 8000] average reconstruction error: 0.00000369 \n",
      "Epoch [5981 / 8000] average reconstruction error: 0.00000359 \n",
      "Epoch [5991 / 8000] average reconstruction error: 0.00000363 \n",
      "Epoch [6001 / 8000] average reconstruction error: 0.00000368 \n",
      "Epoch [6011 / 8000] average reconstruction error: 0.00000352 \n",
      "Epoch [6021 / 8000] average reconstruction error: 0.00000375 \n",
      "Epoch [6031 / 8000] average reconstruction error: 0.00000366 \n",
      "Epoch [6041 / 8000] average reconstruction error: 0.00000354 \n",
      "Epoch [6051 / 8000] average reconstruction error: 0.00000341 \n",
      "Epoch [6061 / 8000] average reconstruction error: 0.00000345 \n",
      "Epoch [6071 / 8000] average reconstruction error: 0.00000340 \n",
      "Epoch [6081 / 8000] average reconstruction error: 0.00000338 \n",
      "Epoch [6091 / 8000] average reconstruction error: 0.00000344 \n",
      "Epoch [6101 / 8000] average reconstruction error: 0.00000336 \n",
      "Epoch [6111 / 8000] average reconstruction error: 0.00000327 \n",
      "Epoch [6121 / 8000] average reconstruction error: 0.00000325 \n",
      "Epoch [6131 / 8000] average reconstruction error: 0.00000330 \n",
      "Epoch [6141 / 8000] average reconstruction error: 0.00000351 \n",
      "Epoch [6151 / 8000] average reconstruction error: 0.00000376 \n",
      "Epoch [6161 / 8000] average reconstruction error: 0.00000328 \n",
      "Epoch [6171 / 8000] average reconstruction error: 0.00000318 \n",
      "Epoch [6181 / 8000] average reconstruction error: 0.00000319 \n",
      "Epoch [6191 / 8000] average reconstruction error: 0.00000305 \n",
      "Epoch [6201 / 8000] average reconstruction error: 0.00000307 \n",
      "Epoch [6211 / 8000] average reconstruction error: 0.00000301 \n",
      "Epoch [6221 / 8000] average reconstruction error: 0.00000316 \n",
      "Epoch [6231 / 8000] average reconstruction error: 0.00000298 \n",
      "Epoch [6241 / 8000] average reconstruction error: 0.00000292 \n",
      "Epoch [6251 / 8000] average reconstruction error: 0.00000305 \n",
      "Epoch [6261 / 8000] average reconstruction error: 0.00000288 \n",
      "Epoch [6271 / 8000] average reconstruction error: 0.00000286 \n",
      "Epoch [6281 / 8000] average reconstruction error: 0.00000286 \n",
      "Epoch [6291 / 8000] average reconstruction error: 0.00000330 \n",
      "Epoch [6301 / 8000] average reconstruction error: 0.00000279 \n",
      "Epoch [6311 / 8000] average reconstruction error: 0.00000321 \n",
      "Epoch [6321 / 8000] average reconstruction error: 0.00000295 \n",
      "Epoch [6331 / 8000] average reconstruction error: 0.00000274 \n",
      "Epoch [6341 / 8000] average reconstruction error: 0.00000278 \n",
      "Epoch [6351 / 8000] average reconstruction error: 0.00000287 \n",
      "Epoch [6361 / 8000] average reconstruction error: 0.00000262 \n",
      "Epoch [6371 / 8000] average reconstruction error: 0.00000261 \n",
      "Epoch [6381 / 8000] average reconstruction error: 0.00000263 \n",
      "Epoch [6391 / 8000] average reconstruction error: 0.00000350 \n",
      "Epoch [6401 / 8000] average reconstruction error: 0.00000285 \n",
      "Epoch [6411 / 8000] average reconstruction error: 0.00000264 \n",
      "Epoch [6421 / 8000] average reconstruction error: 0.00000257 \n",
      "Epoch [6431 / 8000] average reconstruction error: 0.00000251 \n",
      "Epoch [6441 / 8000] average reconstruction error: 0.00000249 \n",
      "Epoch [6451 / 8000] average reconstruction error: 0.00000243 \n",
      "Epoch [6461 / 8000] average reconstruction error: 0.00000244 \n",
      "Epoch [6471 / 8000] average reconstruction error: 0.00000242 \n",
      "Epoch [6481 / 8000] average reconstruction error: 0.00000239 \n",
      "Epoch [6491 / 8000] average reconstruction error: 0.00000236 \n",
      "Epoch [6501 / 8000] average reconstruction error: 0.00000232 \n",
      "Epoch [6511 / 8000] average reconstruction error: 0.00000234 \n",
      "Epoch [6521 / 8000] average reconstruction error: 0.00000231 \n",
      "Epoch [6531 / 8000] average reconstruction error: 0.00000232 \n",
      "Epoch [6541 / 8000] average reconstruction error: 0.00000227 \n",
      "Epoch [6551 / 8000] average reconstruction error: 0.00000226 \n",
      "Epoch [6561 / 8000] average reconstruction error: 0.00000218 \n",
      "Epoch [6571 / 8000] average reconstruction error: 0.00000225 \n",
      "Epoch [6581 / 8000] average reconstruction error: 0.00000214 \n",
      "Epoch [6591 / 8000] average reconstruction error: 0.00000245 \n",
      "Epoch [6601 / 8000] average reconstruction error: 0.00000208 \n",
      "Epoch [6611 / 8000] average reconstruction error: 0.00000227 \n",
      "Epoch [6621 / 8000] average reconstruction error: 0.00000215 \n",
      "Epoch [6631 / 8000] average reconstruction error: 0.00000221 \n",
      "Epoch [6641 / 8000] average reconstruction error: 0.00000202 \n",
      "Epoch [6651 / 8000] average reconstruction error: 0.00000199 \n",
      "Epoch [6661 / 8000] average reconstruction error: 0.00000224 \n",
      "Epoch [6671 / 8000] average reconstruction error: 0.00000202 \n",
      "Epoch [6681 / 8000] average reconstruction error: 0.00000196 \n",
      "Epoch [6691 / 8000] average reconstruction error: 0.00000323 \n",
      "Epoch [6701 / 8000] average reconstruction error: 0.00000239 \n",
      "Epoch [6711 / 8000] average reconstruction error: 0.00000207 \n",
      "Epoch [6721 / 8000] average reconstruction error: 0.00000187 \n",
      "Epoch [6731 / 8000] average reconstruction error: 0.00000195 \n",
      "Epoch [6741 / 8000] average reconstruction error: 0.00000200 \n",
      "Epoch [6751 / 8000] average reconstruction error: 0.00000214 \n",
      "Epoch [6761 / 8000] average reconstruction error: 0.00000203 \n",
      "Epoch [6771 / 8000] average reconstruction error: 0.00000183 \n",
      "Epoch [6781 / 8000] average reconstruction error: 0.00000173 \n",
      "Epoch [6791 / 8000] average reconstruction error: 0.00000181 \n",
      "Epoch [6801 / 8000] average reconstruction error: 0.00000170 \n",
      "Epoch [6811 / 8000] average reconstruction error: 0.00000168 \n",
      "Epoch [6821 / 8000] average reconstruction error: 0.00000169 \n",
      "Epoch [6831 / 8000] average reconstruction error: 0.00000177 \n",
      "Epoch [6841 / 8000] average reconstruction error: 0.00000190 \n",
      "Epoch [6851 / 8000] average reconstruction error: 0.00000178 \n",
      "Epoch [6861 / 8000] average reconstruction error: 0.00000163 \n",
      "Epoch [6871 / 8000] average reconstruction error: 0.00000160 \n",
      "Epoch [6881 / 8000] average reconstruction error: 0.00000180 \n",
      "Epoch [6891 / 8000] average reconstruction error: 0.00000276 \n",
      "Epoch [6901 / 8000] average reconstruction error: 0.00000153 \n",
      "Epoch [6911 / 8000] average reconstruction error: 0.00000150 \n",
      "Epoch [6921 / 8000] average reconstruction error: 0.00000169 \n",
      "Epoch [6931 / 8000] average reconstruction error: 0.00000162 \n",
      "Epoch [6941 / 8000] average reconstruction error: 0.00000191 \n",
      "Epoch [6951 / 8000] average reconstruction error: 0.00000197 \n",
      "Epoch [6961 / 8000] average reconstruction error: 0.00000166 \n",
      "Epoch [6971 / 8000] average reconstruction error: 0.00000152 \n",
      "Epoch [6981 / 8000] average reconstruction error: 0.00000141 \n",
      "Epoch [6991 / 8000] average reconstruction error: 0.00000142 \n",
      "Epoch [7001 / 8000] average reconstruction error: 0.00000156 \n",
      "Epoch [7011 / 8000] average reconstruction error: 0.00000143 \n",
      "Epoch [7021 / 8000] average reconstruction error: 0.00000140 \n",
      "Epoch [7031 / 8000] average reconstruction error: 0.00000134 \n",
      "Epoch [7041 / 8000] average reconstruction error: 0.00000139 \n",
      "Epoch [7051 / 8000] average reconstruction error: 0.00000146 \n",
      "Epoch [7061 / 8000] average reconstruction error: 0.00000171 \n",
      "Epoch [7071 / 8000] average reconstruction error: 0.00000131 \n",
      "Epoch [7081 / 8000] average reconstruction error: 0.00000131 \n",
      "Epoch [7091 / 8000] average reconstruction error: 0.00000125 \n",
      "Epoch [7101 / 8000] average reconstruction error: 0.00000129 \n",
      "Epoch [7111 / 8000] average reconstruction error: 0.00000124 \n",
      "Epoch [7121 / 8000] average reconstruction error: 0.00000150 \n",
      "Epoch [7131 / 8000] average reconstruction error: 0.00000135 \n",
      "Epoch [7141 / 8000] average reconstruction error: 0.00000132 \n",
      "Epoch [7151 / 8000] average reconstruction error: 0.00000119 \n",
      "Epoch [7161 / 8000] average reconstruction error: 0.00000121 \n",
      "Epoch [7171 / 8000] average reconstruction error: 0.00000127 \n",
      "Epoch [7181 / 8000] average reconstruction error: 0.00000125 \n",
      "Epoch [7191 / 8000] average reconstruction error: 0.00000135 \n",
      "Epoch [7201 / 8000] average reconstruction error: 0.00000130 \n",
      "Epoch [7211 / 8000] average reconstruction error: 0.00000115 \n",
      "Epoch [7221 / 8000] average reconstruction error: 0.00000129 \n",
      "Epoch [7231 / 8000] average reconstruction error: 0.00000123 \n",
      "Epoch [7241 / 8000] average reconstruction error: 0.00000151 \n",
      "Epoch [7251 / 8000] average reconstruction error: 0.00000118 \n",
      "Epoch [7261 / 8000] average reconstruction error: 0.00000125 \n",
      "Epoch [7271 / 8000] average reconstruction error: 0.00000116 \n",
      "Epoch [7281 / 8000] average reconstruction error: 0.00000188 \n",
      "Epoch [7291 / 8000] average reconstruction error: 0.00000113 \n",
      "Epoch [7301 / 8000] average reconstruction error: 0.00000109 \n",
      "Epoch [7311 / 8000] average reconstruction error: 0.00000103 \n",
      "Epoch [7321 / 8000] average reconstruction error: 0.00000124 \n",
      "Epoch [7331 / 8000] average reconstruction error: 0.00000107 \n",
      "Epoch [7341 / 8000] average reconstruction error: 0.00000105 \n",
      "Epoch [7351 / 8000] average reconstruction error: 0.00000106 \n",
      "Epoch [7361 / 8000] average reconstruction error: 0.00000116 \n",
      "Epoch [7371 / 8000] average reconstruction error: 0.00000102 \n",
      "Epoch [7381 / 8000] average reconstruction error: 0.00000153 \n",
      "Epoch [7391 / 8000] average reconstruction error: 0.00000115 \n",
      "Epoch [7401 / 8000] average reconstruction error: 0.00000141 \n",
      "Epoch [7411 / 8000] average reconstruction error: 0.00000100 \n",
      "Epoch [7421 / 8000] average reconstruction error: 0.00000089 \n",
      "Epoch [7431 / 8000] average reconstruction error: 0.00000097 \n",
      "Epoch [7441 / 8000] average reconstruction error: 0.00000135 \n",
      "Epoch [7451 / 8000] average reconstruction error: 0.00000131 \n",
      "Epoch [7461 / 8000] average reconstruction error: 0.00000102 \n",
      "Epoch [7471 / 8000] average reconstruction error: 0.00000102 \n",
      "Epoch [7481 / 8000] average reconstruction error: 0.00000086 \n",
      "Epoch [7491 / 8000] average reconstruction error: 0.00000090 \n",
      "Epoch [7501 / 8000] average reconstruction error: 0.00000096 \n",
      "Epoch [7511 / 8000] average reconstruction error: 0.00000099 \n",
      "Epoch [7521 / 8000] average reconstruction error: 0.00000105 \n",
      "Epoch [7531 / 8000] average reconstruction error: 0.00000082 \n",
      "Epoch [7541 / 8000] average reconstruction error: 0.00000102 \n",
      "Epoch [7551 / 8000] average reconstruction error: 0.00000128 \n",
      "Epoch [7561 / 8000] average reconstruction error: 0.00000081 \n",
      "Epoch [7571 / 8000] average reconstruction error: 0.00000094 \n",
      "Epoch [7581 / 8000] average reconstruction error: 0.00000079 \n",
      "Epoch [7591 / 8000] average reconstruction error: 0.00000089 \n",
      "Epoch [7601 / 8000] average reconstruction error: 0.00000076 \n",
      "Epoch [7611 / 8000] average reconstruction error: 0.00000079 \n",
      "Epoch [7621 / 8000] average reconstruction error: 0.00000080 \n",
      "Epoch [7631 / 8000] average reconstruction error: 0.00000098 \n",
      "Epoch [7641 / 8000] average reconstruction error: 0.00000099 \n",
      "Epoch [7651 / 8000] average reconstruction error: 0.00000100 \n",
      "Epoch [7661 / 8000] average reconstruction error: 0.00000209 \n",
      "Epoch [7671 / 8000] average reconstruction error: 0.00000092 \n",
      "Epoch [7681 / 8000] average reconstruction error: 0.00000081 \n",
      "Epoch [7691 / 8000] average reconstruction error: 0.00000079 \n",
      "Epoch [7701 / 8000] average reconstruction error: 0.00000083 \n",
      "Epoch [7711 / 8000] average reconstruction error: 0.00000076 \n",
      "Epoch [7721 / 8000] average reconstruction error: 0.00000073 \n",
      "Epoch [7731 / 8000] average reconstruction error: 0.00000068 \n",
      "Epoch [7741 / 8000] average reconstruction error: 0.00000078 \n",
      "Epoch [7751 / 8000] average reconstruction error: 0.00000135 \n",
      "Epoch [7761 / 8000] average reconstruction error: 0.00000093 \n",
      "Epoch [7771 / 8000] average reconstruction error: 0.00000075 \n",
      "Epoch [7781 / 8000] average reconstruction error: 0.00000066 \n",
      "Epoch [7791 / 8000] average reconstruction error: 0.00000082 \n",
      "Epoch [7801 / 8000] average reconstruction error: 0.00000070 \n",
      "Epoch [7811 / 8000] average reconstruction error: 0.00000105 \n",
      "Epoch [7821 / 8000] average reconstruction error: 0.00000076 \n",
      "Epoch [7831 / 8000] average reconstruction error: 0.00000073 \n",
      "Epoch [7841 / 8000] average reconstruction error: 0.00000065 \n",
      "Epoch [7851 / 8000] average reconstruction error: 0.00000078 \n",
      "Epoch [7861 / 8000] average reconstruction error: 0.00000074 \n",
      "Epoch [7871 / 8000] average reconstruction error: 0.00000061 \n",
      "Epoch [7881 / 8000] average reconstruction error: 0.00000060 \n",
      "Epoch [7891 / 8000] average reconstruction error: 0.00000075 \n",
      "Epoch [7901 / 8000] average reconstruction error: 0.00000103 \n",
      "Epoch [7911 / 8000] average reconstruction error: 0.00000066 \n",
      "Epoch [7921 / 8000] average reconstruction error: 0.00000058 \n",
      "Epoch [7931 / 8000] average reconstruction error: 0.00000062 \n",
      "Epoch [7941 / 8000] average reconstruction error: 0.00000086 \n",
      "Epoch [7951 / 8000] average reconstruction error: 0.00000056 \n",
      "Epoch [7961 / 8000] average reconstruction error: 0.00000080 \n",
      "Epoch [7971 / 8000] average reconstruction error: 0.00000065 \n",
      "Epoch [7981 / 8000] average reconstruction error: 0.00000078 \n",
      "Epoch [7991 / 8000] average reconstruction error: 0.00000114 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss_avg = []\n",
    "\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        enc_fold = torch_f.Fold(device)\n",
    "        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "        # Collect computation nodes recursively from encoding process\n",
    "        n_nodes = []\n",
    "        for example in batch: #example es un arbolito\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            encode_structure_fold(enc_fold, example)\n",
    "            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "       \n",
    "        # Apply the computations on the encoder model\n",
    "       \n",
    "        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "        \n",
    "        \n",
    "        # Initialize torchfold for *decoding*\n",
    "        dec_fold = torch_f.Fold(device)\n",
    "        # Collect computation nodes recursively from decoding process\n",
    "        dec_fold_nodes = []\n",
    "        kld_fold_nodes = []\n",
    "\n",
    "        t_l = []\n",
    "        for f in enc_fold_nodes:\n",
    "            for t in f:\n",
    "                t_l.append(t)\n",
    "        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\n",
    "            #print(\"example\", example)\n",
    "            #print(\"fnode\", fnode) \n",
    "            #root_code, kl_div = torch.chunk(fnode, 2, 0)\n",
    "            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\n",
    "        # Apply the computations on the decoder model\n",
    "\n",
    "                       \n",
    "        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        #print(\"n\", n_nodes)\n",
    "        total_loss = torch.div(total_loss[0], n_nodes)\n",
    "        #print(\"div\", total_loss)\n",
    "        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        total_loss = total_loss#*10\n",
    "        \n",
    "        #print(\"total_loss\", total_loss)\n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %.8f ' % (epoch+1, epochs, total_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder con batch - decoder sin batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        # Initialize torchfold for *encoding*\\n\\n        \\n        enc_fold = torch_f.Fold(device)\\n        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\\n        # Collect computation nodes recursively from encoding process\\n        n_nodes = []\\n        for example in batch: #example es un arbolito\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            encode_structure_fold(enc_fold, example)\\n            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\\n       \\n        # Apply the computations on the encoder model\\n       \\n        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\\n        encodeado_con_batch = enc_fold_nodes\\n        \\n        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\\n        #print(\"decoded\", decoded)\\n        l = []\\n        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\\n        l = []\\n        ce_loss_list = decoded.traverseInorderCE(decoded, l)\\n            \\n        mse_loss = sum(mse_loss_list) \\n        ce_loss  = sum(ce_loss_list)  \\n        total_loss = (0.5*ce_loss + mse_loss)\\n        #print(\"total_loss\", total_loss)\\n        total_loss = total_loss / len(mse_loss_list)\\n        \\n        \\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        enc_fold = torch_f.Fold(device)\n",
    "        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "        # Collect computation nodes recursively from encoding process\n",
    "        n_nodes = []\n",
    "        for example in batch: #example es un arbolito\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            encode_structure_fold(enc_fold, example)\n",
    "            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "       \n",
    "        # Apply the computations on the encoder model\n",
    "       \n",
    "        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "        encodeado_con_batch = enc_fold_nodes\n",
    "        \n",
    "        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\n",
    "        #print(\"decoded\", decoded)\n",
    "        l = []\n",
    "        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\n",
    "        l = []\n",
    "        ce_loss_list = decoded.traverseInorderCE(decoded, l)\n",
    "            \n",
    "        mse_loss = sum(mse_loss_list) \n",
    "        ce_loss  = sum(ce_loss_list)  \n",
    "        total_loss = (0.5*ce_loss + mse_loss)\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        total_loss = total_loss / len(mse_loss_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder sin batch - decoder con batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        # Initialize torchfold for *encoding*\\n\\n        \\n        \\n        enc_fold_nodes = []\\n        n_nodes = []\\n        for example in batch:\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            enc_fold = encode_structure(example).to(device)\\n        #print(\"encodeado sin batch\", enc_fold)\\n        enc_fold_nodes.append(enc_fold)\\n        encodeado_sin_batch = enc_fold\\n        # Split into a list of fold nodes per example\\n        #enc_fold_nodes = torch.split(enc_fold_nodes[0], 1, 0) #divide ele ncodeado en vectores de un elemento\\n        \\n        \\n        # Initialize torchfold for *decoding*\\n        dec_fold = torch_f.Fold(device)\\n        # Collect computation nodes recursively from decoding process\\n        dec_fold_nodes = []\\n        kld_fold_nodes = []\\n\\n        t_l = []\\n        for f in enc_fold_nodes:\\n            for t in f:\\n                t_l.append(t)\\n        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\\n            #print(\"example\", example)\\n            #print(\"fnode\", fnode) \\n            #root_code, kl_div = torch.chunk(fnode, 2, 0)\\n            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\\n        # Apply the computations on the decoder model\\n        #print(\"dec fold nodes\", dec_fold_nodes)\\n           \\n                       \\n        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\\n        #print(\"total_loss\", total_loss)\\n        n_nodes = torch.tensor(n_nodes, device = device)\\n        #print(\"n\", n_nodes)\\n        total_loss = torch.div(total_loss[0], n_nodes)\\n        #print(\"div\", total_loss)\\n        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\\n        #print(\"total_loss\", total_loss)\\n        \\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        mse_loss_avg[-1] += (mse_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        \n",
    "        enc_fold_nodes = []\n",
    "        n_nodes = []\n",
    "        for example in batch:\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            enc_fold = encode_structure(example).to(device)\n",
    "        #print(\"encodeado sin batch\", enc_fold)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "        encodeado_sin_batch = enc_fold\n",
    "        # Split into a list of fold nodes per example\n",
    "        #enc_fold_nodes = torch.split(enc_fold_nodes[0], 1, 0) #divide ele ncodeado en vectores de un elemento\n",
    "        \n",
    "        \n",
    "        # Initialize torchfold for *decoding*\n",
    "        dec_fold = torch_f.Fold(device)\n",
    "        # Collect computation nodes recursively from decoding process\n",
    "        dec_fold_nodes = []\n",
    "        kld_fold_nodes = []\n",
    "\n",
    "        t_l = []\n",
    "        for f in enc_fold_nodes:\n",
    "            for t in f:\n",
    "                t_l.append(t)\n",
    "        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\n",
    "            #print(\"example\", example)\n",
    "            #print(\"fnode\", fnode) \n",
    "            #root_code, kl_div = torch.chunk(fnode, 2, 0)\n",
    "            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\n",
    "        # Apply the computations on the decoder model\n",
    "        #print(\"dec fold nodes\", dec_fold_nodes)\n",
    "           \n",
    "                       \n",
    "        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        #print(\"n\", n_nodes)\n",
    "        total_loss = torch.div(total_loss[0], n_nodes)\n",
    "        #print(\"div\", total_loss)\n",
    "        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        mse_loss_avg[-1] += (mse_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder sin batch - decoder sin batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n    ce_avg.append(0)\\n    mse_avg.append(0)\\n\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        \\n        enc_fold_nodes = []\\n        n_nodes = []\\n        for example in batch:\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            enc_fold = encode_structure(example).to(device)\\n        #print(\"encodeado sin batch\", enc_fold)\\n        enc_fold_nodes.append(enc_fold)\\n        encodeado_sin_batch = enc_fold\\n        \\n        \\n        \\n        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\\n        #print(\"decoded\", decoded)\\n        l = []\\n        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\\n        l = []\\n        ce_loss_list = decoded.traverseInorderCE(decoded, l)\\n            \\n        mse_loss = sum(mse_loss_list) \\n        ce_loss  = sum(ce_loss_list)  \\n       \\n        ce = [0.4*a for a in ce_loss_list]\\n\\n\\n        total_loss = (0.4*ce_loss + mse_loss)\\n        total_loss = total_loss / len(mse_loss_list)\\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        mse_avg[-1] += (mse_loss.item())\\n        ce_avg[-1] += (ce_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss, \\'mse loss\\': mse_loss, \\'ce loss\\': ce_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "    ce_avg.append(0)\n",
    "    mse_avg.append(0)\n",
    "\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "        enc_fold_nodes = []\n",
    "        n_nodes = []\n",
    "        for example in batch:\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            enc_fold = encode_structure(example).to(device)\n",
    "        #print(\"encodeado sin batch\", enc_fold)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "        encodeado_sin_batch = enc_fold\n",
    "        \n",
    "        \n",
    "        \n",
    "        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\n",
    "        #print(\"decoded\", decoded)\n",
    "        l = []\n",
    "        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\n",
    "        l = []\n",
    "        ce_loss_list = decoded.traverseInorderCE(decoded, l)\n",
    "            \n",
    "        mse_loss = sum(mse_loss_list) \n",
    "        ce_loss  = sum(ce_loss_list)  \n",
    "       \n",
    "        ce = [0.4*a for a in ce_loss_list]\n",
    "\n",
    "\n",
    "        total_loss = (0.4*ce_loss + mse_loss)\n",
    "        total_loss = total_loss / len(mse_loss_list)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        mse_avg[-1] += (mse_loss.item())\n",
    "        ce_avg[-1] += (ce_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss, 'mse loss': mse_loss, 'ce loss': ce_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7987\n"
     ]
    }
   ],
   "source": [
    "encoder = GRASSEncoder(input_size = 4, feature_size=256, hidden_size=512).to(device)\n",
    "decoder = GRASSDecoder(latent_size=256, hidden_size=512, mult = mult).to(device)\n",
    "\n",
    "checkpoint = torch.load(\"outputs/best_model.pth\")\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "print(\"epoch\", epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4808e-07, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_best_model.best_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 49\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m dec \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m encoded \u001b[39min\u001b[39;00m enc_fold_nodes:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m    dec\u001b[39m.\u001b[39mappend(decode_testing_grass(encoded, \u001b[39minput\u001b[39;49m, \u001b[39m100\u001b[39;49m, decoder))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m total_loss \u001b[39m=\u001b[39m dec_fold\u001b[39m.\u001b[39mapply(decoder, [dec_fold_nodes])\u001b[39m#[0]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m n_nodes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(n_nodes, device \u001b[39m=\u001b[39m device)\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 49\u001b[0m in \u001b[0;36mdecode_testing_grass\u001b[1;34m(v, root, max, decoder)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m d\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m createNode\u001b[39m.\u001b[39mcount \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m dec \u001b[39m=\u001b[39m decode_node (v, root, \u001b[39mmax\u001b[39;49m, decoder)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dec\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 49\u001b[0m in \u001b[0;36mdecode_testing_grass.<locals>.decode_node\u001b[1;34m(v, node, max, decoder)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39melif\u001b[39;00m label \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m createNode\u001b[39m.\u001b[39mcount \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     left, right, radius \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39mbifurcationDecoder(v)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     lossEstructura \u001b[39m=\u001b[39m calcularLossEstructura(cl, node)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     lossAtrs \u001b[39m=\u001b[39m calcularLossAtributo( node, radius )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mif\u001b[39;00m lossEstructura \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 49\u001b[0m in \u001b[0;36mcalcularLossEstructura\u001b[1;34m(cl_p, original)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m original \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m mult \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39mround\u001b[39m(qzero),\u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39mround\u001b[39;49m(qOne),\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39mround\u001b[39m(qtwo)], device \u001b[39m=\u001b[39m device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ce \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss(weight \u001b[39m=\u001b[39m mult)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X64sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m original\u001b[39m.\u001b[39mchilds() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "train_loss_avg.append(0)\n",
    "for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "\n",
    "    enc_fold_nodes = []\n",
    "    n_nodes = []\n",
    "    for example in batch:\n",
    "        c = []\n",
    "        n = example.count_nodes(example, c)\n",
    "        n_nodes.append(len(n))\n",
    "        enc_fold = encode_structure(example).to(device)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "  \n",
    "    dec = []\n",
    "    for encoded in enc_fold_nodes:\n",
    "       dec.append(decode_testing_grass(encoded, input, 100, decoder))\n",
    "    total_loss = dec_fold.apply(decoder, [dec_fold_nodes])#[0]\n",
    "    n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        \n",
    "    total_loss = torch.div(total_loss[0], n_nodes)\n",
    "print(\"testing loss\", total_loss)\n",
    "avg_testing_loss = total_loss.sum() / len(batch) \n",
    "print(\"testing loss\", avg_testing_loss)\n",
    "\n",
    "        #total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 50\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m enc_fold_nodes \u001b[39m=\u001b[39m enc_fold\u001b[39m.\u001b[39mapply(encoder, [enc_fold_nodes])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m encoded \u001b[39m=\u001b[39m enc_fold_nodes[\u001b[39m0\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m decoded \u001b[39m=\u001b[39m decode_testing_grass(encoded, \u001b[39minput\u001b[39;49m, \u001b[39m100\u001b[39;49m, decoder)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m count \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m numerar_nodos(decoded, count)\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 50\u001b[0m in \u001b[0;36mdecode_testing_grass\u001b[1;34m(v, root, max, decoder)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m d\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m createNode\u001b[39m.\u001b[39mcount \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m dec \u001b[39m=\u001b[39m decode_node (v, root, \u001b[39mmax\u001b[39;49m, decoder)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dec\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 50\u001b[0m in \u001b[0;36mdecode_testing_grass.<locals>.decode_node\u001b[1;34m(v, node, max, decoder)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39melif\u001b[39;00m label \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m createNode\u001b[39m.\u001b[39mcount \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     left, right, radius \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39mbifurcationDecoder(v)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     lossEstructura \u001b[39m=\u001b[39m calcularLossEstructura(cl, node)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     lossAtrs \u001b[39m=\u001b[39m calcularLossAtributo( node, radius )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mif\u001b[39;00m lossEstructura \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 50\u001b[0m in \u001b[0;36mcalcularLossEstructura\u001b[1;34m(cl_p, original)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m original \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m mult \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39mround\u001b[39m(qzero),\u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39mround\u001b[39;49m(qOne),\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39mround\u001b[39m(qtwo)], device \u001b[39m=\u001b[39m device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ce \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss(weight \u001b[39m=\u001b[39m mult)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X65sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m original\u001b[39m.\u001b[39mchilds() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "input = iter(data_loader).next()[0]\n",
    "enc_fold = torch_f.Fold(device)\n",
    "enc_fold_nodes = []\n",
    "enc_fold_nodes.append(encode_structure_fold(enc_fold, input))\n",
    "enc_fold_nodes = enc_fold.apply(encoder, [enc_fold_nodes])\n",
    "encoded = enc_fold_nodes[0]\n",
    "decoded = decode_testing_grass(encoded, input, 100, decoder)\n",
    "\n",
    "count = []\n",
    "numerar_nodos(decoded, count)\n",
    "decoded.traverseInorder(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deab0e7956474d2193eb1d324a13d61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.5, 0.5,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTree(input, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ac5dcade5047b192b5ffc9eee14cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.4989887…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTree(decoded, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "n_nodes = input.count_nodes(input,c)\n",
    "len(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "n_nodes = decoded.count_nodes(decoded,c)\n",
    "len(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0 2\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "decoded.traverseInorderChilds(decoded, li)\n",
    "zero = [a for a in li if a == 0]\n",
    "one = [a for a in li if a == 1]\n",
    "two = [a for a in li if a == 2]\n",
    "qzero = len(zero)\n",
    "qOne = len(one)\n",
    "qtwo = len(two)\n",
    "print(qzero, qOne, qtwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0 2\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "input.traverseInorderChilds(input, li)\n",
    "zero = [a for a in li if a == 0]\n",
    "one = [a for a in li if a == 1]\n",
    "two = [a for a in li if a == 2]\n",
    "qzero = len(zero)\n",
    "qOne = len(one)\n",
    "qtwo = len(two)\n",
    "print(qzero, qOne, qtwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (20,10))\n",
    "plt.plot(train_loss_avg) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverseleaf(root):\n",
    "    if root is not None:\n",
    "        traverseleaf(root.left)\n",
    "        if root.is_leaf():\n",
    "            print(root.radius)\n",
    "        traverseleaf(root.right)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traversebif(root):\n",
    "    if root is not None:\n",
    "        traversebif(root.left)\n",
    "        if root.is_two_child():\n",
    "            print(root.radius)\n",
    "        traversebif(root.right)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0002, -0.0002, -0.0003, -0.0004]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4996, 0.4995, 0.4996, 0.4996]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "////\n",
      "tensor([0., 0., 0., 0.], device='cuda:0')\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "traversebif(decoded)\n",
    "print(\"////\")\n",
    "traversebif(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3999, 0.1251, 0.1419, 0.4994]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8998, 0.6250, 0.6429, 0.9998]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9982, 1.0001, 0.9994, 1.0011]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "traverseleaf(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8000, 0.2500, 0.2857, 1.0000], device='cuda:0')\n",
      "tensor([0.4000, 0.1250, 0.1429, 0.5000], device='cuda:0')\n",
      "tensor([0.9000, 0.6250, 0.6429, 1.0000], device='cuda:0')\n",
      "tensor([1., 1., 1., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "traverseleaf(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py_torc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f3e717cd274da89498094fde320e6eab1bf0f52911d27cf47473187acb3fe8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
