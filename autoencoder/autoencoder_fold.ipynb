{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import raiseExceptions\n",
    "from tokenize import Double\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from vec3 import Vec3\n",
    "import meshplot as mp\n",
    "import torch\n",
    "torch.manual_seed(125)\n",
    "import random\n",
    "random.seed(125)\n",
    "import torch_f as torch_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_fn(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        wrapper.count += 1\n",
    "        return f(*args, **kwargs)\n",
    "    wrapper.count = 0\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase nodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Class Node\n",
    "    \"\"\"\n",
    "    def __init__(self, value, radius, left = None, right = None, position = None, cl_prob= None, ce = None, mse = None, level = None, treelevel = None):\n",
    "        self.left = left\n",
    "        self.data = value\n",
    "        self.radius = radius\n",
    "        self.position = position\n",
    "        self.right = right\n",
    "        self.prob = cl_prob\n",
    "        self.mse = mse\n",
    "        self.ce = ce\n",
    "        self.children = [self.left, self.right]\n",
    "        self.level = level\n",
    "        self.treelevel = treelevel\n",
    "    \n",
    "    def agregarHijo(self, children):\n",
    "\n",
    "        if self.right is None:\n",
    "            self.right = children\n",
    "        elif self.left is None:\n",
    "            self.left = children\n",
    "\n",
    "        else:\n",
    "            raise ValueError (\"solo arbol binario \")\n",
    "\n",
    "\n",
    "    def is_leaf(self):\n",
    "        if self.right is None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_two_child(self):\n",
    "        if self.right is not None and self.left is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_one_child(self):\n",
    "        if self.is_two_child():\n",
    "            return False\n",
    "        elif self.is_leaf():\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def childs(self):\n",
    "        if self.is_leaf():\n",
    "            return 0\n",
    "        if self.is_one_child():\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    \n",
    "    \n",
    "    def traverseInorder(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorder(root.left)\n",
    "            print (root.data, root.radius)\n",
    "            self.traverseInorder(root.right)\n",
    "\n",
    "    def traverseInorderwl(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderwl(root.left)\n",
    "            print (root.data, root.radius, root.level, root.treelevel)\n",
    "            self.traverseInorderwl(root.right)\n",
    "\n",
    "    def get_tree_level(self, root, c):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.get_tree_level(root.left, c)\n",
    "            c.append(root.level)\n",
    "            self.get_tree_level(root.right, c)\n",
    "\n",
    "    def set_tree_level(self, root, c):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.set_tree_level(root.left, c)\n",
    "            root.treelevel = c\n",
    "            self.set_tree_level(root.right, c)\n",
    "\n",
    "    def traverseInorderLoss(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderLoss(root.left, loss)\n",
    "            loss.append(root.prob)\n",
    "            self.traverseInorderLoss(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderMSE(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderMSE(root.left, loss)\n",
    "            loss.append(root.mse)\n",
    "            self.traverseInorderMSE(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderCE(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderCE(root.left, loss)\n",
    "            loss.append(root.ce)\n",
    "            self.traverseInorderCE(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderChilds(self, root, l):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderChilds(root.left, l)\n",
    "            l.append(root.childs())\n",
    "            self.traverseInorderChilds(root.right, l)\n",
    "            return l\n",
    "\n",
    "    def preorder(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            print (root.data, root.radius)\n",
    "            self.preorder(root.left)\n",
    "            self.preorder(root.right)\n",
    "\n",
    "    def cloneBinaryTree(self, root):\n",
    "     \n",
    "        # base case\n",
    "        if root is None:\n",
    "            return None\n",
    "    \n",
    "        # create a new node with the same data as the root node\n",
    "        root_copy = Node(root.data, root.radius)\n",
    "    \n",
    "        # clone the left and right subtree\n",
    "        root_copy.left = self.cloneBinaryTree(root.left)\n",
    "        root_copy.right = self.cloneBinaryTree(root.right)\n",
    "    \n",
    "        # return cloned root node\n",
    "        return root_copy\n",
    "\n",
    "    def height(self, root):\n",
    "    # Check if the binary tree is empty\n",
    "        if root is None:\n",
    "            return 0 \n",
    "        # Recursively call height of each node\n",
    "        leftAns = self.height(root.left)\n",
    "        rightAns = self.height(root.right)\n",
    "    \n",
    "        # Return max(leftHeight, rightHeight) at each iteration\n",
    "        return max(leftAns, rightAns) + 1\n",
    "\n",
    "    # Print nodes at a current level\n",
    "    def printCurrentLevel(self, root, level):\n",
    "        if root is None:\n",
    "            return\n",
    "        if level == 1:\n",
    "            print(root.data, end=\" \")\n",
    "        elif level > 1:\n",
    "            self.printCurrentLevel(root.left, level-1)\n",
    "            self.printCurrentLevel(root.right, level-1)\n",
    "\n",
    "    def printLevelOrder(self, root):\n",
    "        h = self.height(root)\n",
    "        for i in range(1, h+1):\n",
    "            self.printCurrentLevel(root, i)\n",
    "\n",
    "\n",
    "    \n",
    "    def count_nodes(self, root, counter):\n",
    "        if   root is not None:\n",
    "            self.count_nodes(root.left, counter)\n",
    "            counter.append(root.data)\n",
    "            self.count_nodes(root.right, counter)\n",
    "            return counter\n",
    "\n",
    "    \n",
    "    def serialize(self, root):\n",
    "        def post_order(root):\n",
    "            if root:\n",
    "                post_order(root.left)\n",
    "                post_order(root.right)\n",
    "                ret[0] += str(root.data)+'_'+ str(root.radius) +';'\n",
    "                \n",
    "            else:\n",
    "                ret[0] += '#;'           \n",
    "\n",
    "        ret = ['']\n",
    "        post_order(root)\n",
    "        return ret[0][:-1]  # remove last ,\n",
    "\n",
    "    def toGraph( self, graph, index, dec, proc=True):\n",
    "        \n",
    "        \n",
    "        radius = self.radius.cpu().detach().numpy()\n",
    "        if dec:\n",
    "            radius= radius[0]\n",
    "        #print(\"posicion\", self.data, radius)\n",
    "        #print(\"right\", self.right)\n",
    "        \n",
    "        #graph.add_nodes_from( [ (index, {'posicion': radius[0:3], 'radio': radius[3] } ) ])\n",
    "        graph.add_nodes_from( [ (self.data, {'posicion': radius[0:3], 'radio': radius[3] } ) ])\n",
    "        \n",
    "\n",
    "        if self.right is not None:\n",
    "            #leftIndex = self.right.toGraph( graph, index + 1, dec)#\n",
    "            self.right.toGraph( graph, index + 1, dec)#\n",
    "            \n",
    "            #graph.add_edge( index, index + 1 )\n",
    "            graph.add_edge( self.data, self.right.data )\n",
    "            #if proc:\n",
    "            #    nx.set_edge_attributes( graph, {(index, index+1) : {'procesada':False}})\n",
    "        \n",
    "        if self.left is not None:\n",
    "            #retIndex = self.left.toGraph( graph, leftIndex, dec )#\n",
    "            self.left.toGraph( graph, 0, dec )#\n",
    "\n",
    "            #graph.add_edge( index, leftIndex)\n",
    "            graph.add_edge( self.data, self.left.data)\n",
    "            #if proc:\n",
    "            #    nx.set_edge_attributes( graph, {(index, leftIndex) : {'procesada':False}})\n",
    "\n",
    "        else:\n",
    "            #return index + 1\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTree( root, dec ):\n",
    "    graph = nx.Graph()\n",
    "    root.toGraph( graph, 0, dec)\n",
    "    edges=nx.get_edge_attributes(graph,'procesada')\n",
    "\n",
    "    p = mp.plot( np.array([ graph.nodes[v]['posicion'] for v in graph.nodes]), shading={'point_size':0.1}, return_plot=True)\n",
    "\n",
    "    for arista in graph.edges:\n",
    "        p.add_lines( graph.nodes[arista[0]]['posicion'], graph.nodes[arista[1]]['posicion'])\n",
    "\n",
    "    return \n",
    "\n",
    "def traverse(root, tree):\n",
    "       \n",
    "        if root is not None:\n",
    "            traverse(root.left, tree)\n",
    "            tree.append((root.radius, root.data))\n",
    "            traverse(root.right, tree)\n",
    "            return tree\n",
    "\n",
    "def traverse_2(tree1, tree2, t_l):\n",
    "       \n",
    "        if tree1 is not None:\n",
    "            traverse_2(tree1.left, tree2.left, t_l)\n",
    "            if tree2:\n",
    "                t_l.append((tree1.radius, tree2.radius))\n",
    "                print((tree1.radius, tree2.radius))\n",
    "            else:\n",
    "                t_l.append(tree1.radius)\n",
    "                print((tree1.radius))\n",
    "            traverse_2(tree1.right, tree2, t_l)\n",
    "            return t_l\n",
    "            \n",
    "\n",
    "def traverse_conexiones(root, tree):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            traverse_conexiones(root.left, tree)\n",
    "            if root.right is not None:\n",
    "                tree.append((root.data, root.right.data))\n",
    "            if root.left is not None:\n",
    "                tree.append((root.data, root.left.data))\n",
    "            traverse_conexiones(root.right, tree)\n",
    "            return tree\n",
    "\n",
    "def arbolAGrafo (nodoRaiz):\n",
    "    \n",
    "    conexiones = []\n",
    "    lineas = traverse_conexiones(nodoRaiz, conexiones)\n",
    "    tree = []\n",
    "    tree = traverse(nodoRaiz, tree)\n",
    "\n",
    "    vertices = []\n",
    "    verticesCrudos = []\n",
    "    for node in tree:\n",
    "        vertice = node[0][0][:3]\n",
    "        rad = node[0][0][-1]\n",
    "        num = node[1]\n",
    "        \n",
    "        #vertices.append((num, {'posicion': Vec3( vertice[0], vertice[1], vertice[2]), 'radio': rad} ))\n",
    "        vertices.append((len(verticesCrudos),{'posicion': Vec3( vertice[0], vertice[1], vertice[2]), 'radio': rad}))\n",
    "        verticesCrudos.append(vertice)\n",
    "\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from( vertices )\n",
    "    G.add_edges_from( lineas )\n",
    "    \n",
    "    return G\n",
    "\n",
    "@count_fn\n",
    "def createNode(data, radius, position = None, left = None, right = None, cl_prob = None, ce = None, mse=None):\n",
    "        \"\"\"\n",
    "        Utility function to create a node.\n",
    "        \"\"\"\n",
    "        return Node(data, radius, position, left, right, cl_prob, ce, mse)\n",
    " \n",
    "def deserialize(data):\n",
    "    if  not data:\n",
    "        return \n",
    "    nodes = data.split(';')  \n",
    "    #print(\"node\",nodes[3])\n",
    "    def post_order(nodes):\n",
    "                \n",
    "        if nodes[-1] == '#':\n",
    "            nodes.pop()\n",
    "            return None\n",
    "        node = nodes.pop().split('_')\n",
    "        data = int(node[0])\n",
    "        #radius = float(node[1])\n",
    "        #print(\"node\", node)\n",
    "        #breakpoint()\n",
    "        radius = node[1]\n",
    "        #print(\"radius\", radius)\n",
    "        rad = radius.split(\",\")\n",
    "        rad [0] = rad[0].replace('[','')\n",
    "        rad [3] = rad[3].replace(']','')\n",
    "        r = []\n",
    "        for value in rad:\n",
    "            r.append(float(value))\n",
    "        #r =[float(num) for num in radius if num.isdigit()]\n",
    "        r = torch.tensor(r, device=device)\n",
    "        #breakpoint()\n",
    "        root = createNode(data, r)\n",
    "        root.right = post_order(nodes)\n",
    "        root.left = post_order(nodes)\n",
    "        \n",
    "        return root    \n",
    "    return post_order(nodes)    \n",
    "\n",
    "\n",
    "def read_tree(filename):\n",
    "    with open('./trees/' +'prof3/' +filename, \"r\") as f:\n",
    "        byte = f.read() \n",
    "        return byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternalEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, feature_size: int, hidden_size: int):\n",
    "        super(InternalEncoder, self).__init__()\n",
    "\n",
    "        #print(\"init\")\n",
    "        # Encoders atributos\n",
    "        self.attribute_lin_encoder_1 = nn.Linear(input_size,feature_size)\n",
    "        self.attribute_lin_encoder_2 = nn.Linear(feature_size,hidden_size)\n",
    "        self.attribute_lin_encoder_3 = nn.Linear(hidden_size,feature_size)\n",
    "\n",
    "        # Encoders derecho e izquierdo\n",
    "        self.right_lin_encoder_1 = nn.Linear(feature_size,hidden_size)\n",
    "        self.right_lin_encoder_2 = nn.Linear(hidden_size,feature_size)\n",
    "        #self.right_lin_encoder_3 = nn.Linear(feature_size,feature_size)\n",
    "\n",
    "        self.left_lin_encoder_1  = nn.Linear(feature_size,hidden_size)\n",
    "        self.left_lin_encoder_2  = nn.Linear(hidden_size,feature_size)\n",
    "        #self.left_lin_encoder_3  = nn.Linear(feature_size,feature_size)\n",
    "\n",
    "\n",
    "        # Encoder final\n",
    "        self.final_lin_encoder_1 = nn.Linear(2*feature_size, feature_size)\n",
    "\n",
    "        # Funciones / Parametros utiles\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "\n",
    "    def forward(self, input, right_input, left_input):\n",
    "        # Encodeo los atributos\n",
    "        attributes = self.attribute_lin_encoder_1(input)\n",
    "        attributes = self.tanh(attributes)\n",
    "        attributes = self.attribute_lin_encoder_2(attributes)\n",
    "        attributes = self.tanh(attributes)\n",
    "        attributes = self.attribute_lin_encoder_3(attributes)\n",
    "        attributes = self.tanh(attributes)\n",
    "        #print(\"attributes\", attributes)\n",
    "\n",
    "        # Encodeo el derecho\n",
    "        if right_input is not None:\n",
    "            #print(\"right input\", right_input)\n",
    "            context = self.right_lin_encoder_1(right_input)\n",
    "            context = self.tanh(context)\n",
    "            context = self.right_lin_encoder_2(context)\n",
    "            #context = self.tanh(context)\n",
    "            #context = self.right_lin_encoder_3(context)\n",
    "            \n",
    "            # Encodeo el izquierdo\n",
    "            #print(\"left input\", left_input)\n",
    "            if left_input is not None:\n",
    "                left = self.left_lin_encoder_1(left_input)\n",
    "                #print(\"izquierdo\", left.shape)\n",
    "                left = self.tanh(left)\n",
    "                #left = self.left_lin_encoder_2(left)\n",
    "                #left = self.tanh(left)\n",
    "                context += self.left_lin_encoder_2(left)\n",
    "                #print(\"context izquierdo\", context.shape)\n",
    "        else:\n",
    "            context = torch.zeros(input.shape[0],self.feature_size, requires_grad=True, device=device)\n",
    "        \n",
    "\n",
    "        context = self.tanh(context)\n",
    "        feature = torch.cat((attributes,context), 1)\n",
    "        feature = self.final_lin_encoder_1(feature)\n",
    "        feature = self.tanh(feature)\n",
    "        #print(\"output\", feature)\n",
    "        return feature\n",
    "\n",
    "       \n",
    "    \n",
    "\n",
    "class GRASSEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, feature_size : int, hidden_size: int):\n",
    "        super(GRASSEncoder, self).__init__()\n",
    "        self.leaf_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        self.internal_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        self.bifurcation_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        \n",
    "    def leafEncoder(self, node, right=None, left = None):\n",
    "        return self.internal_encoder(node, right, left)\n",
    "    def internalEncoder(self, node, right, left = None):\n",
    "        return self.internal_encoder(node, right, left)\n",
    "    def bifurcationEncoder(self, node, right, left):\n",
    "        \n",
    "        return self.bifurcation_encoder(node, right, left)\n",
    "\n",
    "Grassencoder = GRASSEncoder(input_size = 4, feature_size=512, hidden_size=1024)\n",
    "Grassencoder = Grassencoder.to(device)\n",
    "\n",
    "\n",
    "def encode_structure_fold(fold, root):\n",
    "    \n",
    "    \n",
    "    def encode_node(node):\n",
    "        \n",
    "        if node is None:\n",
    "            return\n",
    "        \n",
    "        if node.is_leaf():\n",
    "            return fold.add('leafEncoder', node.radius)\n",
    "        else:\n",
    "            left = encode_node(node.left)\n",
    "            right = encode_node(node.right)\n",
    "            if left is not None:\n",
    "             \n",
    "                return fold.add('bifurcationEncoder', node.radius, right, left)\n",
    "            else:\n",
    "                return fold.add('internalEncoder', node.radius, right)\n",
    "        \n",
    "\n",
    "    encoding = encode_node(root)\n",
    "    \n",
    "    return encoding\n",
    "  \n",
    "def encode_structure(root):\n",
    "    \n",
    "    def encode_node(node):\n",
    "          \n",
    "        if node is None:\n",
    "            return\n",
    "        if node.is_leaf():\n",
    "            return Grassencoder.leafEncoder(node.radius.reshape(-1,4))\n",
    "        else :\n",
    "            left = encode_node(node.left)\n",
    "            right = encode_node(node.right)\n",
    "            if left is not None:\n",
    "                return Grassencoder.bifurcationEncoder(node.radius.reshape(-1,4), right, left)\n",
    "            else:\n",
    "                return Grassencoder.internalEncoder(node.radius.reshape(-1,4), right)\n",
    "        \n",
    "\n",
    "    encoding = encode_node(root)\n",
    "   \n",
    "    return encoding\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerar_nodos(root, count):\n",
    "    if root is not None:\n",
    "        numerar_nodos(root.left, count)\n",
    "        root.data = len(count)\n",
    "        count.append(1)\n",
    "        numerar_nodos(root.right, count)\n",
    "        return \n",
    "\n",
    "\n",
    "def traversefeatures(root, features):\n",
    "       \n",
    "    if root is not None:\n",
    "        traversefeatures(root.left, features)\n",
    "        features.append(root.radius)\n",
    "        traversefeatures(root.right, features)\n",
    "        return features\n",
    "\n",
    "def norm(root, minx, miny, minz, minr, maxx, maxy, maxz, maxr):\n",
    "    \n",
    "    if root is not None:\n",
    "        mx = minx.clone().detach()\n",
    "        my = miny.clone().detach()\n",
    "        mz = minz.clone().detach()\n",
    "        mr = minr.clone().detach()\n",
    "        Mx = maxx.clone().detach()\n",
    "        My = maxy.clone().detach()\n",
    "        Mz = maxz.clone().detach()\n",
    "        Mr = maxr.clone().detach()\n",
    "       \n",
    "        root.radius[0] = (root.radius[0] - minx)/(maxx - minx)\n",
    "        root.radius[1] = (root.radius[1] - miny)/(maxy - miny)\n",
    "        root.radius[2] = (root.radius[2] - minz)/(maxz - minz)\n",
    "        root.radius[3] = (root.radius[3] - minr)/(maxr - minr)\n",
    "        \n",
    "        norm(root.left, mx, my, mz, mr, Mx, My, Mz, Mr)\n",
    "        norm(root.right, mx, my, mz, mr, Mx, My, Mz, Mr)\n",
    "        return \n",
    "\n",
    "def normalize_features(root):\n",
    "    features = []\n",
    "    features = traversefeatures(root, features)\n",
    "    \n",
    "    x = [tensor[0] for tensor in features]\n",
    "    y = [tensor[1] for tensor in features]\n",
    "    z = [tensor[2] for tensor in features]\n",
    "    r = [tensor[3] for tensor in features]\n",
    " \n",
    "    norm(root, min(x), min(y), min(z), min(r), max(x), max(y), max(z), max(r))\n",
    "\n",
    "    return \n",
    "\n",
    "def traversefeatures(root, features):\n",
    "       \n",
    "    if root is not None:\n",
    "        traversefeatures(root.left, features)\n",
    "        features.append(root.radius)\n",
    "        traversefeatures(root.right, features)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tree0.dat', 'tree1.dat', 'tree10.dat', 'tree11.dat', 'tree12.dat', 'tree13.dat', 'tree14.dat', 'tree15.dat', 'tree16.dat', 'tree17.dat']\n"
     ]
    }
   ],
   "source": [
    "def my_collate(batch):\n",
    "    return batch\n",
    "\n",
    "#t_list = ['ArteryObjAN1-7.dat','ArteryObjAN1-0.dat', 'ArteryObjAN1-17.dat',  'ArteryObjAN1-11.dat']\n",
    "\n",
    "#t_list = ['ArteryObjAN1-0.dat','ArteryObjAN1-7.dat', 'ArteryObjAN1-17.dat',  'ArteryObjAN1-11.dat', 'ArteryObjAN1-19.dat', 'ArteryObjAN2-4.dat', 'ArteryObjAN2-6.dat', \n",
    "#           'ArteryObjAN25-18.dat']\n",
    "#t_list = ['ArteryObjAN1-17-55.dat', 'ArteryObjAN1-17-22.dat', \"ArteryObjAN1-17-12.dat\", \"ArteryObjAN1-17-9.dat\",'ArteryObjAN1-17-42.dat', 'ArteryObjAN1-17-64.dat', \"ArteryObjAN1-17-70.dat\", \"ArteryObjAN1-17-1.dat\"]\n",
    "#t_list = ['ArteryObjAN1-17.dat']\n",
    "#t_list = ['ArteryObjAN1-11.dat']\n",
    "\n",
    "\n",
    "#t_list = ['test2.dat']\n",
    "\n",
    "#t_list = ['ArteryObjAN31-14.dat']\n",
    "#t_list = os.listdir(\"./trees\")[:20]\n",
    "t_list = os.listdir(\"./trees/prof3\")[:10]\n",
    "print(t_list)\n",
    "class tDataset(Dataset):\n",
    "    def __init__(self, dir, transform=None):\n",
    "        self.names = dir\n",
    "        self.transform = transform\n",
    "        self.data = [] #lista con las strings de todos los arboles\n",
    "        for file in self.names:\n",
    "            self.data.append(read_tree(file))\n",
    "        self.trees = []\n",
    "        for tree in self.data:\n",
    "            deserial = deserialize(tree)\n",
    "            normalize_features(deserial)\n",
    "            self.trees.append(deserial)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #file = self.names[idx]\n",
    "        #string = read_tree(file)\n",
    "        tree = self.trees[idx]\n",
    "        return tree\n",
    "\n",
    "batch_size = 10\n",
    "dataset = tDataset(t_list)\n",
    "data_loader = DataLoader(dataset, batch_size = batch_size, shuffle=True, collate_fn=my_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47cbec983b5f40159916abfbc329df71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.5, 0.5,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0., 0., 0., 0.], device='cuda:0')\n",
      "2 tensor([1., 1., 1., 1.], device='cuda:0')\n",
      "1 tensor([0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input = iter(data_loader).next()[0]\n",
    "plotTree(input, False)\n",
    "input.traverseInorder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED con batch [tensor([[-0.0554, -0.0035,  0.0399,  0.0745,  0.1057,  0.0828,  0.0751, -0.0148,\n",
      "         -0.0286, -0.0111, -0.0284,  0.0852, -0.0192, -0.0870, -0.0061,  0.0030,\n",
      "          0.0399, -0.0041,  0.1329,  0.0011,  0.0156,  0.0300,  0.0506,  0.0202,\n",
      "         -0.0008, -0.0289,  0.0886,  0.0455, -0.0389,  0.0456,  0.0651,  0.0290,\n",
      "         -0.0204,  0.0108, -0.0928,  0.0160,  0.0005,  0.0795,  0.0231, -0.0293,\n",
      "          0.0142, -0.0217, -0.0321, -0.1034,  0.0899,  0.0002,  0.0769,  0.0412,\n",
      "          0.0021, -0.0853,  0.0359,  0.0389,  0.0193, -0.0079, -0.0749,  0.0312,\n",
      "          0.0754, -0.0060,  0.0184, -0.0793, -0.0408, -0.0492, -0.0027,  0.0674,\n",
      "          0.0574, -0.0246,  0.0077,  0.0277, -0.0371, -0.0067, -0.0114,  0.0139,\n",
      "         -0.1223, -0.0191,  0.0018, -0.1168,  0.0436,  0.0434, -0.0255,  0.0296,\n",
      "          0.0667,  0.0539,  0.0184, -0.0490,  0.0652,  0.0490, -0.0498,  0.0167,\n",
      "          0.0364, -0.0437,  0.0302, -0.0335,  0.0002, -0.0314, -0.0683,  0.0028,\n",
      "         -0.0156,  0.0457, -0.0329, -0.0277,  0.0094,  0.0663, -0.0020,  0.0713,\n",
      "          0.0170,  0.0135, -0.0396, -0.0478,  0.1199,  0.0337, -0.0279, -0.0375,\n",
      "         -0.0214,  0.0313, -0.0469, -0.0340, -0.0473, -0.0241, -0.0619,  0.0058,\n",
      "         -0.0226,  0.0036, -0.0285,  0.0217,  0.0817, -0.0362, -0.0589,  0.0939,\n",
      "         -0.0391, -0.0928, -0.0153,  0.0041,  0.0148, -0.0755,  0.0092,  0.0278,\n",
      "          0.0416, -0.0212, -0.0970, -0.0056,  0.0838,  0.0414,  0.0232, -0.0180,\n",
      "          0.0280, -0.0638, -0.0972, -0.0275,  0.0050,  0.0857, -0.0383,  0.0166,\n",
      "         -0.0344, -0.0048,  0.0486,  0.0720,  0.0160,  0.0026,  0.0198,  0.0275,\n",
      "         -0.0429,  0.0153, -0.0902,  0.0846,  0.0225, -0.0145, -0.0285, -0.0302,\n",
      "          0.0296,  0.0327, -0.0887,  0.0372, -0.0152,  0.0028, -0.0298,  0.0275,\n",
      "         -0.0026, -0.0400, -0.0028, -0.0483, -0.0400,  0.0327,  0.0326,  0.0408,\n",
      "          0.0003,  0.0131,  0.0007, -0.0303,  0.0713, -0.0946, -0.0221,  0.0839,\n",
      "         -0.0562,  0.0417, -0.0066,  0.0957,  0.0049, -0.0263,  0.0688,  0.0912,\n",
      "          0.0121,  0.0406, -0.0151, -0.0099, -0.0115, -0.1263,  0.0110, -0.1372,\n",
      "         -0.0330, -0.0226,  0.0492, -0.0319, -0.0009,  0.0785,  0.0333,  0.0140,\n",
      "          0.0039, -0.0208, -0.0234,  0.0731, -0.0657,  0.0367, -0.0763,  0.0139,\n",
      "         -0.0292,  0.0345,  0.0200,  0.0177, -0.0192,  0.0194, -0.0102, -0.0812,\n",
      "         -0.0363,  0.0110,  0.0002, -0.0236, -0.0352,  0.0391, -0.0070,  0.0448,\n",
      "         -0.0469, -0.0701, -0.0425,  0.0192,  0.0125, -0.0787,  0.1129,  0.0326,\n",
      "          0.0146, -0.0294,  0.0362,  0.0664, -0.0052, -0.0031, -0.0164, -0.0158,\n",
      "          0.0131,  0.0214, -0.0053, -0.0050, -0.0403, -0.0457, -0.0220,  0.0466,\n",
      "         -0.0094, -0.0373,  0.0315,  0.0826,  0.0015, -0.0080,  0.0538,  0.0594,\n",
      "         -0.0287,  0.0735,  0.0114, -0.0437, -0.0491,  0.0914, -0.0014, -0.0550,\n",
      "         -0.0412,  0.0385,  0.0184, -0.0347,  0.0005, -0.1566,  0.0064, -0.0041,\n",
      "         -0.0160,  0.0676, -0.0356, -0.0308, -0.0040, -0.1198,  0.0314,  0.0251,\n",
      "          0.0313,  0.0524,  0.0279, -0.0051, -0.0161, -0.0228, -0.0354, -0.0072,\n",
      "          0.0491,  0.0395, -0.0812,  0.0586,  0.0278, -0.0414,  0.0264,  0.0305,\n",
      "         -0.0578,  0.0133,  0.0359,  0.0857,  0.0238, -0.0025,  0.0300,  0.0486,\n",
      "          0.0556,  0.0658, -0.0177, -0.0287, -0.0402, -0.0110,  0.0286,  0.0480,\n",
      "         -0.0773, -0.0260,  0.0284,  0.0023,  0.0336,  0.0065,  0.0589, -0.0199,\n",
      "          0.0231, -0.0465, -0.0073,  0.0394, -0.0211,  0.0395,  0.0050, -0.0527,\n",
      "         -0.0095, -0.0003,  0.0179,  0.0859,  0.0261,  0.0780,  0.0294, -0.0204,\n",
      "          0.0501, -0.0149,  0.0201, -0.0703,  0.0205, -0.0055, -0.0412,  0.1058,\n",
      "         -0.0451, -0.0212,  0.0013,  0.0323, -0.0003, -0.0524, -0.0045, -0.0250,\n",
      "         -0.0304, -0.0243, -0.0970,  0.0187,  0.0577,  0.0617,  0.0294, -0.0225,\n",
      "         -0.0378,  0.0295,  0.0258, -0.0011, -0.0206, -0.0005, -0.0390,  0.0592,\n",
      "          0.0029,  0.0401,  0.0129, -0.1026,  0.0038, -0.0267,  0.0327,  0.0205,\n",
      "         -0.0473, -0.0094,  0.0408,  0.0254, -0.0655,  0.0067,  0.0143, -0.0645,\n",
      "         -0.0038, -0.0011, -0.0726, -0.0660,  0.0020, -0.0039,  0.0648, -0.0300,\n",
      "         -0.0574,  0.0059, -0.0917,  0.0830,  0.0379,  0.0871,  0.0409, -0.0675,\n",
      "          0.0383, -0.0446,  0.0156, -0.0646, -0.0245,  0.0341, -0.0868, -0.0152,\n",
      "         -0.0197, -0.0078, -0.0134,  0.0634, -0.0685, -0.0665, -0.0054,  0.0060,\n",
      "         -0.0014,  0.0074,  0.0278, -0.0779, -0.0254, -0.0280, -0.0053, -0.0252,\n",
      "         -0.0469, -0.0299, -0.0481, -0.0177, -0.0349,  0.0223,  0.0567, -0.0698,\n",
      "          0.0253, -0.0253, -0.0283, -0.0302,  0.0828,  0.0074, -0.0542, -0.0234,\n",
      "         -0.0017, -0.0083, -0.0073, -0.0816,  0.0129,  0.0026, -0.0656,  0.0343,\n",
      "         -0.0383,  0.0846,  0.0322, -0.0452,  0.0115,  0.0069,  0.0151, -0.0577,\n",
      "          0.0020, -0.0308, -0.0870, -0.0192, -0.0546, -0.0558,  0.0459,  0.0537,\n",
      "         -0.0552,  0.0314,  0.0652, -0.0771,  0.0309, -0.0561,  0.0020, -0.0059,\n",
      "         -0.0436,  0.0545,  0.0709,  0.0950,  0.0103, -0.0421, -0.0189, -0.0432,\n",
      "         -0.0320, -0.0703,  0.0178,  0.1129, -0.0038, -0.0205,  0.0694, -0.0392,\n",
      "          0.0191,  0.0379,  0.0142, -0.0458, -0.0204,  0.0189, -0.0203, -0.0140]],\n",
      "       device='cuda:0', grad_fn=<StackBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "import torch_f\n",
    "enc_fold = torch_f.Fold(device)\n",
    "enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "batch = iter(data_loader).next()\n",
    "#for example in batch:\n",
    "        #enc_fold.add('leafEncoder', example.radius)\n",
    "        #enc_fold_nodes.append(enc_fold.add('leafEncoder', example.radius))\n",
    "        #enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "\n",
    "        #print(\"enc fold nodes\", enc_fold)\n",
    "for example in data_loader:\n",
    "        example = example[0]\n",
    "        enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "print(\"ENCODED con batch\",enc_fold_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encodeado sin batch tensor([[-0.0554, -0.0035,  0.0399,  0.0745,  0.1057,  0.0828,  0.0751, -0.0148,\n",
      "         -0.0286, -0.0111, -0.0284,  0.0852, -0.0192, -0.0870, -0.0061,  0.0030,\n",
      "          0.0399, -0.0041,  0.1329,  0.0011,  0.0156,  0.0300,  0.0506,  0.0202,\n",
      "         -0.0008, -0.0289,  0.0886,  0.0455, -0.0389,  0.0456,  0.0651,  0.0290,\n",
      "         -0.0204,  0.0108, -0.0928,  0.0160,  0.0005,  0.0795,  0.0231, -0.0293,\n",
      "          0.0142, -0.0217, -0.0321, -0.1034,  0.0899,  0.0002,  0.0769,  0.0412,\n",
      "          0.0021, -0.0853,  0.0359,  0.0389,  0.0193, -0.0079, -0.0749,  0.0312,\n",
      "          0.0754, -0.0060,  0.0184, -0.0793, -0.0408, -0.0492, -0.0027,  0.0674,\n",
      "          0.0574, -0.0246,  0.0077,  0.0277, -0.0371, -0.0067, -0.0114,  0.0139,\n",
      "         -0.1223, -0.0191,  0.0018, -0.1168,  0.0436,  0.0434, -0.0255,  0.0296,\n",
      "          0.0667,  0.0539,  0.0184, -0.0490,  0.0652,  0.0490, -0.0498,  0.0167,\n",
      "          0.0364, -0.0437,  0.0302, -0.0335,  0.0002, -0.0314, -0.0683,  0.0028,\n",
      "         -0.0156,  0.0457, -0.0329, -0.0277,  0.0094,  0.0663, -0.0020,  0.0713,\n",
      "          0.0170,  0.0135, -0.0396, -0.0478,  0.1199,  0.0337, -0.0279, -0.0375,\n",
      "         -0.0214,  0.0313, -0.0469, -0.0340, -0.0473, -0.0241, -0.0619,  0.0058,\n",
      "         -0.0226,  0.0036, -0.0285,  0.0217,  0.0817, -0.0362, -0.0589,  0.0939,\n",
      "         -0.0391, -0.0928, -0.0153,  0.0041,  0.0148, -0.0755,  0.0092,  0.0278,\n",
      "          0.0416, -0.0212, -0.0970, -0.0056,  0.0838,  0.0414,  0.0232, -0.0180,\n",
      "          0.0280, -0.0638, -0.0972, -0.0275,  0.0050,  0.0857, -0.0383,  0.0166,\n",
      "         -0.0344, -0.0048,  0.0486,  0.0720,  0.0160,  0.0026,  0.0198,  0.0275,\n",
      "         -0.0429,  0.0153, -0.0902,  0.0846,  0.0225, -0.0145, -0.0285, -0.0302,\n",
      "          0.0296,  0.0327, -0.0887,  0.0372, -0.0152,  0.0028, -0.0298,  0.0275,\n",
      "         -0.0026, -0.0400, -0.0028, -0.0483, -0.0400,  0.0327,  0.0326,  0.0408,\n",
      "          0.0003,  0.0131,  0.0007, -0.0303,  0.0713, -0.0946, -0.0221,  0.0839,\n",
      "         -0.0562,  0.0417, -0.0066,  0.0957,  0.0049, -0.0263,  0.0688,  0.0912,\n",
      "          0.0121,  0.0406, -0.0151, -0.0099, -0.0115, -0.1263,  0.0110, -0.1372,\n",
      "         -0.0330, -0.0226,  0.0492, -0.0319, -0.0009,  0.0785,  0.0333,  0.0140,\n",
      "          0.0039, -0.0208, -0.0234,  0.0731, -0.0657,  0.0367, -0.0763,  0.0139,\n",
      "         -0.0292,  0.0345,  0.0200,  0.0177, -0.0192,  0.0194, -0.0102, -0.0812,\n",
      "         -0.0363,  0.0110,  0.0002, -0.0236, -0.0352,  0.0391, -0.0070,  0.0448,\n",
      "         -0.0469, -0.0701, -0.0425,  0.0192,  0.0125, -0.0787,  0.1129,  0.0326,\n",
      "          0.0146, -0.0294,  0.0362,  0.0664, -0.0052, -0.0031, -0.0164, -0.0158,\n",
      "          0.0131,  0.0214, -0.0053, -0.0050, -0.0403, -0.0457, -0.0220,  0.0466,\n",
      "         -0.0094, -0.0373,  0.0315,  0.0826,  0.0015, -0.0080,  0.0538,  0.0594,\n",
      "         -0.0287,  0.0735,  0.0114, -0.0437, -0.0491,  0.0914, -0.0014, -0.0550,\n",
      "         -0.0412,  0.0385,  0.0184, -0.0347,  0.0005, -0.1566,  0.0064, -0.0041,\n",
      "         -0.0160,  0.0676, -0.0356, -0.0308, -0.0040, -0.1198,  0.0314,  0.0251,\n",
      "          0.0313,  0.0524,  0.0279, -0.0051, -0.0161, -0.0228, -0.0354, -0.0072,\n",
      "          0.0491,  0.0395, -0.0812,  0.0586,  0.0278, -0.0414,  0.0264,  0.0305,\n",
      "         -0.0578,  0.0133,  0.0359,  0.0857,  0.0238, -0.0025,  0.0300,  0.0486,\n",
      "          0.0556,  0.0658, -0.0177, -0.0287, -0.0402, -0.0110,  0.0286,  0.0480,\n",
      "         -0.0773, -0.0260,  0.0284,  0.0023,  0.0336,  0.0065,  0.0589, -0.0199,\n",
      "          0.0231, -0.0465, -0.0073,  0.0394, -0.0211,  0.0395,  0.0050, -0.0527,\n",
      "         -0.0095, -0.0003,  0.0179,  0.0859,  0.0261,  0.0780,  0.0294, -0.0204,\n",
      "          0.0501, -0.0149,  0.0201, -0.0703,  0.0205, -0.0055, -0.0412,  0.1058,\n",
      "         -0.0451, -0.0212,  0.0013,  0.0323, -0.0003, -0.0524, -0.0045, -0.0250,\n",
      "         -0.0304, -0.0243, -0.0970,  0.0187,  0.0577,  0.0617,  0.0294, -0.0225,\n",
      "         -0.0378,  0.0295,  0.0258, -0.0011, -0.0206, -0.0005, -0.0390,  0.0592,\n",
      "          0.0029,  0.0401,  0.0129, -0.1026,  0.0038, -0.0267,  0.0327,  0.0205,\n",
      "         -0.0473, -0.0094,  0.0408,  0.0254, -0.0655,  0.0067,  0.0143, -0.0645,\n",
      "         -0.0038, -0.0011, -0.0726, -0.0660,  0.0020, -0.0039,  0.0648, -0.0300,\n",
      "         -0.0574,  0.0059, -0.0917,  0.0830,  0.0379,  0.0871,  0.0409, -0.0675,\n",
      "          0.0383, -0.0446,  0.0156, -0.0646, -0.0245,  0.0341, -0.0868, -0.0152,\n",
      "         -0.0197, -0.0078, -0.0134,  0.0634, -0.0685, -0.0665, -0.0054,  0.0060,\n",
      "         -0.0014,  0.0074,  0.0278, -0.0779, -0.0254, -0.0280, -0.0053, -0.0252,\n",
      "         -0.0469, -0.0299, -0.0481, -0.0177, -0.0349,  0.0223,  0.0567, -0.0698,\n",
      "          0.0253, -0.0253, -0.0283, -0.0302,  0.0828,  0.0074, -0.0542, -0.0234,\n",
      "         -0.0017, -0.0083, -0.0073, -0.0816,  0.0129,  0.0026, -0.0656,  0.0343,\n",
      "         -0.0383,  0.0846,  0.0322, -0.0452,  0.0115,  0.0069,  0.0151, -0.0577,\n",
      "          0.0020, -0.0308, -0.0870, -0.0192, -0.0546, -0.0558,  0.0459,  0.0537,\n",
      "         -0.0552,  0.0314,  0.0652, -0.0771,  0.0309, -0.0561,  0.0020, -0.0059,\n",
      "         -0.0436,  0.0545,  0.0709,  0.0950,  0.0103, -0.0421, -0.0189, -0.0432,\n",
      "         -0.0320, -0.0703,  0.0178,  0.1129, -0.0038, -0.0205,  0.0694, -0.0392,\n",
      "          0.0191,  0.0379,  0.0142, -0.0458, -0.0204,  0.0189, -0.0203, -0.0140]],\n",
      "       device='cuda:0', grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for data in data_loader:\n",
    "    data = data[0]\n",
    "\n",
    "enc_f = encode_structure(example).to(device)\n",
    "\n",
    "print(\"encodeado sin batch\", enc_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_fold_nodes[0]-enc_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[5, 5, 3, 6, 6, 5, 5, 5, 3, 5]\n",
      "4.8\n",
      "3.0\n",
      "0.6\n",
      "1.2\n"
     ]
    }
   ],
   "source": [
    "n_no = []\n",
    "qzero = 0\n",
    "qOne = 0\n",
    "qtwo = 0\n",
    "\n",
    "for batch in data_loader:\n",
    "    for tree in batch:\n",
    "        count = []\n",
    "        n = tree.count_nodes(tree, count)\n",
    "        n_no.append(len(n))\n",
    "        li = []\n",
    "        tree.traverseInorderChilds(tree, li)\n",
    "        zero = [a for a in li if a == 0]\n",
    "        one = [a for a in li if a == 1]\n",
    "        two = [a for a in li if a == 2]\n",
    "        qzero += len(zero)\n",
    "        qOne += len(one)\n",
    "        qtwo += len(two)\n",
    "\n",
    "print(len(data_loader)*batch_size)\n",
    "print(n_no)\n",
    "nprom = np.mean(n_no)\n",
    "print(nprom)\n",
    "qzero /= len(data_loader)*batch_size\n",
    "qOne /= len(data_loader)*batch_size\n",
    "qtwo /= len(data_loader)*batch_size\n",
    "\n",
    "print(qzero)\n",
    "print(qOne)\n",
    "print(qtwo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en el loop creo un fold, mando este fold con cada uno de los arboles del batch a encode_structure_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.mlp1 = nn.Linear(latent_size, hidden_size)\n",
    "        self.mlp2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.mlp3 = nn.Linear(hidden_size, 3)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_feature):\n",
    "        #print(\"classifier input\", input_feature)\n",
    "        output = self.mlp1(input_feature)\n",
    "        output = self.tanh(output)\n",
    "        output = self.mlp2(output)\n",
    "        output = self.tanh(output)\n",
    "        output = self.mlp3(output)\n",
    "        #print(\"classifier output\", output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class InternalDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size: int):\\n        super(InternalDecoder, self).__init__()\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.lp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp_right = nn.Linear(latent_size, latent_size)\\n        self.tanh = nn.Tanh()\\n        self.mlp2 = nn.Linear(latent_size,4)\\n\\n    def forward(self, parent_feature):\\n        #print(\"internal decoder\")\\n        #print(\"input\", parent_feature.shape)\\n        vector = self.mlp(parent_feature)\\n        vector = self.tanh(vector)\\n        vector = self.lp2(vector)\\n        vector = self.tanh(vector)\\n        right_feature = self.mlp_right(vector)\\n        right_feature = self.tanh(right_feature)\\n        rad_feature = self.mlp2(vector)\\n\\n        return right_feature, rad_feature\\n\\nclass BifurcationDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size : int):\\n        super(BifurcationDecoder, self).__init__()\\n        #self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.lp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp_left = nn.Linear(latent_size, latent_size)\\n        self.mlp_right = nn.Linear(latent_size, latent_size)\\n        self.mlp2 = nn.Linear(latent_size,4)\\n        self.tanh = nn.Tanh()\\n\\n    def forward(self, parent_feature):\\n        #print(\"bifurcation decoder input\", parent_feature.shape)\\n        parent_feature = parent_feature.reshape(-1,128)\\n        #print(\"bifurcation decoder input\", parent_feature.shape)\\n        vector = self.mlp(parent_feature)\\n        #print(\"v1\", vector.shape)\\n        vector = self.tanh(vector)\\n        #print(\"v2\", vector.shape)\\n        vector = self.lp2(vector)\\n        #print(\"v3\", vector.shape)\\n        vector = self.tanh(vector)\\n        left_feature = self.mlp_left(vector)\\n        left_feature = self.tanh(left_feature)\\n        right_feature = self.mlp_right(vector)\\n        right_feature = self.tanh(right_feature)\\n        rad_feature = self.mlp2(vector)\\n        #print(\"exiting bif dec\")\\n        return left_feature, right_feature, rad_feature\\n\\n\\nclass featureDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size: int):\\n        super(featureDecoder, self).__init__()\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.mlp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp3 = nn.Linear(latent_size, latent_size)\\n        self.tanh = nn.Tanh()\\n        self.mlp4 = nn.Linear(latent_size,4)\\n\\n    def forward(self, parent_feature):\\n        #print(\"feature decoder input\", parent_feature.shape)\\n\\n        vector = self.mlp(parent_feature)\\n        vector = self.tanh(vector)\\n        vector = self.mlp2(vector)\\n        vector = self.tanh(vector)\\n        vector = self.mlp3(vector)\\n        vector = self.tanh(vector)\\n        vector = self.mlp4(vector)\\n       \\n        return vector\\n\\n\\n\\nclass GRASSDecoder(nn.Module):\\n    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\\n        super(GRASSDecoder, self).__init__()\\n        self.feature_decoder = featureDecoder(latent_size, hidden_size)\\n        self.internal_decoder = InternalDecoder(latent_size, hidden_size)\\n        self.bifurcation_decoder = BifurcationDecoder(latent_size, hidden_size)\\n        self.node_classifier = NodeClassifier(latent_size, hidden_size)\\n        self.mseLoss = nn.MSELoss()  # pytorch\\'s mean squared error loss\\n        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch\\'s cross entropy loss (NOTE: no softmax is needed before)\\n\\n    def featureDecoder(self, feature):\\n        return self.feature_decoder(feature)\\n\\n    def internalDecoder(self, feature):\\n        return self.internal_decoder(feature)\\n\\n    def bifurcationDecoder(self, feature):\\n        return self.bifurcation_decoder(feature)\\n\\n    def nodeClassifier(self, feature):\\n        return self.node_classifier(feature)\\n\\n    def calcularLossAtributo(self, nodo, radio):\\n        if nodo is None:\\n            return\\n        else:\\n            #print(\"radio\", radio)\\n            #print(\"nodo\", nodo)\\n            nodo = torch.stack(nodo)\\n            #print(\"nodo stack\", nodo)\\n            #radio = radio.reshape(-1,4)\\n        \\n            #return mse\\n            #return torch.cat([self.mseLoss(b, gt) for b, gt in zip(radio, nodo)], 0)\\n            z = zip(radio.reshape(-1,4), nodo.reshape(-1,4))\\n            \\n            #for b, gt in z:\\n            #    print(\"bgt\", b, gt)\\n                \\n            \\n            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\\n            #print(\"loss\", l)\\n            return l\\n\\n\\n    def classifyLossEstimator(self, label_vector, original):\\n        if original is None:\\n            return\\n        else:\\n           \\n            v = []\\n            for o in original:\\n                if o == 0:\\n                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\\n                if o == 1:\\n                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\\n                if o == 2:\\n                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\\n                v.append(vector)\\n\\n            v = torch.stack(v)\\n            z = zip(label_vector.reshape(-1,3), v.reshape(-1,3))   \\n            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\\n            \\n            return l\\n            #return c\\n       # return torch.cat([self.creLoss(l.unsqueeze(0), gt).mul(0.2) for l, gt in zip(label_vector, gt_label_vector)], 0)\\n\\n    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\\n        \\n        v = v1.add(v2)\\n        #print(\"v0\", v)\\n        if v3 is not None:\\n            v = v.add(v3)\\n        #print(\"v0\", v)\\n        if v4 is not None:\\n            v = v.add(v4)\\n       \\n        return v\\nif qzero == 0:\\n    qzero = 1\\nif qOne == 0:\\n    qOne = 1\\nif qtwo == 0:\\n    qtwo = 1\\nmult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\\nGrassdecoder = GRASSDecoder(latent_size=128, hidden_size=256, mult = mult)\\nGrassdecoder = Grassdecoder.to(device)\\n'"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class InternalDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size: int):\n",
    "        super(InternalDecoder, self).__init__()\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, latent_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.mlp2 = nn.Linear(latent_size,4)\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"internal decoder\")\n",
    "        #print(\"input\", parent_feature.shape)\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.lp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        rad_feature = self.mlp2(vector)\n",
    "\n",
    "        return right_feature, rad_feature\n",
    "\n",
    "class BifurcationDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(BifurcationDecoder, self).__init__()\n",
    "        #self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp_left = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp2 = nn.Linear(latent_size,4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"bifurcation decoder input\", parent_feature.shape)\n",
    "        parent_feature = parent_feature.reshape(-1,128)\n",
    "        #print(\"bifurcation decoder input\", parent_feature.shape)\n",
    "        vector = self.mlp(parent_feature)\n",
    "        #print(\"v1\", vector.shape)\n",
    "        vector = self.tanh(vector)\n",
    "        #print(\"v2\", vector.shape)\n",
    "        vector = self.lp2(vector)\n",
    "        #print(\"v3\", vector.shape)\n",
    "        vector = self.tanh(vector)\n",
    "        left_feature = self.mlp_left(vector)\n",
    "        left_feature = self.tanh(left_feature)\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        rad_feature = self.mlp2(vector)\n",
    "        #print(\"exiting bif dec\")\n",
    "        return left_feature, right_feature, rad_feature\n",
    "\n",
    "\n",
    "class featureDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size: int):\n",
    "        super(featureDecoder, self).__init__()\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp3 = nn.Linear(latent_size, latent_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.mlp4 = nn.Linear(latent_size,4)\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"feature decoder input\", parent_feature.shape)\n",
    "\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp3(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp4(vector)\n",
    "       \n",
    "        return vector\n",
    "\n",
    "\n",
    "\n",
    "class GRASSDecoder(nn.Module):\n",
    "    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\n",
    "        super(GRASSDecoder, self).__init__()\n",
    "        self.feature_decoder = featureDecoder(latent_size, hidden_size)\n",
    "        self.internal_decoder = InternalDecoder(latent_size, hidden_size)\n",
    "        self.bifurcation_decoder = BifurcationDecoder(latent_size, hidden_size)\n",
    "        self.node_classifier = NodeClassifier(latent_size, hidden_size)\n",
    "        self.mseLoss = nn.MSELoss()  # pytorch's mean squared error loss\n",
    "        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch's cross entropy loss (NOTE: no softmax is needed before)\n",
    "\n",
    "    def featureDecoder(self, feature):\n",
    "        return self.feature_decoder(feature)\n",
    "\n",
    "    def internalDecoder(self, feature):\n",
    "        return self.internal_decoder(feature)\n",
    "\n",
    "    def bifurcationDecoder(self, feature):\n",
    "        return self.bifurcation_decoder(feature)\n",
    "\n",
    "    def nodeClassifier(self, feature):\n",
    "        return self.node_classifier(feature)\n",
    "\n",
    "    def calcularLossAtributo(self, nodo, radio):\n",
    "        if nodo is None:\n",
    "            return\n",
    "        else:\n",
    "            #print(\"radio\", radio)\n",
    "            #print(\"nodo\", nodo)\n",
    "            nodo = torch.stack(nodo)\n",
    "            #print(\"nodo stack\", nodo)\n",
    "            #radio = radio.reshape(-1,4)\n",
    "        \n",
    "            #return mse\n",
    "            #return torch.cat([self.mseLoss(b, gt) for b, gt in zip(radio, nodo)], 0)\n",
    "            z = zip(radio.reshape(-1,4), nodo.reshape(-1,4))\n",
    "            \n",
    "            #for b, gt in z:\n",
    "            #    print(\"bgt\", b, gt)\n",
    "                \n",
    "            \n",
    "            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\n",
    "            #print(\"loss\", l)\n",
    "            return l\n",
    "\n",
    "\n",
    "    def classifyLossEstimator(self, label_vector, original):\n",
    "        if original is None:\n",
    "            return\n",
    "        else:\n",
    "           \n",
    "            v = []\n",
    "            for o in original:\n",
    "                if o == 0:\n",
    "                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\n",
    "                if o == 1:\n",
    "                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\n",
    "                if o == 2:\n",
    "                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\n",
    "                v.append(vector)\n",
    "\n",
    "            v = torch.stack(v)\n",
    "            z = zip(label_vector.reshape(-1,3), v.reshape(-1,3))   \n",
    "            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\n",
    "            \n",
    "            return l\n",
    "            #return c\n",
    "       # return torch.cat([self.creLoss(l.unsqueeze(0), gt).mul(0.2) for l, gt in zip(label_vector, gt_label_vector)], 0)\n",
    "\n",
    "    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\n",
    "        \n",
    "        v = v1.add(v2)\n",
    "        #print(\"v0\", v)\n",
    "        if v3 is not None:\n",
    "            v = v.add(v3)\n",
    "        #print(\"v0\", v)\n",
    "        if v4 is not None:\n",
    "            v = v.add(v4)\n",
    "       \n",
    "        return v\n",
    "if qzero == 0:\n",
    "    qzero = 1\n",
    "if qOne == 0:\n",
    "    qOne = 1\n",
    "if qtwo == 0:\n",
    "    qtwo = 1\n",
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "Grassdecoder = GRASSDecoder(latent_size=128, hidden_size=256, mult = mult)\n",
    "Grassdecoder = Grassdecoder.to(device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(Decoder, self).__init__()\n",
    "        #self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.lp3 = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "        self.mlp_left = nn.Linear(latent_size, hidden_size)\n",
    "        self.mlp_left2 = nn.Linear(hidden_size, latent_size)\n",
    "        #self.mlp_left3 = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, hidden_size)\n",
    "        self.mlp_right2 = nn.Linear(hidden_size, latent_size)\n",
    "        #self.mlp_right3 = nn.Linear(latent_size, latent_size)\n",
    "\n",
    "\n",
    "        self.mlp2 = nn.Linear(latent_size,latent_size)\n",
    "        self.mlp3 = nn.Linear(latent_size,4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def common_branch(self, parent_feature):\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.lp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.lp3(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        return vector\n",
    "\n",
    "    def attr_branch(self, vector):\n",
    "        vector = self.mlp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp3(vector)        \n",
    "        return vector\n",
    "\n",
    "    def right_branch(self, vector):\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        right_feature = self.mlp_right2(right_feature)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        #right_feature = self.mlp_right3(right_feature)\n",
    "        #right_feature = self.tanh(right_feature)\n",
    "        return right_feature\n",
    "\n",
    "    def left_branch(self, vector):\n",
    "        left_feature = self.mlp_left(vector)\n",
    "        left_feature = self.tanh(left_feature)\n",
    "        left_feature = self.mlp_left2(left_feature)\n",
    "        left_feature = self.tanh(left_feature)\n",
    "        #left_feature = self.mlp_left3(left_feature)\n",
    "        #left_feature = self.tanh(left_feature)\n",
    "        return left_feature\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "      \n",
    "        vector      = self.common_branch(parent_feature)\n",
    "        attr_vector = self.attr_branch(vector)\n",
    "        return attr_vector \n",
    "\n",
    "    def forward1(self, parent_feature):\n",
    "    \n",
    "\n",
    "        vector       = self.common_branch(parent_feature)\n",
    "        attr_vector  = self.attr_branch(vector)\n",
    "        right_vector = self.right_branch(vector)\n",
    "        \n",
    "        #print(\"right vector\", right_vector)\n",
    "        #print(\"radius\", attr_vector)\n",
    "        return right_vector, attr_vector\n",
    "\n",
    "    def forward2(self, parent_feature):\n",
    "       \n",
    "\n",
    "        vector       = self.common_branch(parent_feature)\n",
    "        attr_vector  = self.attr_branch(vector)\n",
    "        right_vector = self.right_branch(vector)\n",
    "        left_vector  = self.left_branch(vector)\n",
    "        #print(\"left vector\", left_vector)\n",
    "        #print(\"right vector\", right_vector)\n",
    "        #print(\"radius\", attr_vector)\n",
    "        return left_vector, right_vector, attr_vector\n",
    "\n",
    "\n",
    "\n",
    "class GRASSDecoder(nn.Module):\n",
    "    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\n",
    "        super(GRASSDecoder, self).__init__()\n",
    "        self.decoder = Decoder(latent_size, hidden_size)\n",
    "        self.node_classifier = NodeClassifier(latent_size, hidden_size)\n",
    "        self.mseLoss = nn.MSELoss()  # pytorch's mean squared error loss\n",
    "        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch's cross entropy loss (NOTE: no softmax is needed before)\n",
    "        \n",
    "\n",
    "\n",
    "    def featureDecoder(self, feature):\n",
    "        return self.decoder.forward(feature)\n",
    "\n",
    "    def internalDecoder(self, feature):\n",
    "        return self.decoder.forward1(feature)\n",
    "\n",
    "    def bifurcationDecoder(self, feature):\n",
    "        return self.decoder.forward2(feature)\n",
    "\n",
    "    def nodeClassifier(self, feature):\n",
    "        return self.node_classifier(feature)\n",
    "\n",
    "    def calcularLossAtributo(self, nodo, radio):\n",
    "        #print(\"nodo\", nodo)\n",
    "        #print(\"radio\", radio)\n",
    "        a, b = list(zip(*nodo))# a son los atributos, b los pesos\n",
    "        if nodo is None:\n",
    "            return\n",
    "        else:\n",
    "            nodo = torch.stack(list(a))\n",
    "        \n",
    "            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\n",
    "            #print(\"mse\", l)\n",
    "            return l\n",
    "\n",
    "\n",
    "    def classifyLossEstimator(self, label_vector, original):\n",
    "        if original is None:\n",
    "            return\n",
    "        else:\n",
    "           \n",
    "            v = []\n",
    "            for o in original:\n",
    "                if o == 0:\n",
    "                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\n",
    "                if o == 1:\n",
    "                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\n",
    "                if o == 2:\n",
    "                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\n",
    "                v.append(vector)\n",
    "            \n",
    "\n",
    "            v = torch.stack(v)\n",
    "            \n",
    "            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\n",
    "         \n",
    "\n",
    "            return l\n",
    "            #return c\n",
    "\n",
    "    '''\n",
    "    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\n",
    "        \n",
    "        print(\"loss estructura\", v1)\n",
    "        print(\"loss atributo\", v2)\n",
    "        print(\"right loss\", v3)\n",
    "        print(\"left loss\", v4)\n",
    "\n",
    "\n",
    "        v = v1.add(v2)\n",
    "        #print(\"v0\", v)\n",
    "        if v3 is not None:\n",
    "            v = v.add(v3)\n",
    "        #print(\"v0\", v)\n",
    "        if v4 is not None:\n",
    "            v = v.add(v4)\n",
    "       \n",
    "        return v\n",
    "\n",
    "    '''\n",
    "    def vectorAdder(self, v1, v2):\n",
    "        v = v1.add(v2)\n",
    "        return v\n",
    "\n",
    "    def vectorMult(self, m, v):\n",
    "        #print(\"v\", v)\n",
    "        #print(\"m\", m)\n",
    "        z = zip(v, m)\n",
    "        r = []\n",
    "        for c, d in z:\n",
    "            #print(\"v\", c)\n",
    "            #print(\"m\", d)\n",
    "            r.append(torch.mul(c, d))\n",
    "        #res = [torch.mul(v, m) for v, m in zip(v, m)]\n",
    "        #print(\"res\", r)\n",
    "        return r\n",
    "\n",
    "if round(qzero) == 0:\n",
    "    qzero = 1\n",
    "if round(qOne) == 0:\n",
    "    qOne = 1\n",
    "if round(qtwo) == 0:\n",
    "    qtwo = 1\n",
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "\n",
    "Grassdecoder = GRASSDecoder(latent_size=512, hidden_size=1024, mult = mult)\n",
    "Grassdecoder = Grassdecoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "0.6\n",
      "1.2\n"
     ]
    }
   ],
   "source": [
    "print(qzero)\n",
    "print(qOne)\n",
    "print(qtwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3333, 1.0000, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "print(mult)\n",
    "def calcularLossEstructura(cl_p, original):\n",
    "    \n",
    "    if original is None:\n",
    "        return\n",
    "    mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "    ce = nn.CrossEntropyLoss(weight = mult)\n",
    "\n",
    "    if original.childs() == 0:\n",
    "        vector = [1, 0, 0] \n",
    "    if original.childs() == 1:\n",
    "        vector = [0, 1, 0]\n",
    "    if original.childs() == 2:\n",
    "        vector = [0, 0, 1] \n",
    "\n",
    "    #print(\"original\", vector)\n",
    "    #print(\"prediction\", cl_p)\n",
    "    c = ce(cl_p, torch.tensor(vector, device=device, dtype = torch.float).reshape(1, 3))\n",
    "    #print(\"ce\", 0.4*c)\n",
    "    return c\n",
    "\n",
    "\n",
    "def calcularLossAtributo(nodo, radio):\n",
    "    if nodo is None:\n",
    "        return\n",
    "    #print(\"nodo\", nodo)\n",
    "    #print(\"radio\", radio)\n",
    "\n",
    "    radio = radio.reshape(-1,4)\n",
    "    nodo = nodo.radius.reshape(-1,4)\n",
    "    l2    = nn.MSELoss()\n",
    "   \n",
    "    mse = l2(radio, nodo)\n",
    "    #print(\"mse\", mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchNode(node, key):\n",
    "     \n",
    "    if (node == None):\n",
    "        return False\n",
    " \n",
    "    if (node.data == key):\n",
    "        return node\n",
    "        \n",
    " \n",
    "    \"\"\" then recur on left subtree \"\"\"\n",
    "    res1 = searchNode(node.left, key)\n",
    "    # node found, no need to look further\n",
    "    if res1:\n",
    "        return res1\n",
    " \n",
    "    \"\"\" node is not found in left,\n",
    "    so recur on right subtree \"\"\"\n",
    "    res2 = searchNode(node.right, key)\n",
    "    return res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_nodes 3\n"
     ]
    }
   ],
   "source": [
    "def getLevelUtil(node, data, level):\n",
    "    if (node == None):\n",
    "        return 0\n",
    " \n",
    "    if (node.data == data):\n",
    "        return level\n",
    " \n",
    "    downlevel = getLevelUtil(node.left, data, level + 1)\n",
    "\n",
    "    if (downlevel != 0):\n",
    "        return downlevel\n",
    " \n",
    "    downlevel = getLevelUtil(node.right, data, level + 1)\n",
    "    return downlevel\n",
    " \n",
    "# Returns level of given data value\n",
    " \n",
    " \n",
    "def getLevel(node, data):\n",
    "    return getLevelUtil(node, data, 1)\n",
    " \n",
    "\n",
    "c = []\n",
    "n_nodes = input.count_nodes(input, c)\n",
    "print(\"n_nodes\", len(n_nodes))\n",
    "for x in range(0, len(n_nodes)):\n",
    "        level = getLevel(input, x)\n",
    "        if (level):\n",
    "            #print(\"Level of\", x, \"is\", getLevel(input, x))\n",
    "            node = searchNode(input, x)\n",
    "            node.level = getLevel(input, x)\n",
    "        else:\n",
    "            print(x, \"is not present in tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Node object at 0x000002492506E560>\n",
      "tree level 11\n",
      "<__main__.Node object at 0x0000024924F1B0D0>\n",
      "tree level 11\n",
      "<__main__.Node object at 0x000002492506CEB0>\n",
      "tree level 14\n",
      "<__main__.Node object at 0x0000024924F5A710>\n",
      "tree level 6\n",
      "<__main__.Node object at 0x0000024924F1BB80>\n",
      "tree level 11\n",
      "<__main__.Node object at 0x0000024924F59A50>\n",
      "tree level 11\n",
      "<__main__.Node object at 0x000002492506E2F0>\n",
      "tree level 11\n",
      "<__main__.Node object at 0x00000249057986A0>\n",
      "tree level 6\n",
      "<__main__.Node object at 0x0000024924F1BBE0>\n",
      "tree level 11\n",
      "<__main__.Node object at 0x000002492506E0E0>\n",
      "tree level 14\n"
     ]
    }
   ],
   "source": [
    "for d in data_loader:\n",
    "    for data in d:\n",
    "        print(data)\n",
    "        count = []\n",
    "        numerar_nodos(data, count)\n",
    "        c = []\n",
    "        n_nodes = data.count_nodes(data, c)\n",
    "        for x in range(0, len(n_nodes)):\n",
    "            level = getLevel(data, x)\n",
    "            if (level):\n",
    "                #print(\"Level of\", x, \"is\", getLevel(input, x))\n",
    "                node = searchNode(data, x)\n",
    "                node.level = getLevel(data, x)\n",
    "            else:\n",
    "                print(x, \"is not present in tree\")\n",
    "        tree_level = []\n",
    "        data.get_tree_level(data, tree_level)\n",
    "        print(\"tree level\", sum(tree_level))\n",
    "        data.set_tree_level(data, sum(tree_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_structure_fold_grass(fold, v, root):\n",
    "   \n",
    "    def decode_node(fold, v, node):\n",
    "        \n",
    "        \n",
    "        if node.childs() == 0 : \n",
    "\n",
    "            radio = fold.add('featureDecoder', v)\n",
    "            lossAtributo = fold.add('calcularLossAtributo', node, radio)\n",
    "\n",
    "            label = fold.add('nodeClassifier', v)\n",
    "            \n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)  \n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            \n",
    "            loss =  fold.add('vectorAdder', losse, lossAtributo)       \n",
    "            return loss\n",
    "\n",
    "            \n",
    "            \n",
    "        elif node.childs() == 1 :\n",
    "            right, radius = fold.add('internalDecoder', v).split(2)\n",
    "            label = fold.add('nodeClassifier', v)\n",
    "            nodoSiguiente = node.right\n",
    "            if nodoSiguiente is not None:\n",
    "                right_loss = decode_node(fold, right, nodoSiguiente)\n",
    "\n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)\n",
    "            lossAtributo = fold.add('calcularLossAtributo', node, radius)\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            loss = fold.add('vectorAdder', losse, lossAtributo)\n",
    "            \n",
    "        \n",
    "            loss2 = fold.add('vectorAdder', loss, right_loss)\n",
    "            return loss2\n",
    "            \n",
    "            \n",
    "\n",
    "        elif node.childs() == 2 :\n",
    "            left, right, radius = fold.add('bifurcationDecoder', v).split(3)\n",
    "            \n",
    "            label = fold.add('nodeClassifier', v)            \n",
    "            \n",
    "            nodoSiguienteRight = node.right\n",
    "            nodoSiguienteLeft = node.left\n",
    "\n",
    "\n",
    "            if nodoSiguienteRight is not None:\n",
    "                right_loss = decode_node(fold, right, nodoSiguienteRight)\n",
    "             \n",
    "            if nodoSiguienteLeft is not None:\n",
    "                left_loss  = decode_node(fold, left, nodoSiguienteLeft)\n",
    "\n",
    "            multipl = node.level/node.treelevel\n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            lossAtributo   = fold.add('calcularLossAtributo', node, radius)\n",
    "            loss = fold.add('vectorAdder', losse, lossAtributo)\n",
    "            loss2 = fold.add('vectorAdder', loss, right_loss)\n",
    "            loss3 = fold.add('vectorAdder', loss2, left_loss)\n",
    "            \n",
    "            return loss3\n",
    "            \n",
    "\n",
    "    dec = decode_node (fold, v, root)\n",
    "    return dec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_testing_grass(v, root, max, decoder):\n",
    "    def decode_node(v, node, max, decoder):\n",
    "        cl = decoder.nodeClassifier(v)\n",
    "        _, label = torch.max(cl, 1)\n",
    "        label = label.data\n",
    "        \n",
    "        \n",
    "        if label == 0 and createNode.count <= max: ##output del classifier\n",
    "           \n",
    "            radio = decoder.featureDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node, radio )\n",
    "            if lossEstructura is not None:\n",
    "                multipl = node.level/node.treelevel\n",
    "                lossEstructura = multipl*lossEstructura\n",
    "            \n",
    "            return createNode(1,radio, ce = lossEstructura,  mse = lossAtrs)\n",
    "\n",
    "        elif label == 1 and createNode.count <= max:\n",
    "       \n",
    "            right, radius = decoder.internalDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node, radius )\n",
    "            if lossEstructura is not None:\n",
    "                multipl = node.level/node.treelevel\n",
    "                lossEstructura = multipl*lossEstructura\n",
    "            d = createNode(1, radius, ce = lossEstructura,  mse = lossAtrs) \n",
    "           \n",
    "            if not node is None:\n",
    "                if not node.right is None:\n",
    "                    nodoSiguiente = node.right\n",
    "                else:\n",
    "                    nodoSiguiente = None\n",
    "            else:\n",
    "                nodoSiguiente = None\n",
    "            \n",
    "            d.right = decode_node(right, nodoSiguiente, max, decoder)\n",
    "            \n",
    "\n",
    "            return d\n",
    "       \n",
    "        elif label == 2 and createNode.count <= max:\n",
    "            left, right, radius = decoder.bifurcationDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node, radius )\n",
    "            if lossEstructura is not None:\n",
    "                multipl = node.level/node.treelevel\n",
    "                lossEstructura = multipl*lossEstructura\n",
    "            \n",
    "            d = createNode(1, radius, ce = lossEstructura,  mse = lossAtrs )\n",
    "  \n",
    "            if not node is None: #el nodo existe, me fijo si tiene hijo der/izq\n",
    "                if not node.right is None:\n",
    "                    nodoSiguienteRight = node.right\n",
    "                else:\n",
    "                    nodoSiguienteRight = None\n",
    "                if not node.left is None:\n",
    "                    nodoSiguienteLeft = node.left\n",
    "                else:\n",
    "                    nodoSiguienteLeft = None\n",
    "            else: #el nodo no existe\n",
    "                nodoSiguienteRight = None\n",
    "                nodoSiguienteLeft = None\n",
    "            \n",
    "            d.right = decode_node(right, nodoSiguienteRight, max, decoder)\n",
    "            d.left = decode_node(left, nodoSiguienteLeft, max, decoder)\n",
    "            \n",
    "           \n",
    "            return d\n",
    "            \n",
    "    createNode.count = 0\n",
    "    dec = decode_node (v, root, max, decoder)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, encoder, decoder, optimizer\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            #print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            #print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            #'classifier_state_dict': classifier.state_dict(),\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                \n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, 'outputs/best_model.pth')\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_structure_fold_(v, root):\n",
    "    \n",
    "    def decode_node(v, node):\n",
    "        cl = Grassdecoder.nodeClassifier(v)\n",
    "        _, label = torch.max(cl, 1)\n",
    "        label = label.data\n",
    "\n",
    "        \n",
    "        if node.childs() == 0 : ##output del classifier\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            radio = Grassdecoder.featureDecoder(v)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radio )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "            nd = createNode(1,radio, ce = losse,  mse = lossAtrs)\n",
    "            return nd\n",
    "\n",
    "        elif node.childs() == 1 :\n",
    "        \n",
    "            right, radius = Grassdecoder.internalDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radius )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "            nd = createNode(1, radius, cl_prob = lossAtrs , ce = losse, mse = lossAtrs) \n",
    "            \n",
    "            nodoSiguiente = node.right\n",
    "           \n",
    "            if nodoSiguiente is not None:\n",
    "                nd.right = decode_node(right, nodoSiguiente)\n",
    "               \n",
    "            return nd\n",
    "\n",
    "        elif node.childs() == 2 :\n",
    "            left, right, radius = Grassdecoder.bifurcationDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radius )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "            nd = createNode(1, radius, cl_prob = lossAtrs, ce = losse, mse = lossAtrs)\n",
    "            \n",
    "            nodoSiguienteRight = node.right\n",
    "            nodoSiguienteLeft = node.left\n",
    "\n",
    "            \n",
    "            if nodoSiguienteRight is not None:\n",
    "                nd.right = decode_node(right, nodoSiguienteRight)\n",
    "             \n",
    "            if nodoSiguienteLeft is not None:\n",
    "                nd.left  = decode_node(left, nodoSiguienteLeft)\n",
    "            \n",
    "            return nd\n",
    "            \n",
    "    createNode.count = 0\n",
    "    dec = decode_node (v, root)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder con batch - decoder con batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:15f4x2m2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20bc47243b84571978318f684e57e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.015 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.083234â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lilac-jazz-22</strong>: <a href=\"https://wandb.ai/paufeldman/autoencoder4/runs/15f4x2m2\" target=\"_blank\">https://wandb.ai/paufeldman/autoencoder4/runs/15f4x2m2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221201_171115-15f4x2m2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:15f4x2m2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5406103136ba4b6c93eb9cbb9ccc99cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\wandb\\run-20221201_171246-2fynk57e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/paufeldman/autoencoder4/runs/2fynk57e\" target=\"_blank\">vivid-wind-23</a></strong> to <a href=\"https://wandb.ai/paufeldman/autoencoder4\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/paufeldman/autoencoder4/runs/2fynk57e?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2492506f8b0>"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 8000\n",
    "learning_rate = 1e-5\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "#opt = torch.optim.Adam(params, lr=learning_rate, weight_decay=0.0001) \n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "#opt = torch.optim.SGD(params, lr=learning_rate, momentum = 0.96) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[100], gamma=0.2)\n",
    "import wandb\n",
    "config = {\n",
    "  \"learning_rate\": learning_rate,\n",
    "  \"epochs\": epochs,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"dataset\": t_list,\n",
    "  \"number of trees\": len(data_loader)*batch_size,\n",
    "  \"optim\": opt\n",
    "}\n",
    "wandb.init(project=\"autoencoder4\", entity=\"paufeldman\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 8000] average reconstruction error: 0.3045391142 \n",
      "Epoch [11 / 8000] average reconstruction error: 0.2883607447 \n",
      "Epoch [21 / 8000] average reconstruction error: 0.2714864910 \n",
      "Epoch [31 / 8000] average reconstruction error: 0.2519232631 \n",
      "Epoch [41 / 8000] average reconstruction error: 0.2266475409 \n",
      "Epoch [51 / 8000] average reconstruction error: 0.1907738000 \n",
      "Epoch [61 / 8000] average reconstruction error: 0.1356194317 \n",
      "Epoch [71 / 8000] average reconstruction error: 0.0689624548 \n",
      "Epoch [81 / 8000] average reconstruction error: 0.0447753482 \n",
      "Epoch [91 / 8000] average reconstruction error: 0.0375687443 \n",
      "Epoch [101 / 8000] average reconstruction error: 0.0333281159 \n",
      "Epoch [111 / 8000] average reconstruction error: 0.0314875916 \n",
      "Epoch [121 / 8000] average reconstruction error: 0.0294374209 \n",
      "Epoch [131 / 8000] average reconstruction error: 0.0274171382 \n",
      "Epoch [141 / 8000] average reconstruction error: 0.0254498851 \n",
      "Epoch [151 / 8000] average reconstruction error: 0.0237424020 \n",
      "Epoch [161 / 8000] average reconstruction error: 0.0224016551 \n",
      "Epoch [171 / 8000] average reconstruction error: 0.0213749669 \n",
      "Epoch [181 / 8000] average reconstruction error: 0.0205914602 \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl Kernel se bloqueÃ³ al ejecutar cÃ³digo en la celda actual o en una celda anterior. Revise el cÃ³digo de las celdas para identificar una posible causa del error. Haga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquÃ­</a> para obtener mÃ¡s informaciÃ³n. Vea el [registro] de Jupyter (command:jupyter.viewOutput) para obtener mÃ¡s detalles."
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        enc_fold = torch_f.Fold(device)\n",
    "        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "        # Collect computation nodes recursively from encoding process\n",
    "        n_nodes = []\n",
    "        for example in batch: #example es un arbolito\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            encode_structure_fold(enc_fold, example)\n",
    "            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "       \n",
    "        # Apply the computations on the encoder model\n",
    "       \n",
    "        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "        \n",
    "        \n",
    "        # Initialize torchfold for *decoding*\n",
    "        dec_fold = torch_f.Fold(device)\n",
    "        # Collect computation nodes recursively from decoding process\n",
    "        dec_fold_nodes = []\n",
    "        kld_fold_nodes = []\n",
    "\n",
    "        t_l = []\n",
    "        for f in enc_fold_nodes:\n",
    "            for t in f:\n",
    "                t_l.append(t)\n",
    "        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\n",
    "            #print(\"example\", example)\n",
    "            #print(\"fnode\", fnode) \n",
    "            #root_code, kl_div = torch.chunk(fnode, 2, 0)\n",
    "            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\n",
    "        # Apply the computations on the decoder model\n",
    "\n",
    "                       \n",
    "        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        #print(\"n\", n_nodes)\n",
    "        total_loss = torch.div(total_loss[0], n_nodes)\n",
    "        #print(\"div\", total_loss)\n",
    "        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        #total_loss = total_loss*10\n",
    "        \n",
    "        #print(\"total_loss\", total_loss)\n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %.10f ' % (epoch+1, epochs, total_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder con batch - decoder sin batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        # Initialize torchfold for *encoding*\\n\\n        \\n        enc_fold = torch_f.Fold(device)\\n        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\\n        # Collect computation nodes recursively from encoding process\\n        n_nodes = []\\n        for example in batch: #example es un arbolito\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            encode_structure_fold(enc_fold, example)\\n            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\\n       \\n        # Apply the computations on the encoder model\\n       \\n        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\\n        encodeado_con_batch = enc_fold_nodes\\n        \\n        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\\n        #print(\"decoded\", decoded)\\n        l = []\\n        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\\n        l = []\\n        ce_loss_list = decoded.traverseInorderCE(decoded, l)\\n            \\n        mse_loss = sum(mse_loss_list) \\n        ce_loss  = sum(ce_loss_list)  \\n        total_loss = (0.5*ce_loss + mse_loss)\\n        #print(\"total_loss\", total_loss)\\n        total_loss = total_loss / len(mse_loss_list)\\n        \\n        \\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        enc_fold = torch_f.Fold(device)\n",
    "        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "        # Collect computation nodes recursively from encoding process\n",
    "        n_nodes = []\n",
    "        for example in batch: #example es un arbolito\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            encode_structure_fold(enc_fold, example)\n",
    "            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "       \n",
    "        # Apply the computations on the encoder model\n",
    "       \n",
    "        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "        encodeado_con_batch = enc_fold_nodes\n",
    "        \n",
    "        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\n",
    "        #print(\"decoded\", decoded)\n",
    "        l = []\n",
    "        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\n",
    "        l = []\n",
    "        ce_loss_list = decoded.traverseInorderCE(decoded, l)\n",
    "            \n",
    "        mse_loss = sum(mse_loss_list) \n",
    "        ce_loss  = sum(ce_loss_list)  \n",
    "        total_loss = (0.5*ce_loss + mse_loss)\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        total_loss = total_loss / len(mse_loss_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder sin batch - decoder con batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        # Initialize torchfold for *encoding*\\n\\n        \\n        \\n        enc_fold_nodes = []\\n        n_nodes = []\\n        for example in batch:\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            enc_fold = encode_structure(example).to(device)\\n        #print(\"encodeado sin batch\", enc_fold)\\n        enc_fold_nodes.append(enc_fold)\\n        encodeado_sin_batch = enc_fold\\n        # Split into a list of fold nodes per example\\n        #enc_fold_nodes = torch.split(enc_fold_nodes[0], 1, 0) #divide ele ncodeado en vectores de un elemento\\n        \\n        \\n        # Initialize torchfold for *decoding*\\n        dec_fold = torch_f.Fold(device)\\n        # Collect computation nodes recursively from decoding process\\n        dec_fold_nodes = []\\n        kld_fold_nodes = []\\n\\n        t_l = []\\n        for f in enc_fold_nodes:\\n            for t in f:\\n                t_l.append(t)\\n        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\\n            #print(\"example\", example)\\n            #print(\"fnode\", fnode) \\n            #root_code, kl_div = torch.chunk(fnode, 2, 0)\\n            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\\n        # Apply the computations on the decoder model\\n        #print(\"dec fold nodes\", dec_fold_nodes)\\n           \\n                       \\n        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\\n        #print(\"total_loss\", total_loss)\\n        n_nodes = torch.tensor(n_nodes, device = device)\\n        #print(\"n\", n_nodes)\\n        total_loss = torch.div(total_loss[0], n_nodes)\\n        #print(\"div\", total_loss)\\n        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\\n        #print(\"total_loss\", total_loss)\\n        \\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        mse_loss_avg[-1] += (mse_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        \n",
    "        enc_fold_nodes = []\n",
    "        n_nodes = []\n",
    "        for example in batch:\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            enc_fold = encode_structure(example).to(device)\n",
    "        #print(\"encodeado sin batch\", enc_fold)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "        encodeado_sin_batch = enc_fold\n",
    "        # Split into a list of fold nodes per example\n",
    "        #enc_fold_nodes = torch.split(enc_fold_nodes[0], 1, 0) #divide ele ncodeado en vectores de un elemento\n",
    "        \n",
    "        \n",
    "        # Initialize torchfold for *decoding*\n",
    "        dec_fold = torch_f.Fold(device)\n",
    "        # Collect computation nodes recursively from decoding process\n",
    "        dec_fold_nodes = []\n",
    "        kld_fold_nodes = []\n",
    "\n",
    "        t_l = []\n",
    "        for f in enc_fold_nodes:\n",
    "            for t in f:\n",
    "                t_l.append(t)\n",
    "        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\n",
    "            #print(\"example\", example)\n",
    "            #print(\"fnode\", fnode) \n",
    "            #root_code, kl_div = torch.chunk(fnode, 2, 0)\n",
    "            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\n",
    "        # Apply the computations on the decoder model\n",
    "        #print(\"dec fold nodes\", dec_fold_nodes)\n",
    "           \n",
    "                       \n",
    "        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        #print(\"n\", n_nodes)\n",
    "        total_loss = torch.div(total_loss[0], n_nodes)\n",
    "        #print(\"div\", total_loss)\n",
    "        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        mse_loss_avg[-1] += (mse_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder sin batch - decoder sin batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n    ce_avg.append(0)\\n    mse_avg.append(0)\\n\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        \\n        enc_fold_nodes = []\\n        n_nodes = []\\n        for example in batch:\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            enc_fold = encode_structure(example).to(device)\\n        #print(\"encodeado sin batch\", enc_fold)\\n        enc_fold_nodes.append(enc_fold)\\n        encodeado_sin_batch = enc_fold\\n        \\n        \\n        \\n        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\\n        #print(\"decoded\", decoded)\\n        l = []\\n        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\\n        l = []\\n        ce_loss_list = decoded.traverseInorderCE(decoded, l)\\n            \\n        mse_loss = sum(mse_loss_list) \\n        ce_loss  = sum(ce_loss_list)  \\n       \\n        ce = [0.4*a for a in ce_loss_list]\\n\\n\\n        total_loss = (0.4*ce_loss + mse_loss)\\n        total_loss = total_loss / len(mse_loss_list)\\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        mse_avg[-1] += (mse_loss.item())\\n        ce_avg[-1] += (ce_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss, \\'mse loss\\': mse_loss, \\'ce loss\\': ce_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "    ce_avg.append(0)\n",
    "    mse_avg.append(0)\n",
    "\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "        enc_fold_nodes = []\n",
    "        n_nodes = []\n",
    "        for example in batch:\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            enc_fold = encode_structure(example).to(device)\n",
    "        #print(\"encodeado sin batch\", enc_fold)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "        encodeado_sin_batch = enc_fold\n",
    "        \n",
    "        \n",
    "        \n",
    "        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\n",
    "        #print(\"decoded\", decoded)\n",
    "        l = []\n",
    "        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\n",
    "        l = []\n",
    "        ce_loss_list = decoded.traverseInorderCE(decoded, l)\n",
    "            \n",
    "        mse_loss = sum(mse_loss_list) \n",
    "        ce_loss  = sum(ce_loss_list)  \n",
    "       \n",
    "        ce = [0.4*a for a in ce_loss_list]\n",
    "\n",
    "\n",
    "        total_loss = (0.4*ce_loss + mse_loss)\n",
    "        total_loss = total_loss / len(mse_loss_list)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        mse_avg[-1] += (mse_loss.item())\n",
    "        ce_avg[-1] += (ce_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss, 'mse loss': mse_loss, 'ce loss': ce_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7967\n"
     ]
    }
   ],
   "source": [
    "encoder = GRASSEncoder(input_size = 4, feature_size=512, hidden_size=1024).to(device)\n",
    "decoder = GRASSDecoder(latent_size=512, hidden_size=1024, mult = mult).to(device)\n",
    "\n",
    "checkpoint = torch.load(\"outputs/best_model.pth\")\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "print(\"epoch\", epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0069, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_best_model.best_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss tensor([8.5589e-07, 1.7449e-02, 6.6508e-03, 4.2564e-03, 3.3420e-03, 5.2276e-03,\n",
      "        1.5637e-02, 1.6244e-08, 3.4647e-02, 2.6735e-08], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "testing loss tensor(0.0087, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_loss_avg.append(0)\n",
    "for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "\n",
    "    enc_fold_nodes = []\n",
    "    n_nodes = []\n",
    "    for example in batch:\n",
    "        c = []\n",
    "        n = example.count_nodes(example, c)\n",
    "        n_nodes.append(len(n))\n",
    "        enc_fold = encode_structure(example).to(device)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "  \n",
    "    dec = []\n",
    "    for encoded in enc_fold_nodes:\n",
    "       dec.append(decode_testing_grass(encoded, input, 100, decoder))\n",
    "    total_loss = dec_fold.apply(decoder, [dec_fold_nodes])#[0]\n",
    "    n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        \n",
    "    total_loss = torch.div(total_loss[0], n_nodes)\n",
    "print(\"testing loss\", total_loss)\n",
    "avg_testing_loss = total_loss.sum() / len(batch) \n",
    "print(\"testing loss\", avg_testing_loss)\n",
    "\n",
    "        #total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[0.0002, 0.0003, 0.0003, 0.0004]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "1 tensor([[0.5003, 0.5002, 0.5003, 0.5003]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = iter(data_loader).next()[0]\n",
    "enc_fold = torch_f.Fold(device)\n",
    "enc_fold_nodes = []\n",
    "enc_fold_nodes.append(encode_structure_fold(enc_fold, input))\n",
    "enc_fold_nodes = enc_fold.apply(encoder, [enc_fold_nodes])\n",
    "encoded = enc_fold_nodes[0]\n",
    "decoded = decode_testing_grass(encoded, input, 100, decoder)\n",
    "\n",
    "count = []\n",
    "numerar_nodos(decoded, count)\n",
    "decoded.traverseInorder(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd31839813a4983a550321cfce29d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.5, 0.5,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTree(input, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2435e3e7ebc549e3b63ec72bf86f0f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.2502539â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTree(decoded, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "n_nodes = input.count_nodes(input,c)\n",
    "len(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "n_nodes = decoded.count_nodes(decoded,c)\n",
    "len(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0 2\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "decoded.traverseInorderChilds(decoded, li)\n",
    "zero = [a for a in li if a == 0]\n",
    "one = [a for a in li if a == 1]\n",
    "two = [a for a in li if a == 2]\n",
    "qzero = len(zero)\n",
    "qOne = len(one)\n",
    "qtwo = len(two)\n",
    "print(qzero, qOne, qtwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1 1\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "input.traverseInorderChilds(input, li)\n",
    "zero = [a for a in li if a == 0]\n",
    "one = [a for a in li if a == 1]\n",
    "two = [a for a in li if a == 2]\n",
    "qzero = len(zero)\n",
    "qOne = len(one)\n",
    "qtwo = len(two)\n",
    "print(qzero, qOne, qtwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (20,10))\n",
    "plt.plot(train_loss_avg) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverseleaf(root):\n",
    "    if root is not None:\n",
    "        traverseleaf(root.left)\n",
    "        if root.is_leaf():\n",
    "            print(root.radius)\n",
    "        traverseleaf(root.right)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traversebif(root):\n",
    "    if root is not None:\n",
    "        traversebif(root.left)\n",
    "        if root.is_two_child():\n",
    "            print(root.radius)\n",
    "        traversebif(root.right)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4443, 0.1998, 0.2221, 0.4999]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 5.7776e-05, -9.6735e-05, -7.4375e-06,  5.3395e-05]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "////\n",
      "tensor([0., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "traversebif(decoded)\n",
    "print(\"////\")\n",
    "traversebif(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8888, 0.4000, 0.4444, 0.9998]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9443, 0.7000, 0.7222, 0.9998]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5556, 0.8000, 0.7779, 0.5000]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "traverseleaf(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8889, 0.4000, 0.4444, 1.0000], device='cuda:0')\n",
      "tensor([1., 1., 1., 1.], device='cuda:0')\n",
      "tensor([0.5556, 0.8000, 0.7778, 0.5000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "traverseleaf(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py_torc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f3e717cd274da89498094fde320e6eab1bf0f52911d27cf47473187acb3fe8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
