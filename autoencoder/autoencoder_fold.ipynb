{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import raiseExceptions\n",
    "from tokenize import Double\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from vec3 import Vec3\n",
    "import meshplot as mp\n",
    "import torch\n",
    "torch.manual_seed(125)\n",
    "import random\n",
    "random.seed(125)\n",
    "import torch_f as torch_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_fn(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        wrapper.count += 1\n",
    "        return f(*args, **kwargs)\n",
    "    wrapper.count = 0\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase nodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Class Node\n",
    "    \"\"\"\n",
    "    def __init__(self, value, radius, left = None, right = None, position = None, cl_prob= None, ce = None, mse = None, level = None, treelevel = None):\n",
    "        self.left = left\n",
    "        self.data = value\n",
    "        self.radius = radius\n",
    "        self.position = position\n",
    "        self.right = right\n",
    "        self.prob = cl_prob\n",
    "        self.mse = mse\n",
    "        self.ce = ce\n",
    "        self.children = [self.left, self.right]\n",
    "        self.level = level\n",
    "        self.treelevel = treelevel\n",
    "    \n",
    "    def agregarHijo(self, children):\n",
    "\n",
    "        if self.right is None:\n",
    "            self.right = children\n",
    "        elif self.left is None:\n",
    "            self.left = children\n",
    "\n",
    "        else:\n",
    "            raise ValueError (\"solo arbol binario \")\n",
    "\n",
    "\n",
    "    def is_leaf(self):\n",
    "        if self.right is None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_two_child(self):\n",
    "        if self.right is not None and self.left is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_one_child(self):\n",
    "        if self.is_two_child():\n",
    "            return False\n",
    "        elif self.is_leaf():\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def childs(self):\n",
    "        if self.is_leaf():\n",
    "            return 0\n",
    "        if self.is_one_child():\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    \n",
    "    \n",
    "    def traverseInorder(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorder(root.left)\n",
    "            print (root.data, root.radius)\n",
    "            self.traverseInorder(root.right)\n",
    "\n",
    "    def traverseInorderwl(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderwl(root.left)\n",
    "            print (root.data, root.radius, root.level, root.treelevel)\n",
    "            self.traverseInorderwl(root.right)\n",
    "\n",
    "    def get_tree_level(self, root, c):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.get_tree_level(root.left, c)\n",
    "            c.append(root.level)\n",
    "            self.get_tree_level(root.right, c)\n",
    "\n",
    "    def set_tree_level(self, root, c):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.set_tree_level(root.left, c)\n",
    "            root.treelevel = c\n",
    "            self.set_tree_level(root.right, c)\n",
    "\n",
    "    def traverseInorderLoss(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderLoss(root.left, loss)\n",
    "            loss.append(root.prob)\n",
    "            self.traverseInorderLoss(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderMSE(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderMSE(root.left, loss)\n",
    "            loss.append(root.mse)\n",
    "            self.traverseInorderMSE(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderCE(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderCE(root.left, loss)\n",
    "            loss.append(root.ce)\n",
    "            self.traverseInorderCE(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderChilds(self, root, l):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderChilds(root.left, l)\n",
    "            l.append(root.childs())\n",
    "            self.traverseInorderChilds(root.right, l)\n",
    "            return l\n",
    "\n",
    "    def preorder(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            print (root.data, root.radius)\n",
    "            self.preorder(root.left)\n",
    "            self.preorder(root.right)\n",
    "\n",
    "    def cloneBinaryTree(self, root):\n",
    "     \n",
    "        # base case\n",
    "        if root is None:\n",
    "            return None\n",
    "    \n",
    "        # create a new node with the same data as the root node\n",
    "        root_copy = Node(root.data, root.radius)\n",
    "    \n",
    "        # clone the left and right subtree\n",
    "        root_copy.left = self.cloneBinaryTree(root.left)\n",
    "        root_copy.right = self.cloneBinaryTree(root.right)\n",
    "    \n",
    "        # return cloned root node\n",
    "        return root_copy\n",
    "\n",
    "    def height(self, root):\n",
    "    # Check if the binary tree is empty\n",
    "        if root is None:\n",
    "            return 0 \n",
    "        # Recursively call height of each node\n",
    "        leftAns = self.height(root.left)\n",
    "        rightAns = self.height(root.right)\n",
    "    \n",
    "        # Return max(leftHeight, rightHeight) at each iteration\n",
    "        return max(leftAns, rightAns) + 1\n",
    "\n",
    "    # Print nodes at a current level\n",
    "    def printCurrentLevel(self, root, level):\n",
    "        if root is None:\n",
    "            return\n",
    "        if level == 1:\n",
    "            print(root.data, end=\" \")\n",
    "        elif level > 1:\n",
    "            self.printCurrentLevel(root.left, level-1)\n",
    "            self.printCurrentLevel(root.right, level-1)\n",
    "\n",
    "    def printLevelOrder(self, root):\n",
    "        h = self.height(root)\n",
    "        for i in range(1, h+1):\n",
    "            self.printCurrentLevel(root, i)\n",
    "\n",
    "\n",
    "    \n",
    "    def count_nodes(self, root, counter):\n",
    "        if   root is not None:\n",
    "            self.count_nodes(root.left, counter)\n",
    "            counter.append(root.data)\n",
    "            self.count_nodes(root.right, counter)\n",
    "            return counter\n",
    "\n",
    "    \n",
    "    def serialize(self, root):\n",
    "        def post_order(root):\n",
    "            if root:\n",
    "                post_order(root.left)\n",
    "                post_order(root.right)\n",
    "                ret[0] += str(root.data)+'_'+ str(root.radius) +';'\n",
    "                \n",
    "            else:\n",
    "                ret[0] += '#;'           \n",
    "\n",
    "        ret = ['']\n",
    "        post_order(root)\n",
    "        return ret[0][:-1]  # remove last ,\n",
    "\n",
    "    def toGraph( self, graph, index, dec, proc=True):\n",
    "        \n",
    "        \n",
    "        radius = self.radius.cpu().detach().numpy()\n",
    "        if dec:\n",
    "            radius= radius[0]\n",
    "        #print(\"posicion\", self.data, radius)\n",
    "        #print(\"right\", self.right)\n",
    "        \n",
    "        #graph.add_nodes_from( [ (index, {'posicion': radius[0:3], 'radio': radius[3] } ) ])\n",
    "        graph.add_nodes_from( [ (self.data, {'posicion': radius[0:3], 'radio': radius[3] } ) ])\n",
    "        \n",
    "\n",
    "        if self.right is not None:\n",
    "            #leftIndex = self.right.toGraph( graph, index + 1, dec)#\n",
    "            self.right.toGraph( graph, index + 1, dec)#\n",
    "            \n",
    "            #graph.add_edge( index, index + 1 )\n",
    "            graph.add_edge( self.data, self.right.data )\n",
    "            #if proc:\n",
    "            #    nx.set_edge_attributes( graph, {(index, index+1) : {'procesada':False}})\n",
    "        \n",
    "            if self.left is not None:\n",
    "                #retIndex = self.left.toGraph( graph, leftIndex, dec )#\n",
    "                self.left.toGraph( graph, 0, dec )#\n",
    "\n",
    "                #graph.add_edge( index, leftIndex)\n",
    "                graph.add_edge( self.data, self.left.data)\n",
    "                #if proc:\n",
    "                #    nx.set_edge_attributes( graph, {(index, leftIndex) : {'procesada':False}})\n",
    "            \n",
    "            else:\n",
    "                #return leftIndex\n",
    "                return\n",
    "\n",
    "        else:\n",
    "            #return index + 1\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTree( root, dec ):\n",
    "    graph = nx.Graph()\n",
    "    root.toGraph( graph, 0, dec)\n",
    "    edges=nx.get_edge_attributes(graph,'procesada')\n",
    "\n",
    "    p = mp.plot( np.array([ graph.nodes[v]['posicion'] for v in graph.nodes]), shading={'point_size':0.1}, return_plot=True)\n",
    "\n",
    "    for arista in graph.edges:\n",
    "        p.add_lines( graph.nodes[arista[0]]['posicion'], graph.nodes[arista[1]]['posicion'])\n",
    "\n",
    "    return \n",
    "\n",
    "def traverse(root, tree):\n",
    "       \n",
    "        if root is not None:\n",
    "            traverse(root.left, tree)\n",
    "            tree.append((root.radius, root.data))\n",
    "            traverse(root.right, tree)\n",
    "            return tree\n",
    "\n",
    "def traverse_2(tree1, tree2, t_l):\n",
    "       \n",
    "        if tree1 is not None:\n",
    "            traverse_2(tree1.left, tree2.left, t_l)\n",
    "            if tree2:\n",
    "                t_l.append((tree1.radius, tree2.radius))\n",
    "                print((tree1.radius, tree2.radius))\n",
    "            else:\n",
    "                t_l.append(tree1.radius)\n",
    "                print((tree1.radius))\n",
    "            traverse_2(tree1.right, tree2, t_l)\n",
    "            return t_l\n",
    "            \n",
    "\n",
    "def traverse_conexiones(root, tree):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            traverse_conexiones(root.left, tree)\n",
    "            if root.right is not None:\n",
    "                tree.append((root.data, root.right.data))\n",
    "            if root.left is not None:\n",
    "                tree.append((root.data, root.left.data))\n",
    "            traverse_conexiones(root.right, tree)\n",
    "            return tree\n",
    "\n",
    "def arbolAGrafo (nodoRaiz):\n",
    "    \n",
    "    conexiones = []\n",
    "    lineas = traverse_conexiones(nodoRaiz, conexiones)\n",
    "    tree = []\n",
    "    tree = traverse(nodoRaiz, tree)\n",
    "\n",
    "    vertices = []\n",
    "    verticesCrudos = []\n",
    "    for node in tree:\n",
    "        vertice = node[0][0][:3]\n",
    "        rad = node[0][0][-1]\n",
    "        num = node[1]\n",
    "        \n",
    "        #vertices.append((num, {'posicion': Vec3( vertice[0], vertice[1], vertice[2]), 'radio': rad} ))\n",
    "        vertices.append((len(verticesCrudos),{'posicion': Vec3( vertice[0], vertice[1], vertice[2]), 'radio': rad}))\n",
    "        verticesCrudos.append(vertice)\n",
    "\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from( vertices )\n",
    "    G.add_edges_from( lineas )\n",
    "    \n",
    "    return G\n",
    "\n",
    "@count_fn\n",
    "def createNode(data, radius, position = None, left = None, right = None, cl_prob = None, ce = None, mse=None):\n",
    "        \"\"\"\n",
    "        Utility function to create a node.\n",
    "        \"\"\"\n",
    "        return Node(data, radius, position, left, right, cl_prob, ce, mse)\n",
    " \n",
    "def deserialize(data):\n",
    "    if  not data:\n",
    "        return \n",
    "    nodes = data.split(';')  \n",
    "    #print(\"node\",nodes[3])\n",
    "    def post_order(nodes):\n",
    "                \n",
    "        if nodes[-1] == '#':\n",
    "            nodes.pop()\n",
    "            return None\n",
    "        node = nodes.pop().split('_')\n",
    "        data = int(node[0])\n",
    "        #radius = float(node[1])\n",
    "        #print(\"node\", node)\n",
    "        #breakpoint()\n",
    "        radius = node[1]\n",
    "        #print(\"radius\", radius)\n",
    "        rad = radius.split(\",\")\n",
    "        rad [0] = rad[0].replace('[','')\n",
    "        rad [3] = rad[3].replace(']','')\n",
    "        r = []\n",
    "        for value in rad:\n",
    "            r.append(float(value))\n",
    "        #r =[float(num) for num in radius if num.isdigit()]\n",
    "        r = torch.tensor(r, device=device)\n",
    "        #breakpoint()\n",
    "        root = createNode(data, r)\n",
    "        root.right = post_order(nodes)\n",
    "        root.left = post_order(nodes)\n",
    "        \n",
    "        return root    \n",
    "    return post_order(nodes)    \n",
    "\n",
    "\n",
    "def read_tree(filename):\n",
    "    with open('./trees/' +'prof4/' +filename, \"r\") as f:\n",
    "        byte = f.read() \n",
    "        return byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternalEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, feature_size: int, hidden_size: int):\n",
    "        super(InternalEncoder, self).__init__()\n",
    "\n",
    "        #print(\"init\")\n",
    "        # Encoders atributos\n",
    "        self.attribute_lin_encoder_1 = nn.Linear(input_size,hidden_size)\n",
    "        self.attribute_lin_encoder_2 = nn.Linear(hidden_size,feature_size)\n",
    "\n",
    "        # Encoders derecho e izquierdo\n",
    "        self.right_lin_encoder_1 = nn.Linear(feature_size,feature_size)\n",
    "        self.left_lin_encoder_1  = nn.Linear(feature_size,feature_size)\n",
    "\n",
    "        # Encoder final\n",
    "        self.final_lin_encoder_1 = nn.Linear(2*feature_size, feature_size)\n",
    "\n",
    "        # Funciones / Parametros utiles\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.feature_size = feature_size\n",
    "        #print(\"fin\")\n",
    "\n",
    "    def forward(self, input, right_input, left_input):\n",
    "        # Encodeo los atributos\n",
    "        attributes = self.attribute_lin_encoder_1(input)\n",
    "        attributes = self.tanh(attributes)\n",
    "        attributes = self.attribute_lin_encoder_2(attributes)\n",
    "        attributes = self.tanh(attributes)\n",
    "        #print(\"attributes\", attributes)\n",
    "\n",
    "        # Encodeo el derecho\n",
    "        if right_input is not None:\n",
    "            #print(\"right input\", right_input)\n",
    "            context = self.right_lin_encoder_1(right_input)\n",
    "            #print(\"context derecho\", context)\n",
    "            # Encodeo el izquierdo\n",
    "            #print(\"left input\", left_input)\n",
    "            if left_input is not None:\n",
    "                \n",
    "                context += self.left_lin_encoder_1(left_input)\n",
    "                #print(\"context izquierdo\", context.shape)\n",
    "        else:\n",
    "            context = torch.zeros(input.shape[0],self.feature_size, requires_grad=True, device=device)\n",
    "        \n",
    "\n",
    "        context = self.tanh(context)\n",
    "        #print(\"context shape\", context.shape)\n",
    "        #print(\"attributes shape\", attributes.shape)\n",
    "        #if len(attributes.shape) == 1:\n",
    "            #print(\"len(attributes.shape)\",len(attributes.shape))\n",
    "            #attributes = attributes.reshape(1, 128)\n",
    "        #print(\"attributes shape\", attributes.shape)\n",
    "\n",
    "        feature = torch.cat((attributes,context), 1)\n",
    "        #print(\"feature cat\", feature.shape)\n",
    "\n",
    "        feature = self.final_lin_encoder_1(feature)\n",
    "        feature = self.tanh(feature)\n",
    "        #print(\"output\", feature)\n",
    "        return feature\n",
    "\n",
    "       \n",
    "    \n",
    "\n",
    "class GRASSEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, feature_size : int, hidden_size: int):\n",
    "        super(GRASSEncoder, self).__init__()\n",
    "        self.leaf_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        self.internal_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        self.bifurcation_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        \n",
    "    def leafEncoder(self, node, right=None, left = None):\n",
    "        return self.internal_encoder(node, right, left)\n",
    "    def internalEncoder(self, node, right, left = None):\n",
    "        return self.internal_encoder(node, right, left)\n",
    "    def bifurcationEncoder(self, node, right, left):\n",
    "        \n",
    "        return self.bifurcation_encoder(node, right, left)\n",
    "\n",
    "Grassencoder = GRASSEncoder(input_size = 4, feature_size=256, hidden_size=512)\n",
    "Grassencoder = Grassencoder.to(device)\n",
    "\n",
    "\n",
    "def encode_structure_fold(fold, root):\n",
    "    \n",
    "    \n",
    "    def encode_node(node):\n",
    "        \n",
    "        if node is None:\n",
    "            return\n",
    "        \n",
    "        if node.is_leaf():\n",
    "            return fold.add('leafEncoder', node.radius)\n",
    "        else:\n",
    "            left = encode_node(node.left)\n",
    "            right = encode_node(node.right)\n",
    "            if left is not None:\n",
    "             \n",
    "                return fold.add('bifurcationEncoder', node.radius, right, left)\n",
    "            else:\n",
    "                return fold.add('internalEncoder', node.radius, right)\n",
    "        \n",
    "\n",
    "    encoding = encode_node(root)\n",
    "    \n",
    "    return encoding\n",
    "  \n",
    "def encode_structure(root):\n",
    "    \n",
    "    def encode_node(node):\n",
    "          \n",
    "        if node is None:\n",
    "            return\n",
    "        if node.is_leaf():\n",
    "            return Grassencoder.leafEncoder(node.radius.reshape(-1,4))\n",
    "        else :\n",
    "            left = encode_node(node.left)\n",
    "            right = encode_node(node.right)\n",
    "            if left is not None:\n",
    "                return Grassencoder.bifurcationEncoder(node.radius.reshape(-1,4), right, left)\n",
    "            else:\n",
    "                return Grassencoder.internalEncoder(node.radius.reshape(-1,4), right)\n",
    "        \n",
    "\n",
    "    encoding = encode_node(root)\n",
    "   \n",
    "    return encoding\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerar_nodos(root, count):\n",
    "    if root is not None:\n",
    "        numerar_nodos(root.left, count)\n",
    "        root.data = len(count)\n",
    "        count.append(1)\n",
    "        numerar_nodos(root.right, count)\n",
    "        return \n",
    "\n",
    "\n",
    "def traversefeatures(root, features):\n",
    "       \n",
    "    if root is not None:\n",
    "        traversefeatures(root.left, features)\n",
    "        features.append(root.radius)\n",
    "        traversefeatures(root.right, features)\n",
    "        return features\n",
    "\n",
    "def norm(root, minx, miny, minz, minr, maxx, maxy, maxz, maxr):\n",
    "    \n",
    "    if root is not None:\n",
    "        mx = minx.clone().detach()\n",
    "        my = miny.clone().detach()\n",
    "        mz = minz.clone().detach()\n",
    "        mr = minr.clone().detach()\n",
    "        Mx = maxx.clone().detach()\n",
    "        My = maxy.clone().detach()\n",
    "        Mz = maxz.clone().detach()\n",
    "        Mr = maxr.clone().detach()\n",
    "       \n",
    "        root.radius[0] = (root.radius[0] - minx)/(maxx - minx)\n",
    "        root.radius[1] = (root.radius[1] - miny)/(maxy - miny)\n",
    "        root.radius[2] = (root.radius[2] - minz)/(maxz - minz)\n",
    "        root.radius[3] = (root.radius[3] - minr)/(maxr - minr)\n",
    "        \n",
    "        norm(root.left, mx, my, mz, mr, Mx, My, Mz, Mr)\n",
    "        norm(root.right, mx, my, mz, mr, Mx, My, Mz, Mr)\n",
    "        return \n",
    "\n",
    "def normalize_features(root):\n",
    "    features = []\n",
    "    features = traversefeatures(root, features)\n",
    "    \n",
    "    x = [tensor[0] for tensor in features]\n",
    "    y = [tensor[1] for tensor in features]\n",
    "    z = [tensor[2] for tensor in features]\n",
    "    r = [tensor[3] for tensor in features]\n",
    " \n",
    "    norm(root, min(x), min(y), min(z), min(r), max(x), max(y), max(z), max(r))\n",
    "\n",
    "    return \n",
    "\n",
    "def traversefeatures(root, features):\n",
    "       \n",
    "    if root is not None:\n",
    "        traversefeatures(root.left, features)\n",
    "        features.append(root.radius)\n",
    "        traversefeatures(root.right, features)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tree0.dat', 'tree1.dat', 'tree10.dat', 'tree100.dat', 'tree101.dat', 'tree102.dat', 'tree103.dat', 'tree104.dat', 'tree105.dat', 'tree106.dat', 'tree107.dat', 'tree108.dat', 'tree109.dat', 'tree11.dat', 'tree110.dat', 'tree111.dat', 'tree112.dat', 'tree113.dat', 'tree114.dat', 'tree115.dat', 'tree116.dat', 'tree117.dat', 'tree118.dat', 'tree119.dat', 'tree12.dat', 'tree120.dat', 'tree121.dat', 'tree122.dat', 'tree123.dat', 'tree124.dat', 'tree125.dat', 'tree126.dat', 'tree127.dat', 'tree128.dat', 'tree129.dat', 'tree13.dat', 'tree130.dat', 'tree131.dat', 'tree132.dat', 'tree133.dat', 'tree134.dat', 'tree135.dat', 'tree136.dat', 'tree137.dat', 'tree138.dat', 'tree139.dat', 'tree14.dat', 'tree140.dat', 'tree141.dat', 'tree142.dat', 'tree143.dat', 'tree144.dat', 'tree145.dat', 'tree146.dat', 'tree147.dat', 'tree148.dat', 'tree149.dat', 'tree15.dat', 'tree150.dat', 'tree151.dat', 'tree152.dat', 'tree153.dat', 'tree154.dat', 'tree155.dat', 'tree156.dat', 'tree157.dat', 'tree158.dat', 'tree159.dat', 'tree16.dat', 'tree160.dat', 'tree161.dat', 'tree162.dat', 'tree163.dat', 'tree164.dat', 'tree165.dat', 'tree166.dat', 'tree167.dat', 'tree168.dat', 'tree169.dat', 'tree17.dat', 'tree170.dat', 'tree171.dat', 'tree172.dat', 'tree173.dat', 'tree174.dat', 'tree175.dat', 'tree176.dat', 'tree177.dat', 'tree178.dat', 'tree179.dat', 'tree18.dat', 'tree180.dat', 'tree181.dat', 'tree182.dat', 'tree183.dat', 'tree184.dat', 'tree185.dat', 'tree186.dat', 'tree187.dat', 'tree188.dat']\n"
     ]
    }
   ],
   "source": [
    "def my_collate(batch):\n",
    "    return batch\n",
    "\n",
    "#t_list = ['ArteryObjAN1-7.dat','ArteryObjAN1-0.dat', 'ArteryObjAN1-17.dat',  'ArteryObjAN1-11.dat']\n",
    "\n",
    "#t_list = ['ArteryObjAN1-0.dat','ArteryObjAN1-7.dat', 'ArteryObjAN1-17.dat',  'ArteryObjAN1-11.dat', 'ArteryObjAN1-19.dat', 'ArteryObjAN2-4.dat', 'ArteryObjAN2-6.dat', \n",
    "#           'ArteryObjAN25-18.dat']\n",
    "#t_list = ['ArteryObjAN1-17-55.dat', 'ArteryObjAN1-17-22.dat', \"ArteryObjAN1-17-12.dat\", \"ArteryObjAN1-17-9.dat\",'ArteryObjAN1-17-42.dat', 'ArteryObjAN1-17-64.dat', \"ArteryObjAN1-17-70.dat\", \"ArteryObjAN1-17-1.dat\"]\n",
    "#t_list = ['ArteryObjAN1-17.dat']\n",
    "#t_list = ['ArteryObjAN1-11.dat']\n",
    "\n",
    "\n",
    "#t_list = ['test2.dat']\n",
    "\n",
    "#t_list = ['ArteryObjAN31-14.dat']\n",
    "#t_list = os.listdir(\"./trees\")[:20]\n",
    "t_list = os.listdir(\"./trees/prof4\")[:100]\n",
    "print(t_list)\n",
    "class tDataset(Dataset):\n",
    "    def __init__(self, dir, transform=None):\n",
    "        self.names = dir\n",
    "        self.transform = transform\n",
    "        self.data = [] #lista con las strings de todos los arboles\n",
    "        for file in self.names:\n",
    "            self.data.append(read_tree(file))\n",
    "        self.trees = []\n",
    "        for tree in self.data:\n",
    "            deserial = deserialize(tree)\n",
    "            normalize_features(deserial)\n",
    "            self.trees.append(deserial)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #file = self.names[idx]\n",
    "        #string = read_tree(file)\n",
    "        tree = self.trees[idx]\n",
    "        return tree\n",
    "\n",
    "batch_size = 10\n",
    "dataset = tDataset(t_list)\n",
    "data_loader = DataLoader(dataset, batch_size = batch_size, shuffle=True, collate_fn=my_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35889df412144e459f2f933d895f7547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.5, 0.5,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input = iter(data_loader).next()[0]\n",
    "plotTree(input, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED con batch [tensor([[-0.0369,  0.0722,  0.0463,  ..., -0.1315,  0.0880, -0.1210],\n",
      "        [-0.0383,  0.0896,  0.0549,  ..., -0.1297,  0.0896, -0.1314],\n",
      "        [-0.0163, -0.0257, -0.0430,  ..., -0.1147, -0.0510, -0.0982],\n",
      "        ...,\n",
      "        [-0.0519, -0.0238, -0.0140,  ..., -0.0512, -0.0440, -0.0269],\n",
      "        [-0.0419,  0.0792,  0.0505,  ..., -0.1365,  0.0886, -0.1208],\n",
      "        [-0.0537, -0.0190, -0.0034,  ..., -0.0559, -0.0423, -0.0243]],\n",
      "       device='cuda:0', grad_fn=<StackBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "import torch_f\n",
    "enc_fold = torch_f.Fold(device)\n",
    "enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "batch = iter(data_loader).next()\n",
    "#for example in batch:\n",
    "        #enc_fold.add('leafEncoder', example.radius)\n",
    "        #enc_fold_nodes.append(enc_fold.add('leafEncoder', example.radius))\n",
    "        #enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "\n",
    "        #print(\"enc fold nodes\", enc_fold)\n",
    "for example in data_loader:\n",
    "        example = example[0]\n",
    "        enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "print(\"ENCODED con batch\",enc_fold_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encodeado sin batch tensor([[-5.3673e-02, -1.8987e-02, -3.3840e-03, -8.0235e-02,  2.4374e-03,\n",
      "         -1.0227e-01, -4.4057e-02,  4.4910e-02, -4.9848e-02,  8.7446e-02,\n",
      "         -6.4462e-02,  5.8115e-02,  5.1485e-02, -2.6213e-02, -7.9711e-02,\n",
      "         -8.8594e-02, -9.5641e-03, -2.8530e-02,  3.6163e-02, -1.3914e-02,\n",
      "         -5.4705e-02, -3.2910e-02,  1.8415e-02, -7.6173e-02, -5.3303e-02,\n",
      "          8.8804e-02, -5.3884e-02,  8.6402e-02, -1.3392e-01,  7.3943e-02,\n",
      "          8.3480e-02,  1.1670e-02,  1.0032e-01, -5.6596e-02,  1.6607e-02,\n",
      "          6.9872e-03, -1.6960e-02,  9.8566e-03,  6.0125e-03, -1.2172e-01,\n",
      "          5.4231e-04,  3.9735e-02, -5.9735e-02,  2.2477e-02, -1.2244e-01,\n",
      "         -1.3315e-03, -1.1339e-01,  1.1930e-01, -1.0598e-02, -4.3050e-02,\n",
      "          1.4440e-01,  2.4836e-02,  8.5626e-02, -6.8974e-02, -9.1799e-03,\n",
      "         -5.6688e-02,  1.5639e-02,  1.0087e-01, -2.1649e-02,  1.6499e-01,\n",
      "          6.0021e-02, -1.6853e-02, -4.5539e-02,  6.2408e-03,  8.0819e-02,\n",
      "         -2.2632e-02, -2.8974e-02, -7.0903e-02,  3.7235e-02, -7.1470e-02,\n",
      "          7.4340e-02,  3.4151e-02,  7.6447e-02, -4.0302e-02, -3.2390e-02,\n",
      "          1.5768e-02, -3.9351e-02, -1.5805e-03, -1.2658e-02,  2.3514e-02,\n",
      "          9.1403e-04,  5.4967e-02,  5.4912e-02, -5.8799e-02,  1.1625e-01,\n",
      "         -7.2544e-02, -4.3040e-02,  1.1594e-01, -2.8871e-03, -5.2108e-02,\n",
      "          1.7714e-02,  8.2547e-02,  1.7758e-02, -4.6067e-02,  7.9785e-02,\n",
      "          1.2092e-02,  1.4166e-02,  3.9381e-02,  8.3735e-02,  6.1303e-02,\n",
      "          3.9604e-02,  6.7646e-02,  5.9853e-02, -6.5086e-02, -6.9257e-02,\n",
      "          8.7273e-02, -1.2370e-01,  9.5305e-02, -6.6430e-02, -2.6694e-02,\n",
      "          8.1296e-02,  4.3908e-02,  2.0923e-02, -6.0307e-03,  1.7414e-02,\n",
      "          1.0935e-01,  1.9095e-02, -3.6472e-02,  5.2629e-02, -4.5214e-02,\n",
      "         -3.2720e-02, -2.6401e-02, -1.5717e-01,  6.4277e-02, -8.1418e-02,\n",
      "         -6.4283e-02, -1.8439e-01, -1.1525e-01,  4.8261e-02,  2.7203e-02,\n",
      "          2.8718e-02, -5.3994e-02, -1.3322e-02, -5.9301e-04, -1.2135e-01,\n",
      "          1.2667e-02, -9.6068e-02, -3.5953e-02,  5.6360e-02,  6.8619e-03,\n",
      "         -2.6071e-02, -1.2060e-01,  9.4353e-02, -6.8632e-02,  4.5236e-02,\n",
      "         -7.2720e-03,  4.3378e-02,  8.1649e-03,  3.6907e-02,  7.0796e-02,\n",
      "          7.7295e-03, -1.1269e-02, -7.0513e-02,  5.0049e-05,  6.3391e-03,\n",
      "         -5.7111e-02,  1.1891e-01, -7.1089e-03,  1.3607e-01, -7.5649e-02,\n",
      "          1.9608e-02, -8.9558e-03,  4.3839e-02,  8.6843e-03, -1.7812e-02,\n",
      "          2.5587e-02,  8.1519e-02,  1.4124e-02,  5.5742e-02,  6.1052e-03,\n",
      "          4.3260e-02, -3.1894e-02,  1.3781e-01,  6.6442e-02, -9.1931e-02,\n",
      "          4.9567e-02,  1.3092e-01, -8.5867e-02, -3.4710e-02,  1.0475e-01,\n",
      "         -2.1273e-02, -4.4796e-02, -1.2326e-03, -8.7728e-02,  3.9724e-02,\n",
      "         -4.0371e-02,  5.6421e-02,  6.0006e-02, -1.7024e-01, -8.3994e-03,\n",
      "          5.2090e-02,  5.7160e-02, -5.6382e-02, -1.0881e-01,  9.3841e-02,\n",
      "          5.2610e-02, -2.3814e-02,  4.8659e-02,  1.2281e-01, -3.0731e-02,\n",
      "          9.4820e-02,  6.2404e-02,  1.0946e-01,  2.7319e-02, -3.0281e-02,\n",
      "          3.7959e-02, -5.6257e-02,  2.3211e-02, -6.6053e-03,  7.1572e-02,\n",
      "          8.9591e-02, -2.5584e-02, -1.4431e-01,  3.7598e-02, -3.3605e-02,\n",
      "          4.0745e-02,  4.0196e-02,  6.2468e-02, -8.6255e-02,  4.3240e-02,\n",
      "         -2.8605e-02, -3.5571e-02, -2.5350e-01, -6.8278e-02,  3.9035e-02,\n",
      "          6.3181e-02,  1.4264e-01,  2.8895e-02, -5.6805e-02, -4.8733e-03,\n",
      "          5.8046e-02,  1.2790e-02, -1.3978e-04,  1.2426e-02, -5.6710e-02,\n",
      "         -3.0502e-02,  1.3952e-01, -1.0571e-01,  1.4515e-02, -1.4030e-02,\n",
      "          8.0970e-02,  1.7771e-02,  1.2573e-01,  1.1793e-02,  3.4117e-03,\n",
      "         -3.5937e-02, -1.1733e-01,  8.0023e-02,  6.2154e-03,  1.1677e-02,\n",
      "          1.1982e-01, -1.1561e-02,  3.7815e-02, -5.5874e-02, -4.2293e-02,\n",
      "         -2.4255e-02]], device='cuda:0', grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for data in data_loader:\n",
    "    data = data[0]\n",
    "\n",
    "enc_f = encode_structure(example).to(device)\n",
    "\n",
    "print(\"encodeado sin batch\", enc_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0168,  0.0912,  0.0497,  ..., -0.0756,  0.1303, -0.0968],\n",
       "        [ 0.0154,  0.1086,  0.0583,  ..., -0.0739,  0.1319, -0.1072],\n",
       "        [ 0.0374, -0.0067, -0.0396,  ..., -0.0588, -0.0088, -0.0740],\n",
       "        ...,\n",
       "        [ 0.0017, -0.0048, -0.0106,  ...,  0.0047, -0.0017, -0.0027],\n",
       "        [ 0.0117,  0.0982,  0.0539,  ..., -0.0806,  0.1309, -0.0966],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_fold_nodes[0]-enc_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[8, 8, 7, 8, 5, 10, 9, 6, 7, 8, 9, 6, 8, 7, 6, 8, 9, 8, 7, 9, 9, 7, 9, 8, 8, 8, 9, 7, 9, 10, 8, 8, 6, 7, 6, 8, 4, 4, 6, 10, 7, 8, 7, 9, 9, 8, 9, 6, 7, 8, 8, 9, 9, 7, 9, 9, 9, 7, 7, 8, 9, 10, 9, 9, 6, 8, 6, 9, 8, 8, 9, 5, 6, 8, 9, 10, 7, 6, 8, 6, 7, 9, 6, 4, 9, 8, 8, 6, 7, 8, 6, 6, 8, 7, 4, 8, 8, 8, 7, 7]\n",
      "7.61\n",
      "4.37\n",
      "0.9\n",
      "2.34\n"
     ]
    }
   ],
   "source": [
    "n_no = []\n",
    "qzero = 0\n",
    "qOne = 0\n",
    "qtwo = 0\n",
    "\n",
    "for batch in data_loader:\n",
    "    for tree in batch:\n",
    "        count = []\n",
    "        n = tree.count_nodes(tree, count)\n",
    "        n_no.append(len(n))\n",
    "        li = []\n",
    "        tree.traverseInorderChilds(tree, li)\n",
    "        zero = [a for a in li if a == 0]\n",
    "        one = [a for a in li if a == 1]\n",
    "        two = [a for a in li if a == 2]\n",
    "        qzero += len(zero)\n",
    "        qOne += len(one)\n",
    "        qtwo += len(two)\n",
    "\n",
    "print(len(data_loader)*batch_size)\n",
    "print(n_no)\n",
    "nprom = np.mean(n_no)\n",
    "print(nprom)\n",
    "qzero /= len(data_loader)*batch_size\n",
    "qOne /= len(data_loader)*batch_size\n",
    "qtwo /= len(data_loader)*batch_size\n",
    "\n",
    "print(qzero)\n",
    "print(qOne)\n",
    "print(qtwo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en el loop creo un fold, mando este fold con cada uno de los arboles del batch a encode_structure_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.mlp1 = nn.Linear(latent_size, hidden_size)\n",
    "        self.mlp2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.mlp3 = nn.Linear(hidden_size, 3)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_feature):\n",
    "        #print(\"classifier input\", input_feature)\n",
    "        output = self.mlp1(input_feature)\n",
    "        output = self.tanh(output)\n",
    "        output = self.mlp2(output)\n",
    "        output = self.tanh(output)\n",
    "        output = self.mlp3(output)\n",
    "        #print(\"classifier output\", output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class InternalDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size: int):\\n        super(InternalDecoder, self).__init__()\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.lp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp_right = nn.Linear(latent_size, latent_size)\\n        self.tanh = nn.Tanh()\\n        self.mlp2 = nn.Linear(latent_size,4)\\n\\n    def forward(self, parent_feature):\\n        #print(\"internal decoder\")\\n        #print(\"input\", parent_feature.shape)\\n        vector = self.mlp(parent_feature)\\n        vector = self.tanh(vector)\\n        vector = self.lp2(vector)\\n        vector = self.tanh(vector)\\n        right_feature = self.mlp_right(vector)\\n        right_feature = self.tanh(right_feature)\\n        rad_feature = self.mlp2(vector)\\n\\n        return right_feature, rad_feature\\n\\nclass BifurcationDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size : int):\\n        super(BifurcationDecoder, self).__init__()\\n        #self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.lp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp_left = nn.Linear(latent_size, latent_size)\\n        self.mlp_right = nn.Linear(latent_size, latent_size)\\n        self.mlp2 = nn.Linear(latent_size,4)\\n        self.tanh = nn.Tanh()\\n\\n    def forward(self, parent_feature):\\n        #print(\"bifurcation decoder input\", parent_feature.shape)\\n        parent_feature = parent_feature.reshape(-1,128)\\n        #print(\"bifurcation decoder input\", parent_feature.shape)\\n        vector = self.mlp(parent_feature)\\n        #print(\"v1\", vector.shape)\\n        vector = self.tanh(vector)\\n        #print(\"v2\", vector.shape)\\n        vector = self.lp2(vector)\\n        #print(\"v3\", vector.shape)\\n        vector = self.tanh(vector)\\n        left_feature = self.mlp_left(vector)\\n        left_feature = self.tanh(left_feature)\\n        right_feature = self.mlp_right(vector)\\n        right_feature = self.tanh(right_feature)\\n        rad_feature = self.mlp2(vector)\\n        #print(\"exiting bif dec\")\\n        return left_feature, right_feature, rad_feature\\n\\n\\nclass featureDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size: int):\\n        super(featureDecoder, self).__init__()\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.mlp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp3 = nn.Linear(latent_size, latent_size)\\n        self.tanh = nn.Tanh()\\n        self.mlp4 = nn.Linear(latent_size,4)\\n\\n    def forward(self, parent_feature):\\n        #print(\"feature decoder input\", parent_feature.shape)\\n\\n        vector = self.mlp(parent_feature)\\n        vector = self.tanh(vector)\\n        vector = self.mlp2(vector)\\n        vector = self.tanh(vector)\\n        vector = self.mlp3(vector)\\n        vector = self.tanh(vector)\\n        vector = self.mlp4(vector)\\n       \\n        return vector\\n\\n\\n\\nclass GRASSDecoder(nn.Module):\\n    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\\n        super(GRASSDecoder, self).__init__()\\n        self.feature_decoder = featureDecoder(latent_size, hidden_size)\\n        self.internal_decoder = InternalDecoder(latent_size, hidden_size)\\n        self.bifurcation_decoder = BifurcationDecoder(latent_size, hidden_size)\\n        self.node_classifier = NodeClassifier(latent_size, hidden_size)\\n        self.mseLoss = nn.MSELoss()  # pytorch\\'s mean squared error loss\\n        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch\\'s cross entropy loss (NOTE: no softmax is needed before)\\n\\n    def featureDecoder(self, feature):\\n        return self.feature_decoder(feature)\\n\\n    def internalDecoder(self, feature):\\n        return self.internal_decoder(feature)\\n\\n    def bifurcationDecoder(self, feature):\\n        return self.bifurcation_decoder(feature)\\n\\n    def nodeClassifier(self, feature):\\n        return self.node_classifier(feature)\\n\\n    def calcularLossAtributo(self, nodo, radio):\\n        if nodo is None:\\n            return\\n        else:\\n            #print(\"radio\", radio)\\n            #print(\"nodo\", nodo)\\n            nodo = torch.stack(nodo)\\n            #print(\"nodo stack\", nodo)\\n            #radio = radio.reshape(-1,4)\\n        \\n            #return mse\\n            #return torch.cat([self.mseLoss(b, gt) for b, gt in zip(radio, nodo)], 0)\\n            z = zip(radio.reshape(-1,4), nodo.reshape(-1,4))\\n            \\n            #for b, gt in z:\\n            #    print(\"bgt\", b, gt)\\n                \\n            \\n            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\\n            #print(\"loss\", l)\\n            return l\\n\\n\\n    def classifyLossEstimator(self, label_vector, original):\\n        if original is None:\\n            return\\n        else:\\n           \\n            v = []\\n            for o in original:\\n                if o == 0:\\n                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\\n                if o == 1:\\n                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\\n                if o == 2:\\n                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\\n                v.append(vector)\\n\\n            v = torch.stack(v)\\n            z = zip(label_vector.reshape(-1,3), v.reshape(-1,3))   \\n            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\\n            \\n            return l\\n            #return c\\n       # return torch.cat([self.creLoss(l.unsqueeze(0), gt).mul(0.2) for l, gt in zip(label_vector, gt_label_vector)], 0)\\n\\n    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\\n        \\n        v = v1.add(v2)\\n        #print(\"v0\", v)\\n        if v3 is not None:\\n            v = v.add(v3)\\n        #print(\"v0\", v)\\n        if v4 is not None:\\n            v = v.add(v4)\\n       \\n        return v\\nif qzero == 0:\\n    qzero = 1\\nif qOne == 0:\\n    qOne = 1\\nif qtwo == 0:\\n    qtwo = 1\\nmult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\\nGrassdecoder = GRASSDecoder(latent_size=128, hidden_size=256, mult = mult)\\nGrassdecoder = Grassdecoder.to(device)\\n'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class InternalDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size: int):\n",
    "        super(InternalDecoder, self).__init__()\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, latent_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.mlp2 = nn.Linear(latent_size,4)\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"internal decoder\")\n",
    "        #print(\"input\", parent_feature.shape)\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.lp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        rad_feature = self.mlp2(vector)\n",
    "\n",
    "        return right_feature, rad_feature\n",
    "\n",
    "class BifurcationDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(BifurcationDecoder, self).__init__()\n",
    "        #self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp_left = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp2 = nn.Linear(latent_size,4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"bifurcation decoder input\", parent_feature.shape)\n",
    "        parent_feature = parent_feature.reshape(-1,128)\n",
    "        #print(\"bifurcation decoder input\", parent_feature.shape)\n",
    "        vector = self.mlp(parent_feature)\n",
    "        #print(\"v1\", vector.shape)\n",
    "        vector = self.tanh(vector)\n",
    "        #print(\"v2\", vector.shape)\n",
    "        vector = self.lp2(vector)\n",
    "        #print(\"v3\", vector.shape)\n",
    "        vector = self.tanh(vector)\n",
    "        left_feature = self.mlp_left(vector)\n",
    "        left_feature = self.tanh(left_feature)\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        rad_feature = self.mlp2(vector)\n",
    "        #print(\"exiting bif dec\")\n",
    "        return left_feature, right_feature, rad_feature\n",
    "\n",
    "\n",
    "class featureDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size: int):\n",
    "        super(featureDecoder, self).__init__()\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp3 = nn.Linear(latent_size, latent_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.mlp4 = nn.Linear(latent_size,4)\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"feature decoder input\", parent_feature.shape)\n",
    "\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp3(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp4(vector)\n",
    "       \n",
    "        return vector\n",
    "\n",
    "\n",
    "\n",
    "class GRASSDecoder(nn.Module):\n",
    "    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\n",
    "        super(GRASSDecoder, self).__init__()\n",
    "        self.feature_decoder = featureDecoder(latent_size, hidden_size)\n",
    "        self.internal_decoder = InternalDecoder(latent_size, hidden_size)\n",
    "        self.bifurcation_decoder = BifurcationDecoder(latent_size, hidden_size)\n",
    "        self.node_classifier = NodeClassifier(latent_size, hidden_size)\n",
    "        self.mseLoss = nn.MSELoss()  # pytorch's mean squared error loss\n",
    "        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch's cross entropy loss (NOTE: no softmax is needed before)\n",
    "\n",
    "    def featureDecoder(self, feature):\n",
    "        return self.feature_decoder(feature)\n",
    "\n",
    "    def internalDecoder(self, feature):\n",
    "        return self.internal_decoder(feature)\n",
    "\n",
    "    def bifurcationDecoder(self, feature):\n",
    "        return self.bifurcation_decoder(feature)\n",
    "\n",
    "    def nodeClassifier(self, feature):\n",
    "        return self.node_classifier(feature)\n",
    "\n",
    "    def calcularLossAtributo(self, nodo, radio):\n",
    "        if nodo is None:\n",
    "            return\n",
    "        else:\n",
    "            #print(\"radio\", radio)\n",
    "            #print(\"nodo\", nodo)\n",
    "            nodo = torch.stack(nodo)\n",
    "            #print(\"nodo stack\", nodo)\n",
    "            #radio = radio.reshape(-1,4)\n",
    "        \n",
    "            #return mse\n",
    "            #return torch.cat([self.mseLoss(b, gt) for b, gt in zip(radio, nodo)], 0)\n",
    "            z = zip(radio.reshape(-1,4), nodo.reshape(-1,4))\n",
    "            \n",
    "            #for b, gt in z:\n",
    "            #    print(\"bgt\", b, gt)\n",
    "                \n",
    "            \n",
    "            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\n",
    "            #print(\"loss\", l)\n",
    "            return l\n",
    "\n",
    "\n",
    "    def classifyLossEstimator(self, label_vector, original):\n",
    "        if original is None:\n",
    "            return\n",
    "        else:\n",
    "           \n",
    "            v = []\n",
    "            for o in original:\n",
    "                if o == 0:\n",
    "                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\n",
    "                if o == 1:\n",
    "                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\n",
    "                if o == 2:\n",
    "                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\n",
    "                v.append(vector)\n",
    "\n",
    "            v = torch.stack(v)\n",
    "            z = zip(label_vector.reshape(-1,3), v.reshape(-1,3))   \n",
    "            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\n",
    "            \n",
    "            return l\n",
    "            #return c\n",
    "       # return torch.cat([self.creLoss(l.unsqueeze(0), gt).mul(0.2) for l, gt in zip(label_vector, gt_label_vector)], 0)\n",
    "\n",
    "    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\n",
    "        \n",
    "        v = v1.add(v2)\n",
    "        #print(\"v0\", v)\n",
    "        if v3 is not None:\n",
    "            v = v.add(v3)\n",
    "        #print(\"v0\", v)\n",
    "        if v4 is not None:\n",
    "            v = v.add(v4)\n",
    "       \n",
    "        return v\n",
    "if qzero == 0:\n",
    "    qzero = 1\n",
    "if qOne == 0:\n",
    "    qOne = 1\n",
    "if qtwo == 0:\n",
    "    qtwo = 1\n",
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "Grassdecoder = GRASSDecoder(latent_size=128, hidden_size=256, mult = mult)\n",
    "Grassdecoder = Grassdecoder.to(device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(Decoder, self).__init__()\n",
    "        #self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp_left = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp3 = nn.Linear(latent_size,latent_size)\n",
    "        self.mlp2 = nn.Linear(latent_size,4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def common_branch(self, parent_feature):\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.lp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        return vector\n",
    "\n",
    "    def attr_branch(self, vector):\n",
    "        vector = self.mlp2(vector)        \n",
    "        return vector\n",
    "\n",
    "    def right_branch(self, vector):\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        return right_feature\n",
    "\n",
    "    def left_branch(self, vector):\n",
    "        left_feature = self.mlp_left(vector)\n",
    "        left_feature = self.tanh(left_feature)\n",
    "        return left_feature\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "      \n",
    "        vector      = self.common_branch(parent_feature)\n",
    "        attr_vector = self.attr_branch(vector)\n",
    "        return attr_vector \n",
    "\n",
    "    def forward1(self, parent_feature):\n",
    "    \n",
    "\n",
    "        vector       = self.common_branch(parent_feature)\n",
    "        attr_vector  = self.attr_branch(vector)\n",
    "        right_vector = self.right_branch(vector)\n",
    "        \n",
    "        #print(\"right vector\", right_vector)\n",
    "        #print(\"radius\", attr_vector)\n",
    "        return right_vector, attr_vector\n",
    "\n",
    "    def forward2(self, parent_feature):\n",
    "       \n",
    "\n",
    "        vector       = self.common_branch(parent_feature)\n",
    "        attr_vector  = self.attr_branch(vector)\n",
    "        right_vector = self.right_branch(vector)\n",
    "        left_vector  = self.left_branch(vector)\n",
    "        #print(\"left vector\", left_vector)\n",
    "        #print(\"right vector\", right_vector)\n",
    "        #print(\"radius\", attr_vector)\n",
    "        return left_vector, right_vector, attr_vector\n",
    "\n",
    "\n",
    "\n",
    "class GRASSDecoder(nn.Module):\n",
    "    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\n",
    "        super(GRASSDecoder, self).__init__()\n",
    "        self.decoder = Decoder(latent_size, hidden_size)\n",
    "        self.node_classifier = NodeClassifier(latent_size, hidden_size)\n",
    "        self.mseLoss = nn.MSELoss()  # pytorch's mean squared error loss\n",
    "        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch's cross entropy loss (NOTE: no softmax is needed before)\n",
    "        \n",
    "\n",
    "\n",
    "    def featureDecoder(self, feature):\n",
    "        return self.decoder.forward(feature)\n",
    "\n",
    "    def internalDecoder(self, feature):\n",
    "        return self.decoder.forward1(feature)\n",
    "\n",
    "    def bifurcationDecoder(self, feature):\n",
    "        return self.decoder.forward2(feature)\n",
    "\n",
    "    def nodeClassifier(self, feature):\n",
    "        return self.node_classifier(feature)\n",
    "\n",
    "    def calcularLossAtributo(self, nodo, radio):\n",
    "        #print(\"nodo\", nodo)\n",
    "        #print(\"radio\", radio)\n",
    "        a, b = list(zip(*nodo))# a son los atributos, b los pesos\n",
    "        if nodo is None:\n",
    "            return\n",
    "        else:\n",
    "            nodo = torch.stack(list(a))\n",
    "        \n",
    "            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\n",
    "            #print(\"mse\", l)\n",
    "            return l\n",
    "\n",
    "\n",
    "    def classifyLossEstimator(self, label_vector, original):\n",
    "        if original is None:\n",
    "            return\n",
    "        else:\n",
    "           \n",
    "            v = []\n",
    "            for o in original:\n",
    "                if o == 0:\n",
    "                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\n",
    "                if o == 1:\n",
    "                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\n",
    "                if o == 2:\n",
    "                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\n",
    "                v.append(vector)\n",
    "            \n",
    "\n",
    "            v = torch.stack(v)\n",
    "            \n",
    "            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\n",
    "         \n",
    "\n",
    "            return l\n",
    "            #return c\n",
    "\n",
    "    '''\n",
    "    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\n",
    "        \n",
    "        print(\"loss estructura\", v1)\n",
    "        print(\"loss atributo\", v2)\n",
    "        print(\"right loss\", v3)\n",
    "        print(\"left loss\", v4)\n",
    "\n",
    "\n",
    "        v = v1.add(v2)\n",
    "        #print(\"v0\", v)\n",
    "        if v3 is not None:\n",
    "            v = v.add(v3)\n",
    "        #print(\"v0\", v)\n",
    "        if v4 is not None:\n",
    "            v = v.add(v4)\n",
    "       \n",
    "        return v\n",
    "\n",
    "    '''\n",
    "    def vectorAdder(self, v1, v2):\n",
    "        v = v1.add(v2)\n",
    "        return v\n",
    "\n",
    "    def vectorMult(self, m, v):\n",
    "        #print(\"v\", v)\n",
    "        #print(\"m\", m)\n",
    "        z = zip(v, m)\n",
    "        r = []\n",
    "        for c, d in z:\n",
    "            #print(\"v\", c)\n",
    "            #print(\"m\", d)\n",
    "            r.append(torch.mul(c, d))\n",
    "        #res = [torch.mul(v, m) for v, m in zip(v, m)]\n",
    "        #print(\"res\", r)\n",
    "        return r\n",
    "\n",
    "if round(qzero) == 0:\n",
    "    qzero = 1\n",
    "if round(qOne) == 0:\n",
    "    qOne = 1\n",
    "if round(qtwo) == 0:\n",
    "    qtwo = 1\n",
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "\n",
    "Grassdecoder = GRASSDecoder(latent_size=256, hidden_size=512, mult = mult)\n",
    "Grassdecoder = Grassdecoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2500, 1.0000, 0.5000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "print(mult)\n",
    "def calcularLossEstructura(cl_p, original):\n",
    "    \n",
    "    mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "    ce = nn.CrossEntropyLoss(weight = mult)\n",
    "\n",
    "    if original.childs() == 0:\n",
    "        vector = [1, 0, 0] \n",
    "    if original.childs() == 1:\n",
    "        vector = [0, 1, 0]\n",
    "    if original.childs() == 2:\n",
    "        vector = [0, 0, 1] \n",
    "\n",
    "    #print(\"original\", vector)\n",
    "    #print(\"prediction\", cl_p)\n",
    "    c = ce(cl_p, torch.tensor(vector, device=device, dtype = torch.float).reshape(1, 3))\n",
    "    #print(\"ce\", 0.4*c)\n",
    "    return c\n",
    "\n",
    "\n",
    "def calcularLossAtributo(nodo, radio):\n",
    "    #print(\"nodo\", nodo)\n",
    "    #print(\"radio\", radio)\n",
    "\n",
    "    radio = radio.reshape(-1,4)\n",
    "    nodo = nodo.reshape(-1,4)\n",
    "    l2    = nn.MSELoss()\n",
    "   \n",
    "    mse = l2(radio, nodo)\n",
    "    #print(\"mse\", mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchNode(node, key):\n",
    "     \n",
    "    if (node == None):\n",
    "        return False\n",
    " \n",
    "    if (node.data == key):\n",
    "        return node\n",
    "        \n",
    " \n",
    "    \"\"\" then recur on left subtree \"\"\"\n",
    "    res1 = searchNode(node.left, key)\n",
    "    # node found, no need to look further\n",
    "    if res1:\n",
    "        return res1\n",
    " \n",
    "    \"\"\" node is not found in left,\n",
    "    so recur on right subtree \"\"\"\n",
    "    res2 = searchNode(node.right, key)\n",
    "    return res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_nodes 9\n",
      "0 is not present in tree\n",
      "4 is not present in tree\n"
     ]
    }
   ],
   "source": [
    "def getLevelUtil(node, data, level):\n",
    "    if (node == None):\n",
    "        return 0\n",
    " \n",
    "    if (node.data == data):\n",
    "        return level\n",
    " \n",
    "    downlevel = getLevelUtil(node.left, data, level + 1)\n",
    "\n",
    "    if (downlevel != 0):\n",
    "        return downlevel\n",
    " \n",
    "    downlevel = getLevelUtil(node.right, data, level + 1)\n",
    "    return downlevel\n",
    " \n",
    "# Returns level of given data value\n",
    " \n",
    " \n",
    "def getLevel(node, data):\n",
    "    return getLevelUtil(node, data, 1)\n",
    " \n",
    "\n",
    "c = []\n",
    "n_nodes = input.count_nodes(input, c)\n",
    "print(\"n_nodes\", len(n_nodes))\n",
    "for x in range(0, len(n_nodes)):\n",
    "        level = getLevel(input, x)\n",
    "        if (level):\n",
    "            #print(\"Level of\", x, \"is\", getLevel(input, x))\n",
    "            node = searchNode(input, x)\n",
    "            node.level = getLevel(input, x)\n",
    "        else:\n",
    "            print(x, \"is not present in tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Node object at 0x000001FE5BBF79D0>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BC2EC20>\n",
      "tree level 19\n",
      "<__main__.Node object at 0x000001FE5BBF5900>\n",
      "tree level 23\n",
      "<__main__.Node object at 0x000001FE597A5C30>\n",
      "tree level 15\n",
      "<__main__.Node object at 0x000001FE5BBF56C0>\n",
      "tree level 21\n",
      "<__main__.Node object at 0x000001FE5BC2F070>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BBF7C70>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE597A5330>\n",
      "tree level 17\n",
      "<__main__.Node object at 0x000001FE5BC2C8B0>\n",
      "tree level 17\n",
      "<__main__.Node object at 0x000001FE59814F40>\n",
      "tree level 10\n",
      "<__main__.Node object at 0x000001FE5BBF6080>\n",
      "tree level 26\n",
      "<__main__.Node object at 0x000001FE5BC2FB50>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BBF42B0>\n",
      "tree level 26\n",
      "<__main__.Node object at 0x000001FE5BBF7730>\n",
      "tree level 15\n",
      "<__main__.Node object at 0x000001FE5BC2EFB0>\n",
      "tree level 19\n",
      "<__main__.Node object at 0x000001FE5BBF53C0>\n",
      "tree level 23\n",
      "<__main__.Node object at 0x000001FE597A7220>\n",
      "tree level 26\n",
      "<__main__.Node object at 0x000001FE5BBF41C0>\n",
      "tree level 25\n",
      "<__main__.Node object at 0x000001FE5BBF5180>\n",
      "tree level 25\n",
      "<__main__.Node object at 0x000001FE5BBF65F0>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BC2DAE0>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BBF7BE0>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BC2DC30>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BC2DBA0>\n",
      "tree level 17\n",
      "<__main__.Node object at 0x000001FE5BBF4A60>\n",
      "tree level 21\n",
      "<__main__.Node object at 0x000001FE59769540>\n",
      "tree level 10\n",
      "<__main__.Node object at 0x000001FE5BC2E260>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BBF63E0>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BBF6290>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE59768D30>\n",
      "tree level 17\n",
      "<__main__.Node object at 0x000001FE5BC2F0D0>\n",
      "tree level 17\n",
      "<__main__.Node object at 0x000001FE58520130>\n",
      "tree level 10\n",
      "<__main__.Node object at 0x000001FE597A6080>\n",
      "tree level 17\n",
      "<__main__.Node object at 0x000001FE5BBF6770>\n",
      "tree level 23\n",
      "<__main__.Node object at 0x000001FE5BC2E800>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BC2C0D0>\n",
      "tree level 19\n",
      "<__main__.Node object at 0x000001FE5BC2EA40>\n",
      "tree level 25\n",
      "<__main__.Node object at 0x000001FE5BBF7CA0>\n",
      "tree level 17\n",
      "<__main__.Node object at 0x000001FE5BC2E6B0>\n",
      "tree level 25\n",
      "<__main__.Node object at 0x000001FE516B4B80>\n",
      "tree level 10\n",
      "<__main__.Node object at 0x000001FE597A7A60>\n",
      "tree level 25\n",
      "<__main__.Node object at 0x000001FE597A5510>\n",
      "tree level 29\n",
      "<__main__.Node object at 0x000001FE5BC2FA60>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE59768AC0>\n",
      "tree level 25\n",
      "<__main__.Node object at 0x000001FE5BBF4D30>\n",
      "tree level 15\n",
      "<__main__.Node object at 0x000001FE597A43D0>\n",
      "tree level 25\n",
      "<__main__.Node object at 0x000001FE5BBF7DF0>\n",
      "tree level 19\n",
      "<__main__.Node object at 0x000001FE5BBF7460>\n",
      "tree level 26\n",
      "<__main__.Node object at 0x000001FE597A6FE0>\n",
      "tree level 23\n",
      "<__main__.Node object at 0x000001FE5BBF6CB0>\n",
      "tree level 26\n",
      "<__main__.Node object at 0x000001FE597A4F70>\n",
      "tree level 23\n",
      "<__main__.Node object at 0x000001FE5BBF4820>\n",
      "tree level 23\n",
      "<__main__.Node object at 0x000001FE5BBF69E0>\n",
      "tree level 25\n",
      "<__main__.Node object at 0x000001FE5BC2C1C0>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BBF42E0>\n",
      "tree level 19\n",
      "<__main__.Node object at 0x000001FE5BBF49D0>\n",
      "tree level 26\n",
      "<__main__.Node object at 0x000001FE597A7E20>\n",
      "tree level 26\n",
      "<__main__.Node object at 0x000001FE59769510>\n",
      "tree level 19\n",
      "<__main__.Node object at 0x000001FE597A52D0>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE597A4CD0>\n",
      "tree level 19\n",
      "<__main__.Node object at 0x000001FE5BC2C760>\n",
      "tree level 12\n",
      "<__main__.Node object at 0x000001FE5BBF4FA0>\n",
      "tree level 25\n",
      "<__main__.Node object at 0x000001FE5BC2D630>\n",
      "tree level 19\n",
      "<__main__.Node object at 0x000001FE5BC2F220>\n",
      "tree level 19\n",
      "<__main__.Node object at 0x000001FE5BC2E0B0>\n",
      "tree level 25\n",
      "<__main__.Node object at 0x000001FE5BC2F820>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BBF5810>\n",
      "tree level 21\n",
      "<__main__.Node object at 0x000001FE5BC2D870>\n",
      "tree level 26\n",
      "<__main__.Node object at 0x000001FE5BBF7280>\n",
      "tree level 26\n",
      "<__main__.Node object at 0x000001FE597A5720>\n",
      "tree level 29\n",
      "<__main__.Node object at 0x000001FE5BC2D1B0>\n",
      "tree level 12\n",
      "<__main__.Node object at 0x000001FE597A7280>\n",
      "tree level 21\n",
      "<__main__.Node object at 0x000001FE5BC2D990>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE597681F0>\n",
      "tree level 17\n",
      "<__main__.Node object at 0x000001FE5BBF70A0>\n",
      "tree level 26\n",
      "<__main__.Node object at 0x000001FE597A5360>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BBF5390>\n",
      "tree level 29\n",
      "<__main__.Node object at 0x000001FE5BBF46A0>\n",
      "tree level 29\n",
      "<__main__.Node object at 0x000001FE5BC2C370>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BBF5AB0>\n",
      "tree level 18\n",
      "<__main__.Node object at 0x000001FE5BBF7A90>\n",
      "tree level 26\n",
      "<__main__.Node object at 0x000001FE5BC2CCD0>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BC2D540>\n",
      "tree level 19\n",
      "<__main__.Node object at 0x000001FE597A4670>\n",
      "tree level 26\n",
      "<__main__.Node object at 0x000001FE5BC2E4A0>\n",
      "tree level 25\n",
      "<__main__.Node object at 0x000001FE5BC2FE50>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BC2E470>\n",
      "tree level 25\n",
      "<__main__.Node object at 0x000001FE597A59F0>\n",
      "tree level 15\n",
      "<__main__.Node object at 0x000001FE5BC2E050>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BBF4BE0>\n",
      "tree level 21\n",
      "<__main__.Node object at 0x000001FE5BC2EE00>\n",
      "tree level 17\n",
      "<__main__.Node object at 0x000001FE5BC2F430>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BC2CA00>\n",
      "tree level 17\n",
      "<__main__.Node object at 0x000001FE5BC2F580>\n",
      "tree level 19\n",
      "<__main__.Node object at 0x000001FE5BBF7F70>\n",
      "tree level 19\n",
      "<__main__.Node object at 0x000001FE5BBF6D40>\n",
      "tree level 22\n",
      "<__main__.Node object at 0x000001FE5BBF7880>\n",
      "tree level 17\n",
      "<__main__.Node object at 0x000001FE597A7790>\n",
      "tree level 21\n",
      "<__main__.Node object at 0x000001FE5BC2C160>\n",
      "tree level 17\n",
      "<__main__.Node object at 0x000001FE5BBF6EC0>\n",
      "tree level 29\n"
     ]
    }
   ],
   "source": [
    "for d in data_loader:\n",
    "    for data in d:\n",
    "        print(data)\n",
    "        count = []\n",
    "        numerar_nodos(data, count)\n",
    "        c = []\n",
    "        n_nodes = data.count_nodes(data, c)\n",
    "        for x in range(0, len(n_nodes)):\n",
    "            level = getLevel(data, x)\n",
    "            if (level):\n",
    "                #print(\"Level of\", x, \"is\", getLevel(input, x))\n",
    "                node = searchNode(data, x)\n",
    "                node.level = getLevel(data, x)\n",
    "            else:\n",
    "                print(x, \"is not present in tree\")\n",
    "        tree_level = []\n",
    "        data.get_tree_level(data, tree_level)\n",
    "        print(\"tree level\", sum(tree_level))\n",
    "        data.set_tree_level(data, sum(tree_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_structure_fold_grass(fold, v, root):\n",
    "   \n",
    "    def decode_node(fold, v, node):\n",
    "        \n",
    "        \n",
    "        if node.childs() == 0 : \n",
    "\n",
    "            radio = fold.add('featureDecoder', v)\n",
    "            lossAtributo = fold.add('calcularLossAtributo', node, radio)\n",
    "\n",
    "            label = fold.add('nodeClassifier', v)\n",
    "            \n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)  \n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            \n",
    "            loss =  fold.add('vectorAdder', losse, lossAtributo)       \n",
    "            return loss\n",
    "\n",
    "            \n",
    "            \n",
    "        elif node.childs() == 1 :\n",
    "            right, radius = fold.add('internalDecoder', v).split(2)\n",
    "            label = fold.add('nodeClassifier', v)\n",
    "            nodoSiguiente = node.right\n",
    "            if nodoSiguiente is not None:\n",
    "                right_loss = decode_node(fold, right, nodoSiguiente)\n",
    "\n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)\n",
    "            lossAtributo = fold.add('calcularLossAtributo', node, radius)\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            loss = fold.add('vectorAdder', losse, lossAtributo)\n",
    "            \n",
    "        \n",
    "            loss2 = fold.add('vectorAdder', loss, right_loss)\n",
    "            return loss2\n",
    "            \n",
    "            \n",
    "\n",
    "        elif node.childs() == 2 :\n",
    "            left, right, radius = fold.add('bifurcationDecoder', v).split(3)\n",
    "            \n",
    "            label = fold.add('nodeClassifier', v)            \n",
    "            \n",
    "            nodoSiguienteRight = node.right\n",
    "            nodoSiguienteLeft = node.left\n",
    "\n",
    "\n",
    "            if nodoSiguienteRight is not None:\n",
    "                right_loss = decode_node(fold, right, nodoSiguienteRight)\n",
    "             \n",
    "            if nodoSiguienteLeft is not None:\n",
    "                left_loss  = decode_node(fold, left, nodoSiguienteLeft)\n",
    "\n",
    "            multipl = node.level/node.treelevel\n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            lossAtributo   = fold.add('calcularLossAtributo', node, radius)\n",
    "            loss = fold.add('vectorAdder', losse, lossAtributo)\n",
    "            loss2 = fold.add('vectorAdder', loss, right_loss)\n",
    "            loss3 = fold.add('vectorAdder', loss2, left_loss)\n",
    "            \n",
    "            return loss3\n",
    "            \n",
    "\n",
    "    dec = decode_node (fold, v, root)\n",
    "    return dec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_testing_grass(v, root, max, decoder):\n",
    "    def decode_node(v, node, max, decoder):\n",
    "        cl = decoder.nodeClassifier(v)\n",
    "        _, label = torch.max(cl, 1)\n",
    "        label = label.data\n",
    "        \n",
    "        \n",
    "        if label == 0 and createNode.count <= max: ##output del classifier\n",
    "           \n",
    "            radio = decoder.featureDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radio )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "         \n",
    "            return createNode(1,radio, ce = losse,  mse = lossAtrs)\n",
    "\n",
    "        elif label == 1 and createNode.count <= max:\n",
    "       \n",
    "            right, radius = decoder.internalDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radius )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "            d = createNode(1, radius, ce = losse,  mse = lossAtrs) \n",
    "           \n",
    "            if not node is None:\n",
    "                if not node.right is None:\n",
    "                    nodoSiguiente = node.right\n",
    "                else:\n",
    "                    nodoSiguiente = None\n",
    "            else:\n",
    "                nodoSiguiente = None\n",
    "            \n",
    "            d.right = decode_node(right, nodoSiguiente, max, decoder)\n",
    "            \n",
    "\n",
    "            return d\n",
    "       \n",
    "        elif label == 2 and createNode.count <= max:\n",
    "            left, right, radius = decoder.bifurcationDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radius )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "            \n",
    "            d = createNode(1, radius, ce = losse,  mse = lossAtrs )\n",
    "  \n",
    "            if not node is None: #el nodo existe, me fijo si tiene hijo der/izq\n",
    "                if not node.right is None:\n",
    "                    nodoSiguienteRight = node.right\n",
    "                else:\n",
    "                    nodoSiguienteRight = None\n",
    "                if not node.left is None:\n",
    "                    nodoSiguienteLeft = node.left\n",
    "                else:\n",
    "                    nodoSiguienteLeft = None\n",
    "            else: #el nodo no existe\n",
    "                nodoSiguienteRight = None\n",
    "                nodoSiguienteLeft = None\n",
    "            \n",
    "            d.right = decode_node(right, nodoSiguienteRight, max, decoder)\n",
    "            d.left = decode_node(left, nodoSiguienteLeft, max, decoder)\n",
    "            \n",
    "           \n",
    "            return d\n",
    "            \n",
    "    createNode.count = 0\n",
    "    dec = decode_node (v, root, max, decoder)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, encoder, decoder, optimizer\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            #print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            #print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            #'classifier_state_dict': classifier.state_dict(),\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                \n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, 'outputs/best_model.pth')\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_structure_fold_(v, root):\n",
    "    \n",
    "    def decode_node(v, node):\n",
    "        cl = Grassdecoder.nodeClassifier(v)\n",
    "        _, label = torch.max(cl, 1)\n",
    "        label = label.data\n",
    "\n",
    "        \n",
    "        if node.childs() == 0 : ##output del classifier\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            radio = Grassdecoder.featureDecoder(v)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radio )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "            nd = createNode(1,radio, ce = losse,  mse = lossAtrs)\n",
    "            return nd\n",
    "\n",
    "        elif node.childs() == 1 :\n",
    "        \n",
    "            right, radius = Grassdecoder.internalDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radius )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "            nd = createNode(1, radius, cl_prob = lossAtrs , ce = losse, mse = lossAtrs) \n",
    "            \n",
    "            nodoSiguiente = node.right\n",
    "           \n",
    "            if nodoSiguiente is not None:\n",
    "                nd.right = decode_node(right, nodoSiguiente)\n",
    "               \n",
    "            return nd\n",
    "\n",
    "        elif node.childs() == 2 :\n",
    "            left, right, radius = Grassdecoder.bifurcationDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radius )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "            nd = createNode(1, radius, cl_prob = lossAtrs, ce = losse, mse = lossAtrs)\n",
    "            \n",
    "            nodoSiguienteRight = node.right\n",
    "            nodoSiguienteLeft = node.left\n",
    "\n",
    "            \n",
    "            if nodoSiguienteRight is not None:\n",
    "                nd.right = decode_node(right, nodoSiguienteRight)\n",
    "             \n",
    "            if nodoSiguienteLeft is not None:\n",
    "                nd.left  = decode_node(left, nodoSiguienteLeft)\n",
    "            \n",
    "            return nd\n",
    "            \n",
    "    createNode.count = 0\n",
    "    dec = decode_node (v, root)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder con batch - decoder con batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:24hnu2in) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>loss</td><td>â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>8000</td></tr><tr><td>loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">ruby-dream-232</strong>: <a href=\"https://wandb.ai/paufeldman/autoencoder3/runs/24hnu2in\" target=\"_blank\">https://wandb.ai/paufeldman/autoencoder3/runs/24hnu2in</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221128_151348-24hnu2in\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:24hnu2in). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7ad69495a4493aa1c47427fd456e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\wandb\\run-20221128_155718-2ciousye</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/paufeldman/autoencoder3/runs/2ciousye\" target=\"_blank\">lively-leaf-233</a></strong> to <a href=\"https://wandb.ai/paufeldman/autoencoder3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/paufeldman/autoencoder3/runs/2ciousye?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1fe5bccf520>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 8000\n",
    "learning_rate = 1e-5\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "#opt = torch.optim.Adam(params, lr=learning_rate, weight_decay=0.0001) \n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "#opt = torch.optim.SGD(params, lr=learning_rate, momentum = 0.96) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[100], gamma=0.2)\n",
    "import wandb\n",
    "config = {\n",
    "  \"learning_rate\": learning_rate,\n",
    "  \"epochs\": epochs,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"dataset\": t_list,\n",
    "  \"number of trees\": len(data_loader)*batch_size,\n",
    "  \"optim\": opt\n",
    "}\n",
    "wandb.init(project=\"autoencoder3\", entity=\"paufeldman\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 8000] average reconstruction error: 0.44196796 \n",
      "Epoch [11 / 8000] average reconstruction error: 0.24885800 \n",
      "Epoch [21 / 8000] average reconstruction error: 0.04234196 \n",
      "Epoch [31 / 8000] average reconstruction error: 0.02161425 \n",
      "Epoch [41 / 8000] average reconstruction error: 0.01988301 \n",
      "Epoch [51 / 8000] average reconstruction error: 0.01842398 \n",
      "Epoch [61 / 8000] average reconstruction error: 0.02026726 \n",
      "Epoch [71 / 8000] average reconstruction error: 0.01545652 \n",
      "Epoch [81 / 8000] average reconstruction error: 0.01949250 \n",
      "Epoch [91 / 8000] average reconstruction error: 0.01668946 \n",
      "Epoch [101 / 8000] average reconstruction error: 0.01810282 \n",
      "Epoch [111 / 8000] average reconstruction error: 0.01585371 \n",
      "Epoch [121 / 8000] average reconstruction error: 0.01268618 \n",
      "Epoch [131 / 8000] average reconstruction error: 0.01431832 \n",
      "Epoch [141 / 8000] average reconstruction error: 0.00938875 \n",
      "Epoch [151 / 8000] average reconstruction error: 0.01055837 \n",
      "Epoch [161 / 8000] average reconstruction error: 0.01190219 \n",
      "Epoch [171 / 8000] average reconstruction error: 0.00931384 \n",
      "Epoch [181 / 8000] average reconstruction error: 0.00940502 \n",
      "Epoch [191 / 8000] average reconstruction error: 0.00957062 \n",
      "Epoch [201 / 8000] average reconstruction error: 0.00649572 \n",
      "Epoch [211 / 8000] average reconstruction error: 0.00843698 \n",
      "Epoch [221 / 8000] average reconstruction error: 0.00688408 \n",
      "Epoch [231 / 8000] average reconstruction error: 0.00761102 \n",
      "Epoch [241 / 8000] average reconstruction error: 0.00781873 \n",
      "Epoch [251 / 8000] average reconstruction error: 0.00675650 \n",
      "Epoch [261 / 8000] average reconstruction error: 0.00547169 \n",
      "Epoch [271 / 8000] average reconstruction error: 0.00513246 \n",
      "Epoch [281 / 8000] average reconstruction error: 0.00464736 \n",
      "Epoch [291 / 8000] average reconstruction error: 0.00205780 \n",
      "Epoch [301 / 8000] average reconstruction error: 0.00318490 \n",
      "Epoch [311 / 8000] average reconstruction error: 0.00212868 \n",
      "Epoch [321 / 8000] average reconstruction error: 0.00293920 \n",
      "Epoch [331 / 8000] average reconstruction error: 0.00204569 \n",
      "Epoch [341 / 8000] average reconstruction error: 0.00245632 \n",
      "Epoch [351 / 8000] average reconstruction error: 0.00153365 \n",
      "Epoch [361 / 8000] average reconstruction error: 0.00305639 \n",
      "Epoch [371 / 8000] average reconstruction error: 0.00231816 \n",
      "Epoch [381 / 8000] average reconstruction error: 0.00233860 \n",
      "Epoch [391 / 8000] average reconstruction error: 0.00148407 \n",
      "Epoch [401 / 8000] average reconstruction error: 0.00250493 \n",
      "Epoch [411 / 8000] average reconstruction error: 0.00144124 \n",
      "Epoch [421 / 8000] average reconstruction error: 0.00160002 \n",
      "Epoch [431 / 8000] average reconstruction error: 0.00146263 \n",
      "Epoch [441 / 8000] average reconstruction error: 0.00144519 \n",
      "Epoch [451 / 8000] average reconstruction error: 0.00124222 \n",
      "Epoch [461 / 8000] average reconstruction error: 0.00158206 \n",
      "Epoch [471 / 8000] average reconstruction error: 0.00226408 \n",
      "Epoch [481 / 8000] average reconstruction error: 0.00149685 \n",
      "Epoch [491 / 8000] average reconstruction error: 0.00141970 \n",
      "Epoch [501 / 8000] average reconstruction error: 0.00100895 \n",
      "Epoch [511 / 8000] average reconstruction error: 0.00130835 \n",
      "Epoch [521 / 8000] average reconstruction error: 0.00130125 \n",
      "Epoch [531 / 8000] average reconstruction error: 0.00117291 \n",
      "Epoch [541 / 8000] average reconstruction error: 0.00126550 \n",
      "Epoch [551 / 8000] average reconstruction error: 0.00139519 \n",
      "Epoch [561 / 8000] average reconstruction error: 0.00107952 \n",
      "Epoch [571 / 8000] average reconstruction error: 0.00077725 \n",
      "Epoch [581 / 8000] average reconstruction error: 0.00091063 \n",
      "Epoch [591 / 8000] average reconstruction error: 0.00067826 \n",
      "Epoch [601 / 8000] average reconstruction error: 0.00064021 \n",
      "Epoch [611 / 8000] average reconstruction error: 0.00064887 \n",
      "Epoch [621 / 8000] average reconstruction error: 0.00049612 \n",
      "Epoch [631 / 8000] average reconstruction error: 0.00088590 \n",
      "Epoch [641 / 8000] average reconstruction error: 0.00050539 \n",
      "Epoch [651 / 8000] average reconstruction error: 0.00043269 \n",
      "Epoch [661 / 8000] average reconstruction error: 0.00056129 \n",
      "Epoch [671 / 8000] average reconstruction error: 0.00032428 \n",
      "Epoch [681 / 8000] average reconstruction error: 0.00104042 \n",
      "Epoch [691 / 8000] average reconstruction error: 0.00065134 \n",
      "Epoch [701 / 8000] average reconstruction error: 0.00056797 \n",
      "Epoch [711 / 8000] average reconstruction error: 0.00039686 \n",
      "Epoch [721 / 8000] average reconstruction error: 0.00057013 \n",
      "Epoch [731 / 8000] average reconstruction error: 0.00038176 \n",
      "Epoch [741 / 8000] average reconstruction error: 0.00040518 \n",
      "Epoch [751 / 8000] average reconstruction error: 0.00040621 \n",
      "Epoch [761 / 8000] average reconstruction error: 0.00053978 \n",
      "Epoch [771 / 8000] average reconstruction error: 0.00028772 \n",
      "Epoch [781 / 8000] average reconstruction error: 0.00024123 \n",
      "Epoch [791 / 8000] average reconstruction error: 0.00052990 \n",
      "Epoch [801 / 8000] average reconstruction error: 0.00038908 \n",
      "Epoch [811 / 8000] average reconstruction error: 0.00058713 \n",
      "Epoch [821 / 8000] average reconstruction error: 0.00049665 \n",
      "Epoch [831 / 8000] average reconstruction error: 0.00045524 \n",
      "Epoch [841 / 8000] average reconstruction error: 0.00037999 \n",
      "Epoch [851 / 8000] average reconstruction error: 0.00029793 \n",
      "Epoch [861 / 8000] average reconstruction error: 0.00054927 \n",
      "Epoch [871 / 8000] average reconstruction error: 0.00060790 \n",
      "Epoch [881 / 8000] average reconstruction error: 0.00050208 \n",
      "Epoch [891 / 8000] average reconstruction error: 0.00049618 \n",
      "Epoch [901 / 8000] average reconstruction error: 0.00019767 \n",
      "Epoch [911 / 8000] average reconstruction error: 0.00049588 \n",
      "Epoch [921 / 8000] average reconstruction error: 0.00018412 \n",
      "Epoch [931 / 8000] average reconstruction error: 0.00029168 \n",
      "Epoch [941 / 8000] average reconstruction error: 0.00018973 \n",
      "Epoch [951 / 8000] average reconstruction error: 0.00047129 \n",
      "Epoch [961 / 8000] average reconstruction error: 0.00033331 \n",
      "Epoch [971 / 8000] average reconstruction error: 0.00050208 \n",
      "Epoch [981 / 8000] average reconstruction error: 0.00019841 \n",
      "Epoch [991 / 8000] average reconstruction error: 0.00023263 \n",
      "Epoch [1001 / 8000] average reconstruction error: 0.00024501 \n",
      "Epoch [1011 / 8000] average reconstruction error: 0.00033901 \n",
      "Epoch [1021 / 8000] average reconstruction error: 0.00014961 \n",
      "Epoch [1031 / 8000] average reconstruction error: 0.00025144 \n",
      "Epoch [1041 / 8000] average reconstruction error: 0.00024786 \n",
      "Epoch [1051 / 8000] average reconstruction error: 0.00026789 \n",
      "Epoch [1061 / 8000] average reconstruction error: 0.00022159 \n",
      "Epoch [1071 / 8000] average reconstruction error: 0.00028704 \n",
      "Epoch [1081 / 8000] average reconstruction error: 0.00014136 \n",
      "Epoch [1091 / 8000] average reconstruction error: 0.00009560 \n",
      "Epoch [1101 / 8000] average reconstruction error: 0.00011710 \n",
      "Epoch [1111 / 8000] average reconstruction error: 0.00020108 \n",
      "Epoch [1121 / 8000] average reconstruction error: 0.00021370 \n",
      "Epoch [1131 / 8000] average reconstruction error: 0.00023095 \n",
      "Epoch [1141 / 8000] average reconstruction error: 0.00030079 \n",
      "Epoch [1151 / 8000] average reconstruction error: 0.00634573 \n",
      "Epoch [1161 / 8000] average reconstruction error: 0.00023947 \n",
      "Epoch [1171 / 8000] average reconstruction error: 0.00022205 \n",
      "Epoch [1181 / 8000] average reconstruction error: 0.00027285 \n",
      "Epoch [1191 / 8000] average reconstruction error: 0.00020930 \n",
      "Epoch [1201 / 8000] average reconstruction error: 0.00026214 \n",
      "Epoch [1211 / 8000] average reconstruction error: 0.00009073 \n",
      "Epoch [1221 / 8000] average reconstruction error: 0.00013922 \n",
      "Epoch [1231 / 8000] average reconstruction error: 0.00024894 \n",
      "Epoch [1241 / 8000] average reconstruction error: 0.00032009 \n",
      "Epoch [1251 / 8000] average reconstruction error: 0.00024055 \n",
      "Epoch [1261 / 8000] average reconstruction error: 0.00030185 \n",
      "Epoch [1271 / 8000] average reconstruction error: 0.00006040 \n",
      "Epoch [1281 / 8000] average reconstruction error: 0.00014373 \n",
      "Epoch [1291 / 8000] average reconstruction error: 0.00013239 \n",
      "Epoch [1301 / 8000] average reconstruction error: 0.00013791 \n",
      "Epoch [1311 / 8000] average reconstruction error: 0.00015113 \n",
      "Epoch [1321 / 8000] average reconstruction error: 0.00020379 \n",
      "Epoch [1331 / 8000] average reconstruction error: 0.00022584 \n",
      "Epoch [1341 / 8000] average reconstruction error: 0.00018246 \n",
      "Epoch [1351 / 8000] average reconstruction error: 0.00012735 \n",
      "Epoch [1361 / 8000] average reconstruction error: 0.00009631 \n",
      "Epoch [1371 / 8000] average reconstruction error: 0.00020161 \n",
      "Epoch [1381 / 8000] average reconstruction error: 0.00014386 \n",
      "Epoch [1391 / 8000] average reconstruction error: 0.00012055 \n",
      "Epoch [1401 / 8000] average reconstruction error: 0.00022214 \n",
      "Epoch [1411 / 8000] average reconstruction error: 0.00008391 \n",
      "Epoch [1421 / 8000] average reconstruction error: 0.00006470 \n",
      "Epoch [1431 / 8000] average reconstruction error: 0.00015656 \n",
      "Epoch [1441 / 8000] average reconstruction error: 0.00007797 \n",
      "Epoch [1451 / 8000] average reconstruction error: 0.00010600 \n",
      "Epoch [1461 / 8000] average reconstruction error: 0.00010637 \n",
      "Epoch [1471 / 8000] average reconstruction error: 0.00016429 \n",
      "Epoch [1481 / 8000] average reconstruction error: 0.00012250 \n",
      "Epoch [1491 / 8000] average reconstruction error: 0.00031014 \n",
      "Epoch [1501 / 8000] average reconstruction error: 0.00016184 \n",
      "Epoch [1511 / 8000] average reconstruction error: 0.00020077 \n",
      "Epoch [1521 / 8000] average reconstruction error: 0.00006850 \n",
      "Epoch [1531 / 8000] average reconstruction error: 0.00013831 \n",
      "Epoch [1541 / 8000] average reconstruction error: 0.00008980 \n",
      "Epoch [1551 / 8000] average reconstruction error: 0.00009019 \n",
      "Epoch [1561 / 8000] average reconstruction error: 0.00010039 \n",
      "Epoch [1571 / 8000] average reconstruction error: 0.00009734 \n",
      "Epoch [1581 / 8000] average reconstruction error: 0.00004239 \n",
      "Epoch [1591 / 8000] average reconstruction error: 0.00010338 \n",
      "Epoch [1601 / 8000] average reconstruction error: 0.00012749 \n",
      "Epoch [1611 / 8000] average reconstruction error: 0.00014424 \n",
      "Epoch [1621 / 8000] average reconstruction error: 0.00009153 \n",
      "Epoch [1631 / 8000] average reconstruction error: 0.00004905 \n",
      "Epoch [1641 / 8000] average reconstruction error: 0.00012380 \n",
      "Epoch [1651 / 8000] average reconstruction error: 0.00010605 \n",
      "Epoch [1661 / 8000] average reconstruction error: 0.00010746 \n",
      "Epoch [1671 / 8000] average reconstruction error: 0.00005517 \n",
      "Epoch [1681 / 8000] average reconstruction error: 0.00011950 \n",
      "Epoch [1691 / 8000] average reconstruction error: 0.00005867 \n",
      "Epoch [1701 / 8000] average reconstruction error: 0.00010814 \n",
      "Epoch [1711 / 8000] average reconstruction error: 0.00011673 \n",
      "Epoch [1721 / 8000] average reconstruction error: 0.00008632 \n",
      "Epoch [1731 / 8000] average reconstruction error: 0.00003349 \n",
      "Epoch [1741 / 8000] average reconstruction error: 0.00003218 \n",
      "Epoch [1751 / 8000] average reconstruction error: 0.00007651 \n",
      "Epoch [1761 / 8000] average reconstruction error: 0.00003675 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 38\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X52sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m#print(\"total_loss\", total_loss)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X52sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X52sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X52sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X52sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m#scheduler.step()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py_torc\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py_torc\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss_avg = []\n",
    "\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        enc_fold = torch_f.Fold(device)\n",
    "        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "        # Collect computation nodes recursively from encoding process\n",
    "        n_nodes = []\n",
    "        for example in batch: #example es un arbolito\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            encode_structure_fold(enc_fold, example)\n",
    "            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "       \n",
    "        # Apply the computations on the encoder model\n",
    "       \n",
    "        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "        \n",
    "        \n",
    "        # Initialize torchfold for *decoding*\n",
    "        dec_fold = torch_f.Fold(device)\n",
    "        # Collect computation nodes recursively from decoding process\n",
    "        dec_fold_nodes = []\n",
    "        kld_fold_nodes = []\n",
    "\n",
    "        t_l = []\n",
    "        for f in enc_fold_nodes:\n",
    "            for t in f:\n",
    "                t_l.append(t)\n",
    "        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\n",
    "            #print(\"example\", example)\n",
    "            #print(\"fnode\", fnode) \n",
    "            #root_code, kl_div = torch.chunk(fnode, 2, 0)\n",
    "            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\n",
    "        # Apply the computations on the decoder model\n",
    "\n",
    "                       \n",
    "        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        #print(\"n\", n_nodes)\n",
    "        total_loss = torch.div(total_loss[0], n_nodes)\n",
    "        #print(\"div\", total_loss)\n",
    "        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        total_loss = total_loss#*10\n",
    "        \n",
    "        #print(\"total_loss\", total_loss)\n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %.8f ' % (epoch+1, epochs, total_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder con batch - decoder sin batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        # Initialize torchfold for *encoding*\\n\\n        \\n        enc_fold = torch_f.Fold(device)\\n        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\\n        # Collect computation nodes recursively from encoding process\\n        n_nodes = []\\n        for example in batch: #example es un arbolito\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            encode_structure_fold(enc_fold, example)\\n            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\\n       \\n        # Apply the computations on the encoder model\\n       \\n        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\\n        encodeado_con_batch = enc_fold_nodes\\n        \\n        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\\n        #print(\"decoded\", decoded)\\n        l = []\\n        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\\n        l = []\\n        ce_loss_list = decoded.traverseInorderCE(decoded, l)\\n            \\n        mse_loss = sum(mse_loss_list) \\n        ce_loss  = sum(ce_loss_list)  \\n        total_loss = (0.5*ce_loss + mse_loss)\\n        #print(\"total_loss\", total_loss)\\n        total_loss = total_loss / len(mse_loss_list)\\n        \\n        \\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        enc_fold = torch_f.Fold(device)\n",
    "        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "        # Collect computation nodes recursively from encoding process\n",
    "        n_nodes = []\n",
    "        for example in batch: #example es un arbolito\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            encode_structure_fold(enc_fold, example)\n",
    "            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "       \n",
    "        # Apply the computations on the encoder model\n",
    "       \n",
    "        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "        encodeado_con_batch = enc_fold_nodes\n",
    "        \n",
    "        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\n",
    "        #print(\"decoded\", decoded)\n",
    "        l = []\n",
    "        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\n",
    "        l = []\n",
    "        ce_loss_list = decoded.traverseInorderCE(decoded, l)\n",
    "            \n",
    "        mse_loss = sum(mse_loss_list) \n",
    "        ce_loss  = sum(ce_loss_list)  \n",
    "        total_loss = (0.5*ce_loss + mse_loss)\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        total_loss = total_loss / len(mse_loss_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder sin batch - decoder con batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        # Initialize torchfold for *encoding*\\n\\n        \\n        \\n        enc_fold_nodes = []\\n        n_nodes = []\\n        for example in batch:\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            enc_fold = encode_structure(example).to(device)\\n        #print(\"encodeado sin batch\", enc_fold)\\n        enc_fold_nodes.append(enc_fold)\\n        encodeado_sin_batch = enc_fold\\n        # Split into a list of fold nodes per example\\n        #enc_fold_nodes = torch.split(enc_fold_nodes[0], 1, 0) #divide ele ncodeado en vectores de un elemento\\n        \\n        \\n        # Initialize torchfold for *decoding*\\n        dec_fold = torch_f.Fold(device)\\n        # Collect computation nodes recursively from decoding process\\n        dec_fold_nodes = []\\n        kld_fold_nodes = []\\n\\n        t_l = []\\n        for f in enc_fold_nodes:\\n            for t in f:\\n                t_l.append(t)\\n        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\\n            #print(\"example\", example)\\n            #print(\"fnode\", fnode) \\n            #root_code, kl_div = torch.chunk(fnode, 2, 0)\\n            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\\n        # Apply the computations on the decoder model\\n        #print(\"dec fold nodes\", dec_fold_nodes)\\n           \\n                       \\n        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\\n        #print(\"total_loss\", total_loss)\\n        n_nodes = torch.tensor(n_nodes, device = device)\\n        #print(\"n\", n_nodes)\\n        total_loss = torch.div(total_loss[0], n_nodes)\\n        #print(\"div\", total_loss)\\n        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\\n        #print(\"total_loss\", total_loss)\\n        \\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        mse_loss_avg[-1] += (mse_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        \n",
    "        enc_fold_nodes = []\n",
    "        n_nodes = []\n",
    "        for example in batch:\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            enc_fold = encode_structure(example).to(device)\n",
    "        #print(\"encodeado sin batch\", enc_fold)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "        encodeado_sin_batch = enc_fold\n",
    "        # Split into a list of fold nodes per example\n",
    "        #enc_fold_nodes = torch.split(enc_fold_nodes[0], 1, 0) #divide ele ncodeado en vectores de un elemento\n",
    "        \n",
    "        \n",
    "        # Initialize torchfold for *decoding*\n",
    "        dec_fold = torch_f.Fold(device)\n",
    "        # Collect computation nodes recursively from decoding process\n",
    "        dec_fold_nodes = []\n",
    "        kld_fold_nodes = []\n",
    "\n",
    "        t_l = []\n",
    "        for f in enc_fold_nodes:\n",
    "            for t in f:\n",
    "                t_l.append(t)\n",
    "        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\n",
    "            #print(\"example\", example)\n",
    "            #print(\"fnode\", fnode) \n",
    "            #root_code, kl_div = torch.chunk(fnode, 2, 0)\n",
    "            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\n",
    "        # Apply the computations on the decoder model\n",
    "        #print(\"dec fold nodes\", dec_fold_nodes)\n",
    "           \n",
    "                       \n",
    "        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        #print(\"n\", n_nodes)\n",
    "        total_loss = torch.div(total_loss[0], n_nodes)\n",
    "        #print(\"div\", total_loss)\n",
    "        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        mse_loss_avg[-1] += (mse_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder sin batch - decoder sin batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n    ce_avg.append(0)\\n    mse_avg.append(0)\\n\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        \\n        enc_fold_nodes = []\\n        n_nodes = []\\n        for example in batch:\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            enc_fold = encode_structure(example).to(device)\\n        #print(\"encodeado sin batch\", enc_fold)\\n        enc_fold_nodes.append(enc_fold)\\n        encodeado_sin_batch = enc_fold\\n        \\n        \\n        \\n        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\\n        #print(\"decoded\", decoded)\\n        l = []\\n        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\\n        l = []\\n        ce_loss_list = decoded.traverseInorderCE(decoded, l)\\n            \\n        mse_loss = sum(mse_loss_list) \\n        ce_loss  = sum(ce_loss_list)  \\n       \\n        ce = [0.4*a for a in ce_loss_list]\\n\\n\\n        total_loss = (0.4*ce_loss + mse_loss)\\n        total_loss = total_loss / len(mse_loss_list)\\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        mse_avg[-1] += (mse_loss.item())\\n        ce_avg[-1] += (ce_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss, \\'mse loss\\': mse_loss, \\'ce loss\\': ce_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "    ce_avg.append(0)\n",
    "    mse_avg.append(0)\n",
    "\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "        enc_fold_nodes = []\n",
    "        n_nodes = []\n",
    "        for example in batch:\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            enc_fold = encode_structure(example).to(device)\n",
    "        #print(\"encodeado sin batch\", enc_fold)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "        encodeado_sin_batch = enc_fold\n",
    "        \n",
    "        \n",
    "        \n",
    "        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\n",
    "        #print(\"decoded\", decoded)\n",
    "        l = []\n",
    "        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\n",
    "        l = []\n",
    "        ce_loss_list = decoded.traverseInorderCE(decoded, l)\n",
    "            \n",
    "        mse_loss = sum(mse_loss_list) \n",
    "        ce_loss  = sum(ce_loss_list)  \n",
    "       \n",
    "        ce = [0.4*a for a in ce_loss_list]\n",
    "\n",
    "\n",
    "        total_loss = (0.4*ce_loss + mse_loss)\n",
    "        total_loss = total_loss / len(mse_loss_list)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        mse_avg[-1] += (mse_loss.item())\n",
    "        ce_avg[-1] += (ce_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss, 'mse loss': mse_loss, 'ce loss': ce_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1726\n"
     ]
    }
   ],
   "source": [
    "encoder = GRASSEncoder(input_size = 4, feature_size=256, hidden_size=512).to(device)\n",
    "decoder = GRASSDecoder(latent_size=256, hidden_size=512, mult = mult).to(device)\n",
    "\n",
    "checkpoint = torch.load(\"outputs/best_model.pth\")\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "print(\"epoch\", epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'childs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 47\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m dec \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m encoded \u001b[39min\u001b[39;00m enc_fold_nodes:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m    dec\u001b[39m.\u001b[39mappend(decode_testing_grass(encoded, \u001b[39minput\u001b[39;49m, \u001b[39m100\u001b[39;49m, decoder))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m total_loss \u001b[39m=\u001b[39m dec_fold\u001b[39m.\u001b[39mapply(decoder, [dec_fold_nodes])\u001b[39m#[0]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m n_nodes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(n_nodes, device \u001b[39m=\u001b[39m device)\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 47\u001b[0m in \u001b[0;36mdecode_testing_grass\u001b[1;34m(v, root, max, decoder)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m d\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m createNode\u001b[39m.\u001b[39mcount \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m dec \u001b[39m=\u001b[39m decode_node (v, root, \u001b[39mmax\u001b[39;49m, decoder)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dec\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 47\u001b[0m in \u001b[0;36mdecode_testing_grass.<locals>.decode_node\u001b[1;34m(v, node, max, decoder)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     nodoSiguienteLeft \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m d\u001b[39m.\u001b[39mright \u001b[39m=\u001b[39m decode_node(right, nodoSiguienteRight, \u001b[39mmax\u001b[39m, decoder)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m d\u001b[39m.\u001b[39mleft \u001b[39m=\u001b[39m decode_node(left, nodoSiguienteLeft, \u001b[39mmax\u001b[39;49m, decoder)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mreturn\u001b[39;00m d\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 47\u001b[0m in \u001b[0;36mdecode_testing_grass.<locals>.decode_node\u001b[1;34m(v, node, max, decoder)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     nodoSiguienteLeft \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m d\u001b[39m.\u001b[39mright \u001b[39m=\u001b[39m decode_node(right, nodoSiguienteRight, \u001b[39mmax\u001b[39m, decoder)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m d\u001b[39m.\u001b[39mleft \u001b[39m=\u001b[39m decode_node(left, nodoSiguienteLeft, \u001b[39mmax\u001b[39;49m, decoder)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mreturn\u001b[39;00m d\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 47\u001b[0m in \u001b[0;36mdecode_testing_grass.<locals>.decode_node\u001b[1;34m(v, node, max, decoder)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     nodoSiguienteRight \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     nodoSiguienteLeft \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m d\u001b[39m.\u001b[39mright \u001b[39m=\u001b[39m decode_node(right, nodoSiguienteRight, \u001b[39mmax\u001b[39;49m, decoder)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m d\u001b[39m.\u001b[39mleft \u001b[39m=\u001b[39m decode_node(left, nodoSiguienteLeft, \u001b[39mmax\u001b[39m, decoder)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mreturn\u001b[39;00m d\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 47\u001b[0m in \u001b[0;36mdecode_testing_grass.<locals>.decode_node\u001b[1;34m(v, node, max, decoder)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m label \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m createNode\u001b[39m.\u001b[39mcount \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m: \u001b[39m##output del classifier\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     radio \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39mfeatureDecoder(v)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     lossEstructura \u001b[39m=\u001b[39m calcularLossEstructura(cl, node)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     lossAtrs \u001b[39m=\u001b[39m calcularLossAtributo( node\u001b[39m.\u001b[39mradius, radio )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     multipl \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mlevel\u001b[39m/\u001b[39mnode\u001b[39m.\u001b[39mtreelevel\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 47\u001b[0m in \u001b[0;36mcalcularLossEstructura\u001b[1;34m(cl_p, original)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m mult \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39mround\u001b[39m(qzero),\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39mround\u001b[39m(qOne),\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39mround\u001b[39m(qtwo)], device \u001b[39m=\u001b[39m device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ce \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss(weight \u001b[39m=\u001b[39m mult)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m original\u001b[39m.\u001b[39;49mchilds() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     vector \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m] \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#Y116sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m original\u001b[39m.\u001b[39mchilds() \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'childs'"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss_avg = []\n",
    "\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "train_loss_avg.append(0)\n",
    "for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "\n",
    "    enc_fold_nodes = []\n",
    "    n_nodes = []\n",
    "    for example in batch:\n",
    "        c = []\n",
    "        n = example.count_nodes(example, c)\n",
    "        n_nodes.append(len(n))\n",
    "        enc_fold = encode_structure(example).to(device)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "  \n",
    "    dec = []\n",
    "    for encoded in enc_fold_nodes:\n",
    "       dec.append(decode_testing_grass(encoded, input, 100, decoder))\n",
    "    total_loss = dec_fold.apply(decoder, [dec_fold_nodes])#[0]\n",
    "    n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        \n",
    "    total_loss = torch.div(total_loss[0], n_nodes)\n",
    "print(\"testing loss\", total_loss)\n",
    "avg_testing_loss = total_loss.sum() / len(batch) \n",
    "print(\"testing loss\", avg_testing_loss)\n",
    "\n",
    "        #total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[0.6152, 0.6160, 0.6155, 0.6581]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "1 tensor([[0.1951, 0.1922, 0.1925, 0.3283]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "2 tensor([[0.5916, 0.5899, 0.5905, 0.6611]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "3 tensor([[-0.0073, -0.0084, -0.0077, -0.0079]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "4 tensor([[0.8226, 0.8224, 0.8211, 0.6670]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "5 tensor([[0.3995, 0.4009, 0.4010, 0.3275]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "6 tensor([[0.9747, 0.9760, 0.9782, 0.6694]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "7 tensor([[0.8155, 0.8169, 0.8164, 0.9921]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = iter(data_loader).next()[0]\n",
    "enc_fold = torch_f.Fold(device)\n",
    "enc_fold_nodes = []\n",
    "enc_fold_nodes.append(encode_structure_fold(enc_fold, input))\n",
    "enc_fold_nodes = enc_fold.apply(encoder, [enc_fold_nodes])\n",
    "encoded = enc_fold_nodes[0]\n",
    "decoded = decode_testing_grass(encoded, input, 100, decoder)\n",
    "\n",
    "count = []\n",
    "numerar_nodos(decoded, count)\n",
    "decoded.traverseInorder(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31ff24a2f2e4576beb3da9eca6c1915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.5, 0.5,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTree(input, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebedf4b82e14db2b4e7d507803e1f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.4836823â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTree(decoded, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "n_nodes = input.count_nodes(input,c)\n",
    "len(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "n_nodes = decoded.count_nodes(decoded,c)\n",
    "len(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1 3\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "decoded.traverseInorderChilds(decoded, li)\n",
    "zero = [a for a in li if a == 0]\n",
    "one = [a for a in li if a == 1]\n",
    "two = [a for a in li if a == 2]\n",
    "qzero = len(zero)\n",
    "qOne = len(one)\n",
    "qtwo = len(two)\n",
    "print(qzero, qOne, qtwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1 3\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "input.traverseInorderChilds(input, li)\n",
    "zero = [a for a in li if a == 0]\n",
    "one = [a for a in li if a == 1]\n",
    "two = [a for a in li if a == 2]\n",
    "qzero = len(zero)\n",
    "qOne = len(one)\n",
    "qtwo = len(two)\n",
    "print(qzero, qOne, qtwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (20,10))\n",
    "plt.plot(train_loss_avg) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverseleaf(root):\n",
    "    if root is not None:\n",
    "        traverseleaf(root.left)\n",
    "        if root.is_leaf():\n",
    "            print(root.radius)\n",
    "        traverseleaf(root.right)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traversebif(root):\n",
    "    if root is not None:\n",
    "        traversebif(root.left)\n",
    "        if root.is_two_child():\n",
    "            print(root.radius)\n",
    "        traversebif(root.right)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1951, 0.1922, 0.1925, 0.3283]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0073, -0.0084, -0.0077, -0.0079]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3995, 0.4009, 0.4010, 0.3275]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "////\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.3333], device='cuda:0')\n",
      "tensor([0., 0., 0., 0.], device='cuda:0')\n",
      "tensor([0.4000, 0.4000, 0.4000, 0.3333], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "traversebif(decoded)\n",
    "print(\"////\")\n",
    "traversebif(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6152, 0.6160, 0.6155, 0.6581]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5916, 0.5899, 0.5905, 0.6611]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8226, 0.8224, 0.8211, 0.6670]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8155, 0.8169, 0.8164, 0.9921]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "traverseleaf(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6000, 0.6000, 0.6000, 0.6667], device='cuda:0')\n",
      "tensor([0.6000, 0.6000, 0.6000, 0.6667], device='cuda:0')\n",
      "tensor([0.8000, 0.8000, 0.8000, 1.0000], device='cuda:0')\n",
      "tensor([0.8000, 0.8000, 0.8000, 0.6667], device='cuda:0')\n",
      "tensor([0.8000, 0.8000, 0.8000, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "traverseleaf(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py_torc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f3e717cd274da89498094fde320e6eab1bf0f52911d27cf47473187acb3fe8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
