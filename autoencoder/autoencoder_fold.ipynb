{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import raiseExceptions\n",
    "from tokenize import Double\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from vec3 import Vec3\n",
    "import meshplot as mp\n",
    "import torch\n",
    "torch.manual_seed(125)\n",
    "import random\n",
    "random.seed(125)\n",
    "import torch_f as torch_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_fn(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        wrapper.count += 1\n",
    "        return f(*args, **kwargs)\n",
    "    wrapper.count = 0\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase nodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Class Node\n",
    "    \"\"\"\n",
    "    def __init__(self, value, radius, left = None, right = None, position = None, cl_prob= None, ce = None, mse = None, level = None, treelevel = None):\n",
    "        self.left = left\n",
    "        self.data = value\n",
    "        self.radius = radius\n",
    "        self.position = position\n",
    "        self.right = right\n",
    "        self.prob = cl_prob\n",
    "        self.mse = mse\n",
    "        self.ce = ce\n",
    "        self.children = [self.left, self.right]\n",
    "        self.level = level\n",
    "        self.treelevel = treelevel\n",
    "    \n",
    "    def agregarHijo(self, children):\n",
    "\n",
    "        if self.right is None:\n",
    "            self.right = children\n",
    "        elif self.left is None:\n",
    "            self.left = children\n",
    "\n",
    "        else:\n",
    "            raise ValueError (\"solo arbol binario \")\n",
    "\n",
    "\n",
    "    def is_leaf(self):\n",
    "        if self.right is None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_two_child(self):\n",
    "        if self.right is not None and self.left is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_one_child(self):\n",
    "        if self.is_two_child():\n",
    "            return False\n",
    "        elif self.is_leaf():\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def childs(self):\n",
    "        if self.is_leaf():\n",
    "            return 0\n",
    "        if self.is_one_child():\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    \n",
    "    \n",
    "    def traverseInorder(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorder(root.left)\n",
    "            print (root.data, root.radius)\n",
    "            self.traverseInorder(root.right)\n",
    "\n",
    "    def traverseInorderwl(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderwl(root.left)\n",
    "            print (root.data, root.radius, root.level, root.treelevel)\n",
    "            self.traverseInorderwl(root.right)\n",
    "\n",
    "    def get_tree_level(self, root, c):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.get_tree_level(root.left, c)\n",
    "            c.append(root.level)\n",
    "            self.get_tree_level(root.right, c)\n",
    "\n",
    "    def set_tree_level(self, root, c):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.set_tree_level(root.left, c)\n",
    "            root.treelevel = c\n",
    "            self.set_tree_level(root.right, c)\n",
    "\n",
    "    def traverseInorderLoss(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderLoss(root.left, loss)\n",
    "            loss.append(root.prob)\n",
    "            self.traverseInorderLoss(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderMSE(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderMSE(root.left, loss)\n",
    "            loss.append(root.mse)\n",
    "            self.traverseInorderMSE(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderCE(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderCE(root.left, loss)\n",
    "            loss.append(root.ce)\n",
    "            self.traverseInorderCE(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderChilds(self, root, l):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderChilds(root.left, l)\n",
    "            l.append(root.childs())\n",
    "            self.traverseInorderChilds(root.right, l)\n",
    "            return l\n",
    "\n",
    "    def preorder(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            print (root.data, root.radius)\n",
    "            self.preorder(root.left)\n",
    "            self.preorder(root.right)\n",
    "\n",
    "    def cloneBinaryTree(self, root):\n",
    "     \n",
    "        # base case\n",
    "        if root is None:\n",
    "            return None\n",
    "    \n",
    "        # create a new node with the same data as the root node\n",
    "        root_copy = Node(root.data, root.radius)\n",
    "    \n",
    "        # clone the left and right subtree\n",
    "        root_copy.left = self.cloneBinaryTree(root.left)\n",
    "        root_copy.right = self.cloneBinaryTree(root.right)\n",
    "    \n",
    "        # return cloned root node\n",
    "        return root_copy\n",
    "\n",
    "    def height(self, root):\n",
    "    # Check if the binary tree is empty\n",
    "        if root is None:\n",
    "            return 0 \n",
    "        # Recursively call height of each node\n",
    "        leftAns = self.height(root.left)\n",
    "        rightAns = self.height(root.right)\n",
    "    \n",
    "        # Return max(leftHeight, rightHeight) at each iteration\n",
    "        return max(leftAns, rightAns) + 1\n",
    "\n",
    "    # Print nodes at a current level\n",
    "    def printCurrentLevel(self, root, level):\n",
    "        if root is None:\n",
    "            return\n",
    "        if level == 1:\n",
    "            print(root.data, end=\" \")\n",
    "        elif level > 1:\n",
    "            self.printCurrentLevel(root.left, level-1)\n",
    "            self.printCurrentLevel(root.right, level-1)\n",
    "\n",
    "    def printLevelOrder(self, root):\n",
    "        h = self.height(root)\n",
    "        for i in range(1, h+1):\n",
    "            self.printCurrentLevel(root, i)\n",
    "\n",
    "\n",
    "    \n",
    "    def count_nodes(self, root, counter):\n",
    "        if   root is not None:\n",
    "            self.count_nodes(root.left, counter)\n",
    "            counter.append(root.data)\n",
    "            self.count_nodes(root.right, counter)\n",
    "            return counter\n",
    "\n",
    "    \n",
    "    def serialize(self, root):\n",
    "        def post_order(root):\n",
    "            if root:\n",
    "                post_order(root.left)\n",
    "                post_order(root.right)\n",
    "                ret[0] += str(root.data)+'_'+ str(root.radius) +';'\n",
    "                \n",
    "            else:\n",
    "                ret[0] += '#;'           \n",
    "\n",
    "        ret = ['']\n",
    "        post_order(root)\n",
    "        return ret[0][:-1]  # remove last ,\n",
    "\n",
    "    def toGraph( self, graph, index, dec, proc=True):\n",
    "        \n",
    "        \n",
    "        radius = self.radius.cpu().detach().numpy()\n",
    "        if dec:\n",
    "            radius= radius[0]\n",
    "        #print(\"posicion\", self.data, radius)\n",
    "        #print(\"right\", self.right)\n",
    "        \n",
    "        #graph.add_nodes_from( [ (index, {'posicion': radius[0:3], 'radio': radius[3] } ) ])\n",
    "        graph.add_nodes_from( [ (self.data, {'posicion': radius[0:3], 'radio': radius[3] } ) ])\n",
    "        \n",
    "\n",
    "        if self.right is not None:\n",
    "            #leftIndex = self.right.toGraph( graph, index + 1, dec)#\n",
    "            self.right.toGraph( graph, index + 1, dec)#\n",
    "            \n",
    "            #graph.add_edge( index, index + 1 )\n",
    "            graph.add_edge( self.data, self.right.data )\n",
    "            #if proc:\n",
    "            #    nx.set_edge_attributes( graph, {(index, index+1) : {'procesada':False}})\n",
    "        \n",
    "        if self.left is not None:\n",
    "            #retIndex = self.left.toGraph( graph, leftIndex, dec )#\n",
    "            self.left.toGraph( graph, 0, dec )#\n",
    "\n",
    "            #graph.add_edge( index, leftIndex)\n",
    "            graph.add_edge( self.data, self.left.data)\n",
    "            #if proc:\n",
    "            #    nx.set_edge_attributes( graph, {(index, leftIndex) : {'procesada':False}})\n",
    "\n",
    "        else:\n",
    "            #return index + 1\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTree( root, dec ):\n",
    "    graph = nx.Graph()\n",
    "    root.toGraph( graph, 0, dec)\n",
    "    edges=nx.get_edge_attributes(graph,'procesada')\n",
    "\n",
    "    p = mp.plot( np.array([ graph.nodes[v]['posicion'] for v in graph.nodes]), shading={'point_size':0.1}, return_plot=True)\n",
    "\n",
    "    for arista in graph.edges:\n",
    "        p.add_lines( graph.nodes[arista[0]]['posicion'], graph.nodes[arista[1]]['posicion'])\n",
    "\n",
    "    return \n",
    "\n",
    "def traverse(root, tree):\n",
    "       \n",
    "        if root is not None:\n",
    "            traverse(root.left, tree)\n",
    "            tree.append((root.radius, root.data))\n",
    "            traverse(root.right, tree)\n",
    "            return tree\n",
    "\n",
    "def traverse_2(tree1, tree2, t_l):\n",
    "       \n",
    "        if tree1 is not None:\n",
    "            traverse_2(tree1.left, tree2.left, t_l)\n",
    "            if tree2:\n",
    "                t_l.append((tree1.radius, tree2.radius))\n",
    "                print((tree1.radius, tree2.radius))\n",
    "            else:\n",
    "                t_l.append(tree1.radius)\n",
    "                print((tree1.radius))\n",
    "            traverse_2(tree1.right, tree2, t_l)\n",
    "            return t_l\n",
    "            \n",
    "\n",
    "def traverse_conexiones(root, tree):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            traverse_conexiones(root.left, tree)\n",
    "            if root.right is not None:\n",
    "                tree.append((root.data, root.right.data))\n",
    "            if root.left is not None:\n",
    "                tree.append((root.data, root.left.data))\n",
    "            traverse_conexiones(root.right, tree)\n",
    "            return tree\n",
    "\n",
    "def arbolAGrafo (nodoRaiz):\n",
    "    \n",
    "    conexiones = []\n",
    "    lineas = traverse_conexiones(nodoRaiz, conexiones)\n",
    "    tree = []\n",
    "    tree = traverse(nodoRaiz, tree)\n",
    "\n",
    "    vertices = []\n",
    "    verticesCrudos = []\n",
    "    for node in tree:\n",
    "        vertice = node[0][0][:3]\n",
    "        rad = node[0][0][-1]\n",
    "        num = node[1]\n",
    "        \n",
    "        #vertices.append((num, {'posicion': Vec3( vertice[0], vertice[1], vertice[2]), 'radio': rad} ))\n",
    "        vertices.append((len(verticesCrudos),{'posicion': Vec3( vertice[0], vertice[1], vertice[2]), 'radio': rad}))\n",
    "        verticesCrudos.append(vertice)\n",
    "\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from( vertices )\n",
    "    G.add_edges_from( lineas )\n",
    "    \n",
    "    return G\n",
    "\n",
    "@count_fn\n",
    "def createNode(data, radius, position = None, left = None, right = None, cl_prob = None, ce = None, mse=None):\n",
    "        \"\"\"\n",
    "        Utility function to create a node.\n",
    "        \"\"\"\n",
    "        return Node(data, radius, position, left, right, cl_prob, ce, mse)\n",
    " \n",
    "def deserialize(data):\n",
    "    if  not data:\n",
    "        return \n",
    "    nodes = data.split(';')  \n",
    "    #print(\"node\",nodes[3])\n",
    "    def post_order(nodes):\n",
    "                \n",
    "        if nodes[-1] == '#':\n",
    "            nodes.pop()\n",
    "            return None\n",
    "        node = nodes.pop().split('_')\n",
    "        data = int(node[0])\n",
    "        #radius = float(node[1])\n",
    "        #print(\"node\", node)\n",
    "        #breakpoint()\n",
    "        radius = node[1]\n",
    "        #print(\"radius\", radius)\n",
    "        rad = radius.split(\",\")\n",
    "        rad [0] = rad[0].replace('[','')\n",
    "        rad [3] = rad[3].replace(']','')\n",
    "        r = []\n",
    "        for value in rad:\n",
    "            r.append(float(value))\n",
    "        #r =[float(num) for num in radius if num.isdigit()]\n",
    "        r = torch.tensor(r, device=device)\n",
    "        #breakpoint()\n",
    "        root = createNode(data, r)\n",
    "        root.right = post_order(nodes)\n",
    "        root.left = post_order(nodes)\n",
    "        \n",
    "        return root    \n",
    "    return post_order(nodes)    \n",
    "\n",
    "\n",
    "def read_tree(filename):\n",
    "    #with open('./trees/' +'prof3/' +filename, \"r\") as f:\n",
    "    with open('./trees/' +filename, \"r\") as f:\n",
    "        byte = f.read() \n",
    "        return byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternalEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, feature_size: int, hidden_size: int):\n",
    "        super(InternalEncoder, self).__init__()\n",
    "\n",
    "        #print(\"init\")\n",
    "        # Encoders atributos\n",
    "        self.attribute_lin_encoder_1 = nn.Linear(input_size,feature_size)\n",
    "        self.attribute_lin_encoder_2 = nn.Linear(feature_size,hidden_size)\n",
    "        self.attribute_lin_encoder_3 = nn.Linear(hidden_size,feature_size)\n",
    "\n",
    "        # Encoders derecho e izquierdo\n",
    "        self.right_lin_encoder_1 = nn.Linear(feature_size,hidden_size)\n",
    "        self.right_lin_encoder_2 = nn.Linear(hidden_size,feature_size)\n",
    "        #self.right_lin_encoder_3 = nn.Linear(feature_size,feature_size)\n",
    "\n",
    "        self.left_lin_encoder_1  = nn.Linear(feature_size,hidden_size)\n",
    "        self.left_lin_encoder_2  = nn.Linear(hidden_size,feature_size)\n",
    "        #self.left_lin_encoder_3  = nn.Linear(feature_size,feature_size)\n",
    "\n",
    "\n",
    "        # Encoder final\n",
    "        self.final_lin_encoder_1 = nn.Linear(2*feature_size, feature_size)\n",
    "\n",
    "        # Funciones / Parametros utiles\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "\n",
    "    def forward(self, input, right_input, left_input):\n",
    "        # Encodeo los atributos\n",
    "        attributes = self.attribute_lin_encoder_1(input)\n",
    "        attributes = self.tanh(attributes)\n",
    "        attributes = self.attribute_lin_encoder_2(attributes)\n",
    "        attributes = self.tanh(attributes)\n",
    "        attributes = self.attribute_lin_encoder_3(attributes)\n",
    "        attributes = self.tanh(attributes)\n",
    "        #print(\"attributes\", attributes)\n",
    "\n",
    "        # Encodeo el derecho\n",
    "        if right_input is not None:\n",
    "            #print(\"right input\", right_input)\n",
    "            context = self.right_lin_encoder_1(right_input)\n",
    "            context = self.tanh(context)\n",
    "            context = self.right_lin_encoder_2(context)\n",
    "            #context = self.tanh(context)\n",
    "            #context = self.right_lin_encoder_3(context)\n",
    "            \n",
    "            # Encodeo el izquierdo\n",
    "            #print(\"left input\", left_input)\n",
    "            if left_input is not None:\n",
    "                left = self.left_lin_encoder_1(left_input)\n",
    "                #print(\"izquierdo\", left.shape)\n",
    "                left = self.tanh(left)\n",
    "                #left = self.left_lin_encoder_2(left)\n",
    "                #left = self.tanh(left)\n",
    "                context += self.left_lin_encoder_2(left)\n",
    "                #print(\"context izquierdo\", context.shape)\n",
    "        else:\n",
    "            context = torch.zeros(input.shape[0],self.feature_size, requires_grad=True, device=device)\n",
    "        \n",
    "\n",
    "        context = self.tanh(context)\n",
    "        feature = torch.cat((attributes,context), 1)\n",
    "        feature = self.final_lin_encoder_1(feature)\n",
    "        feature = self.tanh(feature)\n",
    "        #print(\"output\", feature)\n",
    "        return feature\n",
    "\n",
    "       \n",
    "    \n",
    "\n",
    "class GRASSEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, feature_size : int, hidden_size: int):\n",
    "        super(GRASSEncoder, self).__init__()\n",
    "        self.leaf_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        self.internal_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        self.bifurcation_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        \n",
    "    def leafEncoder(self, node, right=None, left = None):\n",
    "        return self.internal_encoder(node, right, left)\n",
    "    def internalEncoder(self, node, right, left = None):\n",
    "        return self.internal_encoder(node, right, left)\n",
    "    def bifurcationEncoder(self, node, right, left):\n",
    "        \n",
    "        return self.bifurcation_encoder(node, right, left)\n",
    "\n",
    "Grassencoder = GRASSEncoder(input_size = 4, feature_size=512, hidden_size=1024)\n",
    "Grassencoder = Grassencoder.to(device)\n",
    "\n",
    "\n",
    "def encode_structure_fold(fold, root):\n",
    "    \n",
    "    \n",
    "    def encode_node(node):\n",
    "        \n",
    "        if node is None:\n",
    "            return\n",
    "        \n",
    "        if node.is_leaf():\n",
    "            return fold.add('leafEncoder', node.radius)\n",
    "        else:\n",
    "            left = encode_node(node.left)\n",
    "            right = encode_node(node.right)\n",
    "            if left is not None:\n",
    "             \n",
    "                return fold.add('bifurcationEncoder', node.radius, right, left)\n",
    "            else:\n",
    "                return fold.add('internalEncoder', node.radius, right)\n",
    "        \n",
    "\n",
    "    encoding = encode_node(root)\n",
    "    \n",
    "    return encoding\n",
    "  \n",
    "def encode_structure(root):\n",
    "    \n",
    "    def encode_node(node):\n",
    "          \n",
    "        if node is None:\n",
    "            return\n",
    "        if node.is_leaf():\n",
    "            return Grassencoder.leafEncoder(node.radius.reshape(-1,4))\n",
    "        else :\n",
    "            left = encode_node(node.left)\n",
    "            right = encode_node(node.right)\n",
    "            if left is not None:\n",
    "                return Grassencoder.bifurcationEncoder(node.radius.reshape(-1,4), right, left)\n",
    "            else:\n",
    "                return Grassencoder.internalEncoder(node.radius.reshape(-1,4), right)\n",
    "        \n",
    "\n",
    "    encoding = encode_node(root)\n",
    "   \n",
    "    return encoding\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerar_nodos(root, count):\n",
    "    if root is not None:\n",
    "        numerar_nodos(root.left, count)\n",
    "        root.data = len(count)\n",
    "        count.append(1)\n",
    "        numerar_nodos(root.right, count)\n",
    "        return \n",
    "\n",
    "\n",
    "def traversefeatures(root, features):\n",
    "       \n",
    "    if root is not None:\n",
    "        traversefeatures(root.left, features)\n",
    "        features.append(root.radius)\n",
    "        traversefeatures(root.right, features)\n",
    "        return features\n",
    "\n",
    "def norm(root, minx, miny, minz, minr, maxx, maxy, maxz, maxr):\n",
    "    \n",
    "    if root is not None:\n",
    "        mx = minx.clone().detach()\n",
    "        my = miny.clone().detach()\n",
    "        mz = minz.clone().detach()\n",
    "        mr = minr.clone().detach()\n",
    "        Mx = maxx.clone().detach()\n",
    "        My = maxy.clone().detach()\n",
    "        Mz = maxz.clone().detach()\n",
    "        Mr = maxr.clone().detach()\n",
    "       \n",
    "        root.radius[0] = (root.radius[0] - minx)/(maxx - minx)\n",
    "        root.radius[1] = (root.radius[1] - miny)/(maxy - miny)\n",
    "        root.radius[2] = (root.radius[2] - minz)/(maxz - minz)\n",
    "        root.radius[3] = (root.radius[3] - minr)/(maxr - minr)\n",
    "        \n",
    "        norm(root.left, mx, my, mz, mr, Mx, My, Mz, Mr)\n",
    "        norm(root.right, mx, my, mz, mr, Mx, My, Mz, Mr)\n",
    "        return \n",
    "\n",
    "def normalize_features(root):\n",
    "    features = []\n",
    "    features = traversefeatures(root, features)\n",
    "    \n",
    "    x = [tensor[0] for tensor in features]\n",
    "    y = [tensor[1] for tensor in features]\n",
    "    z = [tensor[2] for tensor in features]\n",
    "    r = [tensor[3] for tensor in features]\n",
    " \n",
    "    norm(root, min(x), min(y), min(z), min(r), max(x), max(y), max(z), max(r))\n",
    "\n",
    "    return \n",
    "\n",
    "def traversefeatures(root, features):\n",
    "       \n",
    "    if root is not None:\n",
    "        traversefeatures(root.left, features)\n",
    "        features.append(root.radius)\n",
    "        traversefeatures(root.right, features)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tree0.dat', 'tree1.dat', 'tree10.dat', 'tree100.dat', 'tree101.dat', 'tree102.dat', 'tree103.dat', 'tree104.dat', 'tree105.dat', 'tree106.dat', 'tree107.dat', 'tree108.dat', 'tree109.dat', 'tree11.dat', 'tree110.dat', 'tree111.dat', 'tree112.dat', 'tree113.dat', 'tree114.dat', 'tree115.dat']\n"
     ]
    }
   ],
   "source": [
    "def my_collate(batch):\n",
    "    return batch\n",
    "\n",
    "#t_list = ['ArteryObjAN1-7.dat','ArteryObjAN1-0.dat', 'ArteryObjAN1-17.dat',  'ArteryObjAN1-11.dat']\n",
    "\n",
    "#t_list = ['ArteryObjAN1-0.dat','ArteryObjAN1-7.dat', 'ArteryObjAN1-17.dat',  'ArteryObjAN1-11.dat', 'ArteryObjAN1-19.dat', 'ArteryObjAN2-4.dat', 'ArteryObjAN2-6.dat', \n",
    "#           'ArteryObjAN25-18.dat']\n",
    "#t_list = ['ArteryObjAN1-17-55.dat', 'ArteryObjAN1-17-22.dat', \"ArteryObjAN1-17-12.dat\", \"ArteryObjAN1-17-9.dat\",'ArteryObjAN1-17-42.dat', 'ArteryObjAN1-17-64.dat', \"ArteryObjAN1-17-70.dat\", \"ArteryObjAN1-17-1.dat\"]\n",
    "#t_list = ['ArteryObjAN1-17.dat']\n",
    "#t_list = ['ArteryObjAN1-11.dat']\n",
    "\n",
    "\n",
    "#t_list = ['test2.dat']\n",
    "\n",
    "#t_list = ['ArteryObjAN31-14.dat']\n",
    "#t_list = os.listdir(\"./trees\")[:20]\n",
    "t_list = os.listdir(\"./trees/prof5\")[:20]\n",
    "print(t_list)\n",
    "class tDataset(Dataset):\n",
    "    def __init__(self, dir, transform=None):\n",
    "        self.names = dir\n",
    "        self.transform = transform\n",
    "        self.data = [] #lista con las strings de todos los arboles\n",
    "        for file in self.names:\n",
    "            self.data.append(read_tree('prof5/'+file))\n",
    "        self.trees = []\n",
    "        for tree in self.data:\n",
    "            deserial = deserialize(tree)\n",
    "            normalize_features(deserial)\n",
    "            self.trees.append(deserial)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #file = self.names[idx]\n",
    "        #string = read_tree(file)\n",
    "        tree = self.trees[idx]\n",
    "        return tree\n",
    "\n",
    "batch_size = 10\n",
    "dataset = tDataset(t_list)\n",
    "data_loader = DataLoader(dataset, batch_size = batch_size, shuffle=True, collate_fn=my_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2319fd71e2544b02905dc4de639ea0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.5, 0.5,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 tensor([0.8421, 0.3077, 0.3478, 1.0000], device='cuda:0')\n",
      "10 tensor([0.6316, 0.2308, 0.2609, 0.7500], device='cuda:0')\n",
      "11 tensor([0.8947, 0.5385, 0.5652, 1.0000], device='cuda:0')\n",
      "9 tensor([0.4211, 0.1538, 0.1739, 0.5000], device='cuda:0')\n",
      "8 tensor([0.2105, 0.0769, 0.0870, 0.2500], device='cuda:0')\n",
      "0 tensor([0., 0., 0., 0.], device='cuda:0')\n",
      "7 tensor([0.9474, 0.7692, 0.7826, 1.0000], device='cuda:0')\n",
      "6 tensor([0.6842, 0.4615, 0.4783, 0.7500], device='cuda:0')\n",
      "5 tensor([0.4737, 0.3846, 0.3913, 0.5000], device='cuda:0')\n",
      "1 tensor([0.2632, 0.3077, 0.3043, 0.2500], device='cuda:0')\n",
      "3 tensor([0.7368, 0.6923, 0.6957, 0.7500], device='cuda:0')\n",
      "4 tensor([1., 1., 1., 1.], device='cuda:0')\n",
      "2 tensor([0.5263, 0.6154, 0.6087, 0.5000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input = iter(data_loader).next()[0]\n",
    "plotTree(input, False)\n",
    "input.traverseInorder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED con batch [tensor([[-0.0240, -0.0804, -0.0667,  ...,  0.0373,  0.0252, -0.0117],\n",
      "        [-0.0636,  0.0128,  0.0566,  ...,  0.0336, -0.0087, -0.0035]],\n",
      "       device='cuda:0', grad_fn=<StackBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "import torch_f\n",
    "enc_fold = torch_f.Fold(device)\n",
    "enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "batch = iter(data_loader).next()\n",
    "#for example in batch:\n",
    "        #enc_fold.add('leafEncoder', example.radius)\n",
    "        #enc_fold_nodes.append(enc_fold.add('leafEncoder', example.radius))\n",
    "        #enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "\n",
    "        #print(\"enc fold nodes\", enc_fold)\n",
    "for example in data_loader:\n",
    "        example = example[0]\n",
    "        enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "print(\"ENCODED con batch\",enc_fold_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encodeado sin batch tensor([[-6.3565e-02,  1.2800e-02,  5.6643e-02,  7.6785e-02,  1.1112e-01,\n",
      "          9.9652e-02,  8.6144e-02, -1.0678e-02, -1.6248e-02, -2.1413e-02,\n",
      "         -1.1831e-02,  9.3471e-02, -2.1985e-02, -8.7761e-02, -3.6197e-03,\n",
      "         -8.6504e-03,  2.7813e-02, -5.0151e-03,  1.3058e-01,  5.4752e-03,\n",
      "          9.4314e-03,  4.0972e-02,  5.0691e-02,  1.4831e-03, -8.2162e-04,\n",
      "         -2.6694e-02,  7.0436e-02,  3.2679e-02, -3.3324e-02,  5.0756e-02,\n",
      "          6.3833e-02,  2.9954e-02, -9.2533e-04,  2.9424e-03, -9.4070e-02,\n",
      "          1.6564e-02, -1.4240e-02,  7.1206e-02,  3.2219e-02, -8.0858e-03,\n",
      "          1.6992e-02, -5.4562e-03, -3.0200e-02, -9.5787e-02,  8.5333e-02,\n",
      "         -4.6153e-03,  8.1972e-02,  4.1122e-02, -1.6673e-02, -8.5443e-02,\n",
      "          3.2675e-02,  5.3539e-02,  7.9505e-03, -5.6123e-03, -7.5445e-02,\n",
      "          3.1193e-02,  5.8324e-02, -5.1287e-03,  3.5413e-02, -8.6599e-02,\n",
      "         -2.8297e-02, -4.5190e-02,  1.1515e-03,  9.2981e-02,  6.3740e-02,\n",
      "         -1.3008e-02,  3.7680e-03,  1.8309e-02, -3.2815e-02, -4.3291e-03,\n",
      "          5.0832e-03,  1.9592e-02, -1.1103e-01, -6.6462e-03, -2.7733e-03,\n",
      "         -1.1663e-01,  3.9494e-02,  4.5700e-02, -2.7691e-02,  3.0154e-02,\n",
      "          6.0392e-02,  4.9757e-02,  1.3495e-02, -5.3358e-02,  7.6813e-02,\n",
      "          5.8484e-02, -3.8527e-02,  2.1916e-02,  3.7922e-02, -4.1814e-02,\n",
      "          4.5529e-02, -1.4990e-02,  7.0315e-03, -1.6094e-02, -6.2035e-02,\n",
      "          1.2400e-02,  2.5829e-03,  4.0141e-02, -4.4436e-02, -3.0779e-02,\n",
      "         -1.1469e-02,  8.0513e-02, -8.8054e-03,  5.1116e-02,  6.7440e-04,\n",
      "          1.3046e-03, -4.2603e-02, -3.1799e-02,  1.1776e-01,  2.8310e-02,\n",
      "         -2.3275e-02, -4.9466e-02, -1.9524e-02,  3.0631e-02, -6.2779e-02,\n",
      "         -2.4717e-02, -4.3218e-02, -8.6433e-03, -6.6615e-02, -1.3659e-02,\n",
      "         -1.0428e-02,  7.9067e-03, -1.6495e-02,  1.9768e-02,  8.7054e-02,\n",
      "         -4.5471e-02, -5.2735e-02,  8.9963e-02, -3.0825e-02, -8.5412e-02,\n",
      "         -7.3848e-03, -6.8652e-03,  7.8440e-04, -8.4300e-02, -2.2432e-04,\n",
      "          3.9016e-02,  3.8208e-02, -1.3212e-02, -9.6551e-02, -7.6377e-03,\n",
      "          6.3379e-02,  4.3077e-02,  5.0849e-03, -1.0924e-02,  2.7003e-02,\n",
      "         -4.3997e-02, -8.5652e-02, -1.6156e-02,  1.4486e-02,  9.2139e-02,\n",
      "         -4.3074e-02,  2.7177e-03, -1.9480e-02, -4.7431e-03,  4.0139e-02,\n",
      "          5.9612e-02,  1.9229e-02,  9.7130e-03,  2.8964e-02,  1.6016e-02,\n",
      "         -5.5631e-02,  2.8030e-02, -1.0520e-01,  8.3067e-02,  3.3751e-02,\n",
      "         -2.3561e-02, -2.2560e-02, -3.3263e-02,  2.4910e-02,  5.1918e-02,\n",
      "         -1.0057e-01,  4.1780e-02, -1.6557e-02, -3.0606e-04, -2.4004e-02,\n",
      "          3.2636e-02,  6.5446e-03, -3.9897e-02, -2.3329e-03, -6.3263e-02,\n",
      "         -2.9539e-02,  4.2026e-02,  2.2300e-02,  2.9591e-02,  2.4146e-05,\n",
      "          2.1892e-02,  6.0512e-03, -3.1015e-02,  8.7842e-02, -9.2946e-02,\n",
      "         -3.4313e-02,  6.6692e-02, -5.4344e-02,  2.9061e-02, -1.5596e-03,\n",
      "          7.6721e-02, -7.3595e-03, -2.1551e-02,  7.2798e-02,  7.7611e-02,\n",
      "          4.7279e-03,  4.9162e-02, -1.1717e-02, -6.4265e-03, -1.0378e-02,\n",
      "         -1.2828e-01,  1.4549e-02, -1.4379e-01, -2.7588e-02, -2.2150e-02,\n",
      "          5.1691e-02, -7.9051e-03,  7.4918e-03,  6.4795e-02,  3.9391e-02,\n",
      "          4.4194e-03, -8.7061e-03, -6.0949e-03, -2.2918e-02,  7.0565e-02,\n",
      "         -5.7212e-02,  4.2042e-02, -6.4800e-02, -1.5028e-03, -4.3662e-02,\n",
      "          3.9934e-02,  1.9493e-02,  2.9680e-02, -2.9662e-02,  3.0825e-02,\n",
      "         -7.7909e-03, -8.3596e-02, -1.8915e-02,  1.1806e-02,  6.2961e-03,\n",
      "         -1.1398e-02, -3.3185e-02,  3.7727e-02, -7.1126e-03,  5.7319e-02,\n",
      "         -2.7922e-02, -6.9374e-02, -5.8976e-02,  1.4406e-02,  1.0970e-03,\n",
      "         -8.4972e-02,  9.8794e-02,  4.7461e-02,  1.0992e-02, -4.5207e-02,\n",
      "          3.5327e-02,  3.5904e-02, -3.3526e-02, -6.3617e-03, -1.0285e-02,\n",
      "         -1.4142e-02, -7.0784e-04,  2.4835e-02,  6.0888e-03,  3.0476e-03,\n",
      "         -4.1060e-02, -5.0792e-02, -3.5178e-02,  4.9996e-02, -2.5364e-02,\n",
      "         -3.5496e-02,  2.9208e-02,  7.8107e-02,  1.5955e-02, -5.5269e-03,\n",
      "          3.7222e-02,  5.7624e-02, -1.2857e-02,  8.0081e-02,  1.0813e-02,\n",
      "         -5.0271e-02, -4.4920e-02,  8.3967e-02, -1.1500e-02, -5.3247e-02,\n",
      "         -1.5953e-02,  2.8709e-02,  3.6273e-02, -3.5956e-02, -6.8124e-03,\n",
      "         -1.5452e-01, -4.2746e-03, -6.6955e-03, -9.7698e-03,  5.4447e-02,\n",
      "         -4.0409e-02, -2.0801e-02,  4.3022e-03, -1.1886e-01,  3.2656e-02,\n",
      "          1.4407e-02,  3.6995e-02,  5.5224e-02,  1.4208e-02, -1.4902e-02,\n",
      "         -1.4303e-02, -4.1210e-03, -4.0368e-02, -1.5406e-03,  5.9018e-02,\n",
      "          4.2931e-02, -8.4432e-02,  6.4384e-02,  4.0654e-02, -5.2551e-02,\n",
      "          4.0584e-02,  3.8720e-02, -3.9281e-02,  1.2372e-02,  3.9700e-02,\n",
      "          8.0027e-02,  2.6821e-02, -1.9108e-02,  2.4802e-02,  4.6784e-02,\n",
      "          4.8544e-02,  5.9828e-02, -3.0448e-02, -3.3851e-02, -3.1425e-02,\n",
      "          1.0603e-03,  1.0590e-02,  3.2583e-02, -6.4561e-02, -1.5488e-02,\n",
      "          2.2881e-02,  1.4557e-02,  3.9918e-02,  9.2763e-03,  5.3985e-02,\n",
      "         -7.0923e-03,  3.2856e-02, -4.1770e-02, -7.0681e-03,  5.4192e-02,\n",
      "         -1.4444e-02,  1.3554e-02,  8.4149e-03, -3.5227e-02, -2.6975e-02,\n",
      "         -2.4103e-03,  1.4995e-02,  7.6949e-02,  2.6154e-02,  6.0998e-02,\n",
      "          2.7897e-02, -3.7343e-03,  5.2487e-02, -2.6789e-02,  1.5669e-02,\n",
      "         -8.3542e-02,  1.1544e-02, -1.4799e-02, -3.5185e-02,  1.0591e-01,\n",
      "         -5.4206e-02, -1.9482e-02, -3.6827e-02,  2.4919e-02, -7.5181e-03,\n",
      "         -6.1068e-02, -3.8746e-03, -1.3660e-02, -2.6443e-02, -2.4010e-02,\n",
      "         -9.4607e-02,  2.1147e-02,  4.6002e-02,  6.0416e-02,  1.5026e-02,\n",
      "         -2.2066e-02, -4.5045e-02,  2.5388e-02,  3.6213e-02,  3.9958e-03,\n",
      "         -3.5884e-02, -4.6157e-03, -2.6967e-02,  6.0196e-02, -6.0987e-03,\n",
      "          4.3176e-02,  1.8149e-02, -1.0015e-01,  5.0533e-03, -3.8120e-02,\n",
      "          3.5905e-02,  1.8766e-02, -5.2932e-02, -7.4213e-03,  4.4974e-02,\n",
      "          1.4077e-02, -5.9715e-02,  3.1880e-03,  1.1625e-02, -4.7069e-02,\n",
      "         -1.4026e-03, -1.7309e-03, -5.5426e-02, -6.7498e-02,  1.0004e-02,\n",
      "         -5.4686e-03,  5.9375e-02, -3.3797e-02, -6.1371e-02,  7.9279e-03,\n",
      "         -8.7456e-02,  8.9973e-02,  5.8201e-02,  1.0152e-01,  3.5823e-02,\n",
      "         -8.6274e-02,  3.7567e-02, -3.0503e-02,  9.0181e-03, -5.4347e-02,\n",
      "         -1.7719e-02,  2.4833e-02, -8.8869e-02, -5.7361e-03, -1.6634e-02,\n",
      "         -5.0742e-03, -1.4417e-02,  5.6415e-02, -4.9187e-02, -6.1749e-02,\n",
      "         -4.7249e-03,  1.0451e-02,  2.5092e-04,  1.3350e-02,  3.6982e-02,\n",
      "         -7.1729e-02, -1.2553e-02, -1.6859e-02, -7.5121e-04, -3.5693e-02,\n",
      "         -4.6178e-02, -2.4736e-02, -5.0790e-02, -1.7972e-02, -3.3507e-02,\n",
      "         -1.3866e-03,  6.4131e-02, -6.6691e-02,  2.7623e-02, -2.9220e-02,\n",
      "         -2.2939e-02, -3.2290e-02,  6.6230e-02,  2.0847e-02, -5.8501e-02,\n",
      "         -3.2054e-02, -9.4430e-03,  7.6174e-04,  8.6336e-03, -7.9367e-02,\n",
      "          3.7145e-02, -8.4834e-03, -4.3270e-02,  4.3111e-02, -2.0881e-02,\n",
      "          1.0543e-01,  2.4254e-02, -2.3742e-02,  1.7277e-02,  4.8544e-03,\n",
      "         -3.2068e-03, -6.7643e-02, -1.5047e-03, -1.9808e-02, -8.6604e-02,\n",
      "         -1.8048e-02, -5.7407e-02, -5.1702e-02,  4.8093e-02,  5.5721e-02,\n",
      "         -7.8507e-02,  3.9658e-02,  5.6261e-02, -8.8978e-02,  3.3652e-02,\n",
      "         -5.6281e-02, -3.5457e-03, -1.3009e-02, -3.3590e-02,  7.1154e-02,\n",
      "          7.3798e-02,  1.1470e-01, -1.7917e-04, -5.1902e-02, -1.9628e-02,\n",
      "         -6.4685e-02, -1.7427e-02, -5.5690e-02,  1.8429e-02,  1.1290e-01,\n",
      "          8.4172e-04, -3.1807e-02,  6.3905e-02, -4.0652e-02,  2.1887e-02,\n",
      "          4.7383e-02,  2.8816e-02, -2.2300e-02, -2.6464e-02,  3.3556e-02,\n",
      "         -8.7470e-03, -3.4963e-03]], device='cuda:0', grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for data in data_loader:\n",
    "    data = data[0]\n",
    "\n",
    "enc_f = encode_structure(example).to(device)\n",
    "\n",
    "print(\"encodeado sin batch\", enc_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0396, -0.0932, -0.1233,  ...,  0.0037,  0.0340, -0.0082],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_fold_nodes[0]-enc_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[8, 5, 5, 5, 6, 5, 13, 8, 5, 10, 5, 5, 5, 7, 5, 5, 5, 6, 8, 8]\n",
      "6.45\n",
      "3.7\n",
      "2.05\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "n_no = []\n",
    "qzero = 0\n",
    "qOne = 0\n",
    "qtwo = 0\n",
    "\n",
    "for batch in data_loader:\n",
    "    for tree in batch:\n",
    "        count = []\n",
    "        n = tree.count_nodes(tree, count)\n",
    "        n_no.append(len(n))\n",
    "        li = []\n",
    "        tree.traverseInorderChilds(tree, li)\n",
    "        zero = [a for a in li if a == 0]\n",
    "        one = [a for a in li if a == 1]\n",
    "        two = [a for a in li if a == 2]\n",
    "        qzero += len(zero)\n",
    "        qOne += len(one)\n",
    "        qtwo += len(two)\n",
    "\n",
    "print(len(data_loader)*batch_size)\n",
    "print(n_no)\n",
    "nprom = np.mean(n_no)\n",
    "print(nprom)\n",
    "qzero /= len(data_loader)*batch_size\n",
    "qOne /= len(data_loader)*batch_size\n",
    "qtwo /= len(data_loader)*batch_size\n",
    "\n",
    "print(qzero)\n",
    "print(qOne)\n",
    "print(qtwo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en el loop creo un fold, mando este fold con cada uno de los arboles del batch a encode_structure_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass NodeClassifier(nn.Module):\\n    \\n    def __init__(self, latent_size : int, hidden_size : int):\\n        super(NodeClassifier, self).__init__()\\n        self.mlp1 = nn.Linear(latent_size, hidden_size)\\n        self.mlp2 = nn.Linear(hidden_size, hidden_size)\\n        self.mlp3 = nn.Linear(hidden_size, 3)\\n        self.tanh = nn.Tanh()\\n\\n    def forward(self, input_feature):\\n        #print(\"classifier input\", input_feature)\\n        output = self.mlp1(input_feature)\\n        output = self.tanh(output)\\n        output = self.mlp2(output)\\n        output = self.tanh(output)\\n        output = self.mlp3(output)\\n        #print(\"classifier output\", output)\\n\\n        return output\\n'"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class NodeClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.mlp1 = nn.Linear(latent_size, hidden_size)\n",
    "        self.mlp2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.mlp3 = nn.Linear(hidden_size, 3)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_feature):\n",
    "        #print(\"classifier input\", input_feature)\n",
    "        output = self.mlp1(input_feature)\n",
    "        output = self.tanh(output)\n",
    "        output = self.mlp2(output)\n",
    "        output = self.tanh(output)\n",
    "        output = self.mlp3(output)\n",
    "        #print(\"classifier output\", output)\n",
    "\n",
    "        return output\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelo import GRASSDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(Decoder, self).__init__()\n",
    "        #self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.lp3 = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "        self.mlp_left = nn.Linear(latent_size, hidden_size)\n",
    "        self.mlp_left2 = nn.Linear(hidden_size, latent_size)\n",
    "        #self.mlp_left3 = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, hidden_size)\n",
    "        self.mlp_right2 = nn.Linear(hidden_size, latent_size)\n",
    "        #self.mlp_right3 = nn.Linear(latent_size, latent_size)\n",
    "\n",
    "\n",
    "        self.mlp2 = nn.Linear(latent_size,latent_size)\n",
    "        self.mlp3 = nn.Linear(latent_size,4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def common_branch(self, parent_feature):\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.lp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.lp3(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        return vector\n",
    "\n",
    "    def attr_branch(self, vector):\n",
    "        vector = self.mlp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp3(vector)        \n",
    "        return vector\n",
    "\n",
    "    def right_branch(self, vector):\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        right_feature = self.mlp_right2(right_feature)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        #right_feature = self.mlp_right3(right_feature)\n",
    "        #right_feature = self.tanh(right_feature)\n",
    "        return right_feature\n",
    "\n",
    "    def left_branch(self, vector):\n",
    "        left_feature = self.mlp_left(vector)\n",
    "        left_feature = self.tanh(left_feature)\n",
    "        left_feature = self.mlp_left2(left_feature)\n",
    "        left_feature = self.tanh(left_feature)\n",
    "        #left_feature = self.mlp_left3(left_feature)\n",
    "        #left_feature = self.tanh(left_feature)\n",
    "        return left_feature\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "      \n",
    "        vector      = self.common_branch(parent_feature)\n",
    "        attr_vector = self.attr_branch(vector)\n",
    "        return attr_vector \n",
    "\n",
    "    def forward1(self, parent_feature):\n",
    "    \n",
    "\n",
    "        vector       = self.common_branch(parent_feature)\n",
    "        attr_vector  = self.attr_branch(vector)\n",
    "        right_vector = self.right_branch(vector)\n",
    "        \n",
    "        #print(\"right vector\", right_vector)\n",
    "        #print(\"radius\", attr_vector)\n",
    "        return right_vector, attr_vector\n",
    "\n",
    "    def forward2(self, parent_feature):\n",
    "       \n",
    "\n",
    "        vector       = self.common_branch(parent_feature)\n",
    "        attr_vector  = self.attr_branch(vector)\n",
    "        right_vector = self.right_branch(vector)\n",
    "        left_vector  = self.left_branch(vector)\n",
    "        #print(\"left vector\", left_vector)\n",
    "        #print(\"right vector\", right_vector)\n",
    "        #print(\"radius\", attr_vector)\n",
    "        return left_vector, right_vector, attr_vector\n",
    "\n",
    "\n",
    "\n",
    "class GRASSDecoder(nn.Module):\n",
    "    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\n",
    "        super(GRASSDecoder, self).__init__()\n",
    "        self.decoder = Decoder(latent_size, hidden_size)\n",
    "        self.node_classifier = NodeClassifier(latent_size, hidden_size)\n",
    "        self.mseLoss = nn.MSELoss()  # pytorch's mean squared error loss\n",
    "        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch's cross entropy loss (NOTE: no softmax is needed before)\n",
    "        \n",
    "\n",
    "\n",
    "    def featureDecoder(self, feature):\n",
    "        return self.decoder.forward(feature)\n",
    "\n",
    "    def internalDecoder(self, feature):\n",
    "        return self.decoder.forward1(feature)\n",
    "\n",
    "    def bifurcationDecoder(self, feature):\n",
    "        return self.decoder.forward2(feature)\n",
    "\n",
    "    def nodeClassifier(self, feature):\n",
    "        return self.node_classifier(feature)\n",
    "\n",
    "    def calcularLossAtributo(self, nodo, radio):\n",
    "        #print(\"nodo\", nodo)\n",
    "        #print(\"radio\", radio)\n",
    "        a, b = list(zip(*nodo))# a son los atributos, b los pesos\n",
    "        if nodo is None:\n",
    "            return\n",
    "        else:\n",
    "            nodo = torch.stack(list(a))\n",
    "        \n",
    "            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\n",
    "            #print(\"mse\", l)\n",
    "            return l\n",
    "\n",
    "\n",
    "    def classifyLossEstimator(self, label_vector, original):\n",
    "        if original is None:\n",
    "            return\n",
    "        else:\n",
    "           \n",
    "            v = []\n",
    "            for o in original:\n",
    "                if o == 0:\n",
    "                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\n",
    "                if o == 1:\n",
    "                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\n",
    "                if o == 2:\n",
    "                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\n",
    "                v.append(vector)\n",
    "            \n",
    "\n",
    "            v = torch.stack(v)\n",
    "            \n",
    "            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\n",
    "         \n",
    "\n",
    "            return l\n",
    "            #return c\n",
    "\n",
    "    def vectorAdder(self, v1, v2):\n",
    "        v = v1.add(v2)\n",
    "        return v\n",
    "\n",
    "    def vectorMult(self, m, v):\n",
    "        #print(\"v\", v)\n",
    "        #print(\"m\", m)\n",
    "        z = zip(v, m)\n",
    "        r = []\n",
    "        for c, d in z:\n",
    "            #print(\"v\", c)\n",
    "            #print(\"m\", d)\n",
    "            r.append(torch.mul(c, d))\n",
    "        #res = [torch.mul(v, m) for v, m in zip(v, m)]\n",
    "        #print(\"res\", r)\n",
    "        return r\n",
    "'''\n",
    "if round(qzero) == 0:\n",
    "    qzero = 1\n",
    "if round(qOne) == 0:\n",
    "    qOne = 1\n",
    "if round(qtwo) == 0:\n",
    "    qtwo = 1\n",
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "\n",
    "Grassdecoder = GRASSDecoder(latent_size=512, hidden_size=1024, mult = mult)\n",
    "Grassdecoder = Grassdecoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7\n",
      "2.05\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "print(qzero)\n",
    "print(qOne)\n",
    "print(qtwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2500, 0.5000, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(mult)\n",
    "def calcularLossEstructura(cl_p, original):\n",
    "    \n",
    "    if original is None:\n",
    "        return\n",
    "    #mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "    ce = nn.CrossEntropyLoss(weight = mult)\n",
    "\n",
    "    if original.childs() == 0:\n",
    "        vector = [1, 0, 0] \n",
    "    if original.childs() == 1:\n",
    "        vector = [0, 1, 0]\n",
    "    if original.childs() == 2:\n",
    "        vector = [0, 0, 1] \n",
    "\n",
    "\n",
    "    c = ce(cl_p, torch.tensor(vector, device=device, dtype = torch.float).reshape(1, 3))\n",
    "    #print(\"original\", vector)\n",
    "    #print(\"clasificador\", cl_p)\n",
    "    #print(\"ce\", 0.4*c)\n",
    "    return c\n",
    "\n",
    "\n",
    "def calcularLossAtributo(nodo, radio):\n",
    "    if nodo is None:\n",
    "        return\n",
    "    #print(\"nodo\", nodo)\n",
    "    #print(\"radio\", radio)\n",
    "\n",
    "    radio = radio.reshape(-1,4)\n",
    "    nodo = nodo.radius.reshape(-1,4)\n",
    "    l2    = nn.MSELoss()\n",
    "   \n",
    "    mse = l2(radio, nodo)\n",
    "    #print(\"mse\", mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchNode(node, key):\n",
    "     \n",
    "    if (node == None):\n",
    "        return False\n",
    " \n",
    "    if (node.data == key):\n",
    "        return node\n",
    "        \n",
    " \n",
    "    \"\"\" then recur on left subtree \"\"\"\n",
    "    res1 = searchNode(node.left, key)\n",
    "    # node found, no need to look further\n",
    "    if res1:\n",
    "        return res1\n",
    " \n",
    "    \"\"\" node is not found in left,\n",
    "    so recur on right subtree \"\"\"\n",
    "    res2 = searchNode(node.right, key)\n",
    "    return res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_nodes 13\n"
     ]
    }
   ],
   "source": [
    "def getLevelUtil(node, data, level):\n",
    "    if (node == None):\n",
    "        return 0\n",
    " \n",
    "    if (node.data == data):\n",
    "        return level\n",
    " \n",
    "    downlevel = getLevelUtil(node.left, data, level + 1)\n",
    "\n",
    "    if (downlevel != 0):\n",
    "        return downlevel\n",
    " \n",
    "    downlevel = getLevelUtil(node.right, data, level + 1)\n",
    "    return downlevel\n",
    " \n",
    "# Returns level of given data value\n",
    " \n",
    " \n",
    "def getLevel(node, data):\n",
    "    return getLevelUtil(node, data, 1)\n",
    " \n",
    "\n",
    "c = []\n",
    "n_nodes = input.count_nodes(input, c)\n",
    "print(\"n_nodes\", len(n_nodes))\n",
    "for x in range(0, len(n_nodes)):\n",
    "        level = getLevel(input, x)\n",
    "        if (level):\n",
    "            #print(\"Level of\", x, \"is\", getLevel(input, x))\n",
    "            node = searchNode(input, x)\n",
    "            node.level = getLevel(input, x)\n",
    "        else:\n",
    "            print(x, \"is not present in tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Node object at 0x000001EAAB726DD0>\n",
      "tree level 46\n",
      "<__main__.Node object at 0x000001EAAB7265C0>\n",
      "tree level 27\n",
      "<__main__.Node object at 0x000001EAAB726950>\n",
      "tree level 36\n",
      "<__main__.Node object at 0x000001EAAB726200>\n",
      "tree level 20\n",
      "<__main__.Node object at 0x000001EAAB727E20>\n",
      "tree level 27\n",
      "<__main__.Node object at 0x000001EAAB7278B0>\n",
      "tree level 15\n",
      "<__main__.Node object at 0x000001EAAB727640>\n",
      "tree level 15\n",
      "<__main__.Node object at 0x000001EAAB725300>\n",
      "tree level 15\n",
      "<__main__.Node object at 0x000001EAAB726EF0>\n",
      "tree level 15\n",
      "<__main__.Node object at 0x000001EAAB724C70>\n",
      "tree level 15\n",
      "<__main__.Node object at 0x000001EAAB726560>\n",
      "tree level 15\n",
      "<__main__.Node object at 0x000001EAAB726050>\n",
      "tree level 15\n",
      "<__main__.Node object at 0x000001EAAB724160>\n",
      "tree level 20\n",
      "<__main__.Node object at 0x000001EAAB7249D0>\n",
      "tree level 29\n",
      "<__main__.Node object at 0x000001EAAB726E00>\n",
      "tree level 15\n",
      "<__main__.Node object at 0x000001EAAB726350>\n",
      "tree level 15\n",
      "<__main__.Node object at 0x000001EAAB7252A0>\n",
      "tree level 24\n",
      "<__main__.Node object at 0x000001EAAB725B70>\n",
      "tree level 15\n",
      "<__main__.Node object at 0x000001EAAB727790>\n",
      "tree level 29\n",
      "<__main__.Node object at 0x000001EAAB724AF0>\n",
      "tree level 15\n"
     ]
    }
   ],
   "source": [
    "for d in data_loader:\n",
    "    for data in d:\n",
    "        print(data)\n",
    "        count = []\n",
    "        numerar_nodos(data, count)\n",
    "        c = []\n",
    "        n_nodes = data.count_nodes(data, c)\n",
    "        for x in range(0, len(n_nodes)):\n",
    "            level = getLevel(data, x)\n",
    "            if (level):\n",
    "                #print(\"Level of\", x, \"is\", getLevel(input, x))\n",
    "                node = searchNode(data, x)\n",
    "                node.level = getLevel(data, x)\n",
    "            else:\n",
    "                print(x, \"is not present in tree\")\n",
    "        tree_level = []\n",
    "        data.get_tree_level(data, tree_level)\n",
    "        print(\"tree level\", sum(tree_level))\n",
    "        data.set_tree_level(data, sum(tree_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_structure_fold_grass(fold, v, root):\n",
    "   \n",
    "    def decode_node(fold, v, node):\n",
    "        \n",
    "        \n",
    "        if node.childs() == 0 : \n",
    "\n",
    "            radio = fold.add('featureDecoder', v)\n",
    "            lossAtributo = fold.add('calcularLossAtributo', node, radio)\n",
    "\n",
    "            label = fold.add('nodeClassifier', v)\n",
    "            \n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)  \n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            \n",
    "            loss =  fold.add('vectorAdder', losse, lossAtributo)       \n",
    "            return loss\n",
    "\n",
    "            \n",
    "            \n",
    "        elif node.childs() == 1 :\n",
    "            right, radius = fold.add('internalDecoder', v).split(2)\n",
    "            label = fold.add('nodeClassifier', v)\n",
    "            nodoSiguiente = node.right\n",
    "            if nodoSiguiente is not None:\n",
    "                right_loss = decode_node(fold, right, nodoSiguiente)\n",
    "\n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)\n",
    "            lossAtributo = fold.add('calcularLossAtributo', node, radius)\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            loss = fold.add('vectorAdder', losse, lossAtributo)\n",
    "            \n",
    "        \n",
    "            loss2 = fold.add('vectorAdder', loss, right_loss)\n",
    "            return loss2\n",
    "            \n",
    "            \n",
    "\n",
    "        elif node.childs() == 2 :\n",
    "            left, right, radius = fold.add('bifurcationDecoder', v).split(3)\n",
    "            \n",
    "            label = fold.add('nodeClassifier', v)            \n",
    "            \n",
    "            nodoSiguienteRight = node.right\n",
    "            nodoSiguienteLeft = node.left\n",
    "\n",
    "\n",
    "            if nodoSiguienteRight is not None:\n",
    "                right_loss = decode_node(fold, right, nodoSiguienteRight)\n",
    "             \n",
    "            if nodoSiguienteLeft is not None:\n",
    "                left_loss  = decode_node(fold, left, nodoSiguienteLeft)\n",
    "\n",
    "            multipl = node.level/node.treelevel\n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            lossAtributo   = fold.add('calcularLossAtributo', node, radius)\n",
    "            loss = fold.add('vectorAdder', losse, lossAtributo)\n",
    "            loss2 = fold.add('vectorAdder', loss, right_loss)\n",
    "            loss3 = fold.add('vectorAdder', loss2, left_loss)\n",
    "            \n",
    "            return loss3\n",
    "            \n",
    "\n",
    "    dec = decode_node (fold, v, root)\n",
    "    return dec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_testing_grass(v, root, max, decoder):\n",
    "    def decode_node(v, node, max, decoder):\n",
    "        cl = decoder.nodeClassifier(v)\n",
    "        _, label = torch.max(cl, 1)\n",
    "        label = label.data\n",
    "        \n",
    "        #print(\"clasificador: \", label)\n",
    "        #if node is not None:\n",
    "        #    print(\"original:\", node.childs())\n",
    "        #else:\n",
    "        #    print(\"nodo en lugar incorrecto\")\n",
    "        if label == 0 and createNode.count <= max: ##output del classifier\n",
    "           \n",
    "            radio = decoder.featureDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node, radio )\n",
    "            if lossEstructura is not None:\n",
    "                multipl = node.level/node.treelevel\n",
    "                lossEstructura = multipl*lossEstructura\n",
    "            \n",
    "            return createNode(1,radio, ce = lossEstructura,  mse = lossAtrs)\n",
    "            #return createNode(1,radio)\n",
    "\n",
    "        elif label == 1 and createNode.count <= max:\n",
    "       \n",
    "            right, radius = decoder.internalDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node, radius )\n",
    "            if lossEstructura is not None:\n",
    "                multipl = node.level/node.treelevel\n",
    "                lossEstructura = multipl*lossEstructura\n",
    "            d = createNode(1, radius, ce = lossEstructura,  mse = lossAtrs) \n",
    "            #d = createNode(1, radius) \n",
    "\n",
    "           \n",
    "            if not node is None:\n",
    "                if not node.right is None:\n",
    "                    nodoSiguiente = node.right\n",
    "                else:\n",
    "                    nodoSiguiente = None\n",
    "            else:\n",
    "                nodoSiguiente = None\n",
    "            \n",
    "            d.right = decode_node(right, nodoSiguiente, max, decoder)\n",
    "            \n",
    "\n",
    "            return d\n",
    "       \n",
    "        elif label == 2 and createNode.count <= max:\n",
    "            left, right, radius = decoder.bifurcationDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node, radius )\n",
    "            if lossEstructura is not None:\n",
    "                multipl = node.level/node.treelevel\n",
    "                lossEstructura = multipl*lossEstructura\n",
    "            \n",
    "            d = createNode(1, radius, ce = lossEstructura,  mse = lossAtrs )\n",
    "            #d = createNode(1, radius )\n",
    "  \n",
    "            if not node is None: #el nodo existe, me fijo si tiene hijo der/izq\n",
    "                if not node.right is None:\n",
    "                    nodoSiguienteRight = node.right\n",
    "                else:\n",
    "                    nodoSiguienteRight = None\n",
    "                if not node.left is None:\n",
    "                    nodoSiguienteLeft = node.left\n",
    "                else:\n",
    "                    nodoSiguienteLeft = None\n",
    "            else: #el nodo no existe\n",
    "                nodoSiguienteRight = None\n",
    "                nodoSiguienteLeft = None\n",
    "            \n",
    "            d.right = decode_node(right, nodoSiguienteRight, max, decoder)\n",
    "            d.left = decode_node(left, nodoSiguienteLeft, max, decoder)\n",
    "            \n",
    "           \n",
    "            return d\n",
    "            \n",
    "    createNode.count = 0\n",
    "    dec = decode_node (v, root, max, decoder)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, encoder, decoder, optimizer\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            #print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            #print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            #'classifier_state_dict': classifier.state_dict(),\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                \n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, 'outputs/best_model.pth')\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_structure_fold_(v, root):\n",
    "    \n",
    "    def decode_node(v, node):\n",
    "        cl = Grassdecoder.nodeClassifier(v)\n",
    "        _, label = torch.max(cl, 1)\n",
    "        label = label.data\n",
    "\n",
    "        \n",
    "        if node.childs() == 0 : ##output del classifier\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            radio = Grassdecoder.featureDecoder(v)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radio )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "            nd = createNode(1,radio, ce = losse,  mse = lossAtrs)\n",
    "            return nd\n",
    "\n",
    "        elif node.childs() == 1 :\n",
    "        \n",
    "            right, radius = Grassdecoder.internalDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radius )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "            nd = createNode(1, radius, cl_prob = lossAtrs , ce = losse, mse = lossAtrs) \n",
    "            \n",
    "            nodoSiguiente = node.right\n",
    "           \n",
    "            if nodoSiguiente is not None:\n",
    "                nd.right = decode_node(right, nodoSiguiente)\n",
    "               \n",
    "            return nd\n",
    "\n",
    "        elif node.childs() == 2 :\n",
    "            left, right, radius = Grassdecoder.bifurcationDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radius )\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = multipl*lossEstructura\n",
    "            nd = createNode(1, radius, cl_prob = lossAtrs, ce = losse, mse = lossAtrs)\n",
    "            \n",
    "            nodoSiguienteRight = node.right\n",
    "            nodoSiguienteLeft = node.left\n",
    "\n",
    "            \n",
    "            if nodoSiguienteRight is not None:\n",
    "                nd.right = decode_node(right, nodoSiguienteRight)\n",
    "             \n",
    "            if nodoSiguienteLeft is not None:\n",
    "                nd.left  = decode_node(left, nodoSiguienteLeft)\n",
    "            \n",
    "            return nd\n",
    "            \n",
    "    createNode.count = 0\n",
    "    dec = decode_node (v, root)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder con batch - decoder con batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6zefzx1d) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d824c0e7b24de997d1529c48e635da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>loss</td><td>â–ˆâ–„â–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5912</td></tr><tr><td>loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">worldly-morning-35</strong>: <a href=\"https://wandb.ai/paufeldman/autoencoder4/runs/6zefzx1d\" target=\"_blank\">https://wandb.ai/paufeldman/autoencoder4/runs/6zefzx1d</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221205_160823-6zefzx1d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6zefzx1d). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da50d0622def47fc81cbb31af0b6cf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\wandb\\run-20221205_161751-3u6b2fb5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/paufeldman/autoencoder4/runs/3u6b2fb5\" target=\"_blank\">prime-plasma-36</a></strong> to <a href=\"https://wandb.ai/paufeldman/autoencoder4\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/paufeldman/autoencoder4/runs/3u6b2fb5?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1ea5fe6e5f0>"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 8000\n",
    "learning_rate = 1e-5\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "#opt = torch.optim.Adam(params, lr=learning_rate, weight_decay=0.0001) \n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "#opt = torch.optim.SGD(params, lr=learning_rate, momentum = 0.96) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[100], gamma=0.2)\n",
    "import wandb\n",
    "config = {\n",
    "  \"learning_rate\": learning_rate,\n",
    "  \"epochs\": epochs,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"dataset\": t_list,\n",
    "  \"number of trees\": len(data_loader)*batch_size,\n",
    "  \"optim\": opt\n",
    "}\n",
    "wandb.init(project=\"autoencoder4\", entity=\"paufeldman\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 8000] average reconstruction error: 0.0625236407 \n",
      "Epoch [11 / 8000] average reconstruction error: 0.0878621563 \n",
      "Epoch [21 / 8000] average reconstruction error: 0.0448194221 \n",
      "Epoch [31 / 8000] average reconstruction error: 0.0555612035 \n",
      "Epoch [41 / 8000] average reconstruction error: 0.0117492992 \n",
      "Epoch [51 / 8000] average reconstruction error: 0.0065610069 \n",
      "Epoch [61 / 8000] average reconstruction error: 0.0074653635 \n",
      "Epoch [71 / 8000] average reconstruction error: 0.0049217790 \n",
      "Epoch [81 / 8000] average reconstruction error: 0.0077782795 \n",
      "Epoch [91 / 8000] average reconstruction error: 0.0084526911 \n",
      "Epoch [101 / 8000] average reconstruction error: 0.0034915321 \n",
      "Epoch [111 / 8000] average reconstruction error: 0.0041958913 \n",
      "Epoch [121 / 8000] average reconstruction error: 0.0075210794 \n",
      "Epoch [131 / 8000] average reconstruction error: 0.0074717076 \n",
      "Epoch [141 / 8000] average reconstruction error: 0.0049878936 \n",
      "Epoch [151 / 8000] average reconstruction error: 0.0052040545 \n",
      "Epoch [161 / 8000] average reconstruction error: 0.0089593297 \n",
      "Epoch [171 / 8000] average reconstruction error: 0.0024613135 \n",
      "Epoch [181 / 8000] average reconstruction error: 0.0061665024 \n",
      "Epoch [191 / 8000] average reconstruction error: 0.0069596367 \n",
      "Epoch [201 / 8000] average reconstruction error: 0.0069882460 \n",
      "Epoch [211 / 8000] average reconstruction error: 0.0062044417 \n",
      "Epoch [221 / 8000] average reconstruction error: 0.0071135969 \n",
      "Epoch [231 / 8000] average reconstruction error: 0.0054211537 \n",
      "Epoch [241 / 8000] average reconstruction error: 0.0046967855 \n",
      "Epoch [251 / 8000] average reconstruction error: 0.0029881124 \n",
      "Epoch [261 / 8000] average reconstruction error: 0.0027265975 \n",
      "Epoch [271 / 8000] average reconstruction error: 0.0039488254 \n",
      "Epoch [281 / 8000] average reconstruction error: 0.0050646495 \n",
      "Epoch [291 / 8000] average reconstruction error: 0.0010397112 \n",
      "Epoch [301 / 8000] average reconstruction error: 0.0015986480 \n",
      "Epoch [311 / 8000] average reconstruction error: 0.0029886921 \n",
      "Epoch [321 / 8000] average reconstruction error: 0.0017890725 \n",
      "Epoch [331 / 8000] average reconstruction error: 0.0016209532 \n",
      "Epoch [341 / 8000] average reconstruction error: 0.0006129554 \n",
      "Epoch [351 / 8000] average reconstruction error: 0.0015219301 \n",
      "Epoch [361 / 8000] average reconstruction error: 0.0011224451 \n",
      "Epoch [371 / 8000] average reconstruction error: 0.0009808138 \n",
      "Epoch [381 / 8000] average reconstruction error: 0.0002664659 \n",
      "Epoch [391 / 8000] average reconstruction error: 0.0003387800 \n",
      "Epoch [401 / 8000] average reconstruction error: 0.0003059753 \n",
      "Epoch [411 / 8000] average reconstruction error: 0.0003891407 \n",
      "Epoch [421 / 8000] average reconstruction error: 0.0002584129 \n",
      "Epoch [431 / 8000] average reconstruction error: 0.0002072059 \n",
      "Epoch [441 / 8000] average reconstruction error: 0.0000587874 \n",
      "Epoch [451 / 8000] average reconstruction error: 0.0002983963 \n",
      "Epoch [461 / 8000] average reconstruction error: 0.0003189565 \n",
      "Epoch [471 / 8000] average reconstruction error: 0.0001719379 \n",
      "Epoch [481 / 8000] average reconstruction error: 0.0010429531 \n",
      "Epoch [491 / 8000] average reconstruction error: 0.0002057049 \n",
      "Epoch [501 / 8000] average reconstruction error: 0.0001694145 \n",
      "Epoch [511 / 8000] average reconstruction error: 0.0001533349 \n",
      "Epoch [521 / 8000] average reconstruction error: 0.0001843515 \n",
      "Epoch [531 / 8000] average reconstruction error: 0.0001621772 \n",
      "Epoch [541 / 8000] average reconstruction error: 0.0001019596 \n",
      "Epoch [551 / 8000] average reconstruction error: 0.0001336950 \n",
      "Epoch [561 / 8000] average reconstruction error: 0.0000868437 \n",
      "Epoch [571 / 8000] average reconstruction error: 0.0000959177 \n",
      "Epoch [581 / 8000] average reconstruction error: 0.0001375905 \n",
      "Epoch [591 / 8000] average reconstruction error: 0.0000424080 \n",
      "Epoch [601 / 8000] average reconstruction error: 0.0000380380 \n",
      "Epoch [611 / 8000] average reconstruction error: 0.0001474649 \n",
      "Epoch [621 / 8000] average reconstruction error: 0.0000487940 \n",
      "Epoch [631 / 8000] average reconstruction error: 0.0000745433 \n",
      "Epoch [641 / 8000] average reconstruction error: 0.0001113316 \n",
      "Epoch [651 / 8000] average reconstruction error: 0.0000735953 \n",
      "Epoch [661 / 8000] average reconstruction error: 0.0000936619 \n",
      "Epoch [671 / 8000] average reconstruction error: 0.0000963939 \n",
      "Epoch [681 / 8000] average reconstruction error: 0.0000621756 \n",
      "Epoch [691 / 8000] average reconstruction error: 0.0000900442 \n",
      "Epoch [701 / 8000] average reconstruction error: 0.0001047852 \n",
      "Epoch [711 / 8000] average reconstruction error: 0.0000918581 \n",
      "Epoch [721 / 8000] average reconstruction error: 0.0000329723 \n",
      "Epoch [731 / 8000] average reconstruction error: 0.0001312520 \n",
      "Epoch [741 / 8000] average reconstruction error: 0.0000436720 \n",
      "Epoch [751 / 8000] average reconstruction error: 0.0000587340 \n",
      "Epoch [761 / 8000] average reconstruction error: 0.0000503857 \n",
      "Epoch [771 / 8000] average reconstruction error: 0.0000896656 \n",
      "Epoch [781 / 8000] average reconstruction error: 0.0000883170 \n",
      "Epoch [791 / 8000] average reconstruction error: 0.0000485252 \n",
      "Epoch [801 / 8000] average reconstruction error: 0.0000899254 \n",
      "Epoch [811 / 8000] average reconstruction error: 0.0000561516 \n",
      "Epoch [821 / 8000] average reconstruction error: 0.0000302658 \n",
      "Epoch [831 / 8000] average reconstruction error: 0.0000544940 \n",
      "Epoch [841 / 8000] average reconstruction error: 0.0000601660 \n",
      "Epoch [851 / 8000] average reconstruction error: 0.0000276354 \n",
      "Epoch [861 / 8000] average reconstruction error: 0.0000364745 \n",
      "Epoch [871 / 8000] average reconstruction error: 0.0000458086 \n",
      "Epoch [881 / 8000] average reconstruction error: 0.0000293482 \n",
      "Epoch [891 / 8000] average reconstruction error: 0.0000191975 \n",
      "Epoch [901 / 8000] average reconstruction error: 0.0000206676 \n",
      "Epoch [911 / 8000] average reconstruction error: 0.0000300049 \n",
      "Epoch [921 / 8000] average reconstruction error: 0.0000210009 \n",
      "Epoch [931 / 8000] average reconstruction error: 0.0000423354 \n",
      "Epoch [941 / 8000] average reconstruction error: 0.0000184536 \n",
      "Epoch [951 / 8000] average reconstruction error: 0.0000348616 \n",
      "Epoch [961 / 8000] average reconstruction error: 0.0002211740 \n",
      "Epoch [971 / 8000] average reconstruction error: 0.0000428125 \n",
      "Epoch [981 / 8000] average reconstruction error: 0.0000322701 \n",
      "Epoch [991 / 8000] average reconstruction error: 0.0000363774 \n",
      "Epoch [1001 / 8000] average reconstruction error: 0.0000267397 \n",
      "Epoch [1011 / 8000] average reconstruction error: 0.0000248430 \n",
      "Epoch [1021 / 8000] average reconstruction error: 0.0000201800 \n",
      "Epoch [1031 / 8000] average reconstruction error: 0.0000318868 \n",
      "Epoch [1041 / 8000] average reconstruction error: 0.0000177178 \n",
      "Epoch [1051 / 8000] average reconstruction error: 0.0000256103 \n",
      "Epoch [1061 / 8000] average reconstruction error: 0.0000141152 \n",
      "Epoch [1071 / 8000] average reconstruction error: 0.0000214596 \n",
      "Epoch [1081 / 8000] average reconstruction error: 0.0001020960 \n",
      "Epoch [1091 / 8000] average reconstruction error: 0.0000186798 \n",
      "Epoch [1101 / 8000] average reconstruction error: 0.0000279756 \n",
      "Epoch [1111 / 8000] average reconstruction error: 0.0000131215 \n",
      "Epoch [1121 / 8000] average reconstruction error: 0.0000112255 \n",
      "Epoch [1131 / 8000] average reconstruction error: 0.0000286153 \n",
      "Epoch [1141 / 8000] average reconstruction error: 0.0000149015 \n",
      "Epoch [1151 / 8000] average reconstruction error: 0.0000223522 \n",
      "Epoch [1161 / 8000] average reconstruction error: 0.0000189179 \n",
      "Epoch [1171 / 8000] average reconstruction error: 0.0000923936 \n",
      "Epoch [1181 / 8000] average reconstruction error: 0.0005856763 \n",
      "Epoch [1191 / 8000] average reconstruction error: 0.0000762652 \n",
      "Epoch [1201 / 8000] average reconstruction error: 0.0000606883 \n",
      "Epoch [1211 / 8000] average reconstruction error: 0.0000251536 \n",
      "Epoch [1221 / 8000] average reconstruction error: 0.0000162117 \n",
      "Epoch [1231 / 8000] average reconstruction error: 0.0000221603 \n",
      "Epoch [1241 / 8000] average reconstruction error: 0.0000160427 \n",
      "Epoch [1251 / 8000] average reconstruction error: 0.0000126107 \n",
      "Epoch [1261 / 8000] average reconstruction error: 0.0000116514 \n",
      "Epoch [1271 / 8000] average reconstruction error: 0.0000087892 \n",
      "Epoch [1281 / 8000] average reconstruction error: 0.0000137822 \n",
      "Epoch [1291 / 8000] average reconstruction error: 0.0000108143 \n",
      "Epoch [1301 / 8000] average reconstruction error: 0.0000145790 \n",
      "Epoch [1311 / 8000] average reconstruction error: 0.0000122724 \n",
      "Epoch [1321 / 8000] average reconstruction error: 0.0000101789 \n",
      "Epoch [1331 / 8000] average reconstruction error: 0.0000146209 \n",
      "Epoch [1341 / 8000] average reconstruction error: 0.0000044167 \n",
      "Epoch [1351 / 8000] average reconstruction error: 0.0000115314 \n",
      "Epoch [1361 / 8000] average reconstruction error: 0.0000080596 \n",
      "Epoch [1371 / 8000] average reconstruction error: 0.0000137230 \n",
      "Epoch [1381 / 8000] average reconstruction error: 0.0000143496 \n",
      "Epoch [1391 / 8000] average reconstruction error: 0.0000093370 \n",
      "Epoch [1401 / 8000] average reconstruction error: 0.0000063398 \n",
      "Epoch [1411 / 8000] average reconstruction error: 0.0000116976 \n",
      "Epoch [1421 / 8000] average reconstruction error: 0.0000064916 \n",
      "Epoch [1431 / 8000] average reconstruction error: 0.0000115707 \n",
      "Epoch [1441 / 8000] average reconstruction error: 0.0000143370 \n",
      "Epoch [1451 / 8000] average reconstruction error: 0.0000064364 \n",
      "Epoch [1461 / 8000] average reconstruction error: 0.0000139097 \n",
      "Epoch [1471 / 8000] average reconstruction error: 0.0000128415 \n",
      "Epoch [1481 / 8000] average reconstruction error: 0.0000116880 \n",
      "Epoch [1491 / 8000] average reconstruction error: 0.0000098238 \n",
      "Epoch [1501 / 8000] average reconstruction error: 0.0000074962 \n",
      "Epoch [1511 / 8000] average reconstruction error: 0.0000085080 \n",
      "Epoch [1521 / 8000] average reconstruction error: 0.0000074053 \n",
      "Epoch [1531 / 8000] average reconstruction error: 0.0000242484 \n",
      "Epoch [1541 / 8000] average reconstruction error: 0.0000218041 \n",
      "Epoch [1551 / 8000] average reconstruction error: 0.0003247305 \n",
      "Epoch [1561 / 8000] average reconstruction error: 0.0002913285 \n",
      "Epoch [1571 / 8000] average reconstruction error: 0.0004318861 \n",
      "Epoch [1581 / 8000] average reconstruction error: 0.0000441545 \n",
      "Epoch [1591 / 8000] average reconstruction error: 0.0000146814 \n",
      "Epoch [1601 / 8000] average reconstruction error: 0.0000109039 \n",
      "Epoch [1611 / 8000] average reconstruction error: 0.0000137777 \n",
      "Epoch [1621 / 8000] average reconstruction error: 0.0000120888 \n",
      "Epoch [1631 / 8000] average reconstruction error: 0.0000085908 \n",
      "Epoch [1641 / 8000] average reconstruction error: 0.0000103531 \n",
      "Epoch [1651 / 8000] average reconstruction error: 0.0000103875 \n",
      "Epoch [1661 / 8000] average reconstruction error: 0.0000106478 \n",
      "Epoch [1671 / 8000] average reconstruction error: 0.0000094426 \n",
      "Epoch [1681 / 8000] average reconstruction error: 0.0000083806 \n",
      "Epoch [1691 / 8000] average reconstruction error: 0.0000120523 \n",
      "Epoch [1701 / 8000] average reconstruction error: 0.0000122542 \n",
      "Epoch [1711 / 8000] average reconstruction error: 0.0000025944 \n",
      "Epoch [1721 / 8000] average reconstruction error: 0.0000049070 \n",
      "Epoch [1731 / 8000] average reconstruction error: 0.0000048667 \n",
      "Epoch [1741 / 8000] average reconstruction error: 0.0000040778 \n",
      "Epoch [1751 / 8000] average reconstruction error: 0.0000079209 \n",
      "Epoch [1761 / 8000] average reconstruction error: 0.0000097424 \n",
      "Epoch [1771 / 8000] average reconstruction error: 0.0000093699 \n",
      "Epoch [1781 / 8000] average reconstruction error: 0.0000107144 \n",
      "Epoch [1791 / 8000] average reconstruction error: 0.0000045578 \n",
      "Epoch [1801 / 8000] average reconstruction error: 0.0000086973 \n",
      "Epoch [1811 / 8000] average reconstruction error: 0.0000102394 \n",
      "Epoch [1821 / 8000] average reconstruction error: 0.0000057465 \n",
      "Epoch [1831 / 8000] average reconstruction error: 0.0000054816 \n",
      "Epoch [1841 / 8000] average reconstruction error: 0.0000079586 \n",
      "Epoch [1851 / 8000] average reconstruction error: 0.0000040567 \n",
      "Epoch [1861 / 8000] average reconstruction error: 0.0000094034 \n",
      "Epoch [1871 / 8000] average reconstruction error: 0.0000041244 \n",
      "Epoch [1881 / 8000] average reconstruction error: 0.0000091856 \n",
      "Epoch [1891 / 8000] average reconstruction error: 0.0000042784 \n",
      "Epoch [1901 / 8000] average reconstruction error: 0.0000048003 \n",
      "Epoch [1911 / 8000] average reconstruction error: 0.0000076927 \n",
      "Epoch [1921 / 8000] average reconstruction error: 0.0000054317 \n",
      "Epoch [1931 / 8000] average reconstruction error: 0.0000106034 \n",
      "Epoch [1941 / 8000] average reconstruction error: 0.0000078395 \n",
      "Epoch [1951 / 8000] average reconstruction error: 0.0000091703 \n",
      "Epoch [1961 / 8000] average reconstruction error: 0.0000084819 \n",
      "Epoch [1971 / 8000] average reconstruction error: 0.0000203421 \n",
      "Epoch [1981 / 8000] average reconstruction error: 0.0001203140 \n",
      "Epoch [1991 / 8000] average reconstruction error: 0.0001864179 \n",
      "Epoch [2001 / 8000] average reconstruction error: 0.0000478581 \n",
      "Epoch [2011 / 8000] average reconstruction error: 0.0000219682 \n",
      "Epoch [2021 / 8000] average reconstruction error: 0.0000377351 \n",
      "Epoch [2031 / 8000] average reconstruction error: 0.0001401981 \n",
      "Epoch [2041 / 8000] average reconstruction error: 0.0000338758 \n",
      "Epoch [2051 / 8000] average reconstruction error: 0.0000242860 \n",
      "Epoch [2061 / 8000] average reconstruction error: 0.0000291308 \n",
      "Epoch [2071 / 8000] average reconstruction error: 0.0000113196 \n",
      "Epoch [2081 / 8000] average reconstruction error: 0.0000086217 \n",
      "Epoch [2091 / 8000] average reconstruction error: 0.0000134675 \n",
      "Epoch [2101 / 8000] average reconstruction error: 0.0000074134 \n",
      "Epoch [2111 / 8000] average reconstruction error: 0.0000118027 \n",
      "Epoch [2121 / 8000] average reconstruction error: 0.0000015271 \n",
      "Epoch [2131 / 8000] average reconstruction error: 0.0000075660 \n",
      "Epoch [2141 / 8000] average reconstruction error: 0.0000032705 \n",
      "Epoch [2151 / 8000] average reconstruction error: 0.0000082889 \n",
      "Epoch [2161 / 8000] average reconstruction error: 0.0000091417 \n",
      "Epoch [2171 / 8000] average reconstruction error: 0.0000014992 \n",
      "Epoch [2181 / 8000] average reconstruction error: 0.0000053284 \n",
      "Epoch [2191 / 8000] average reconstruction error: 0.0000097907 \n",
      "Epoch [2201 / 8000] average reconstruction error: 0.0000106355 \n",
      "Epoch [2211 / 8000] average reconstruction error: 0.0000095219 \n",
      "Epoch [2221 / 8000] average reconstruction error: 0.0000075248 \n",
      "Epoch [2231 / 8000] average reconstruction error: 0.0000072620 \n",
      "Epoch [2241 / 8000] average reconstruction error: 0.0000031549 \n",
      "Epoch [2251 / 8000] average reconstruction error: 0.0000030253 \n",
      "Epoch [2261 / 8000] average reconstruction error: 0.0000076063 \n",
      "Epoch [2271 / 8000] average reconstruction error: 0.0000084905 \n",
      "Epoch [2281 / 8000] average reconstruction error: 0.0000048273 \n",
      "Epoch [2291 / 8000] average reconstruction error: 0.0000086749 \n",
      "Epoch [2301 / 8000] average reconstruction error: 0.0000102703 \n",
      "Epoch [2311 / 8000] average reconstruction error: 0.0000051947 \n",
      "Epoch [2321 / 8000] average reconstruction error: 0.0000115192 \n",
      "Epoch [2331 / 8000] average reconstruction error: 0.0000056824 \n",
      "Epoch [2341 / 8000] average reconstruction error: 0.0000156179 \n",
      "Epoch [2351 / 8000] average reconstruction error: 0.0000164029 \n",
      "Epoch [2361 / 8000] average reconstruction error: 0.0000252617 \n",
      "Epoch [2371 / 8000] average reconstruction error: 0.0000326573 \n",
      "Epoch [2381 / 8000] average reconstruction error: 0.0000377775 \n",
      "Epoch [2391 / 8000] average reconstruction error: 0.0000061674 \n",
      "Epoch [2401 / 8000] average reconstruction error: 0.0000153767 \n",
      "Epoch [2411 / 8000] average reconstruction error: 0.0000058032 \n",
      "Epoch [2421 / 8000] average reconstruction error: 0.0000082331 \n",
      "Epoch [2431 / 8000] average reconstruction error: 0.0000058722 \n",
      "Epoch [2441 / 8000] average reconstruction error: 0.0000049909 \n",
      "Epoch [2451 / 8000] average reconstruction error: 0.0000052863 \n",
      "Epoch [2461 / 8000] average reconstruction error: 0.0000076076 \n",
      "Epoch [2471 / 8000] average reconstruction error: 0.0000028137 \n",
      "Epoch [2481 / 8000] average reconstruction error: 0.0000104465 \n",
      "Epoch [2491 / 8000] average reconstruction error: 0.0000066853 \n",
      "Epoch [2501 / 8000] average reconstruction error: 0.0000057593 \n",
      "Epoch [2511 / 8000] average reconstruction error: 0.0000085930 \n",
      "Epoch [2521 / 8000] average reconstruction error: 0.0000039550 \n",
      "Epoch [2531 / 8000] average reconstruction error: 0.0000120405 \n",
      "Epoch [2541 / 8000] average reconstruction error: 0.0000328242 \n",
      "Epoch [2551 / 8000] average reconstruction error: 0.0000215185 \n",
      "Epoch [2561 / 8000] average reconstruction error: 0.0000110038 \n",
      "Epoch [2571 / 8000] average reconstruction error: 0.0000092339 \n",
      "Epoch [2581 / 8000] average reconstruction error: 0.0000079152 \n",
      "Epoch [2591 / 8000] average reconstruction error: 0.0000054448 \n",
      "Epoch [2601 / 8000] average reconstruction error: 0.0000081902 \n",
      "Epoch [2611 / 8000] average reconstruction error: 0.0000069994 \n",
      "Epoch [2621 / 8000] average reconstruction error: 0.0000092439 \n",
      "Epoch [2631 / 8000] average reconstruction error: 0.0000028351 \n",
      "Epoch [2641 / 8000] average reconstruction error: 0.0000093197 \n",
      "Epoch [2651 / 8000] average reconstruction error: 0.0000053553 \n",
      "Epoch [2661 / 8000] average reconstruction error: 0.0000083023 \n",
      "Epoch [2671 / 8000] average reconstruction error: 0.0000058029 \n",
      "Epoch [2681 / 8000] average reconstruction error: 0.0000166145 \n",
      "Epoch [2691 / 8000] average reconstruction error: 0.0000331250 \n",
      "Epoch [2701 / 8000] average reconstruction error: 0.0001230624 \n",
      "Epoch [2711 / 8000] average reconstruction error: 0.0000102914 \n",
      "Epoch [2721 / 8000] average reconstruction error: 0.0001357001 \n",
      "Epoch [2731 / 8000] average reconstruction error: 0.0000350199 \n",
      "Epoch [2741 / 8000] average reconstruction error: 0.0000105936 \n",
      "Epoch [2751 / 8000] average reconstruction error: 0.0000159248 \n",
      "Epoch [2761 / 8000] average reconstruction error: 0.0000088191 \n",
      "Epoch [2771 / 8000] average reconstruction error: 0.0000036666 \n",
      "Epoch [2781 / 8000] average reconstruction error: 0.0000102288 \n",
      "Epoch [2791 / 8000] average reconstruction error: 0.0000244405 \n",
      "Epoch [2801 / 8000] average reconstruction error: 0.0000975292 \n",
      "Epoch [2811 / 8000] average reconstruction error: 0.0000751618 \n",
      "Epoch [2821 / 8000] average reconstruction error: 0.0000360119 \n",
      "Epoch [2831 / 8000] average reconstruction error: 0.0000227186 \n",
      "Epoch [2841 / 8000] average reconstruction error: 0.0000096535 \n",
      "Epoch [2851 / 8000] average reconstruction error: 0.0000031968 \n",
      "Epoch [2861 / 8000] average reconstruction error: 0.0000025936 \n",
      "Epoch [2871 / 8000] average reconstruction error: 0.0000080991 \n",
      "Epoch [2881 / 8000] average reconstruction error: 0.0000068584 \n",
      "Epoch [2891 / 8000] average reconstruction error: 0.0000076041 \n",
      "Epoch [2901 / 8000] average reconstruction error: 0.0000025999 \n",
      "Epoch [2911 / 8000] average reconstruction error: 0.0000019769 \n",
      "Epoch [2921 / 8000] average reconstruction error: 0.0000096242 \n",
      "Epoch [2931 / 8000] average reconstruction error: 0.0000154455 \n",
      "Epoch [2941 / 8000] average reconstruction error: 0.0000082922 \n",
      "Epoch [2951 / 8000] average reconstruction error: 0.0000018285 \n",
      "Epoch [2961 / 8000] average reconstruction error: 0.0000078879 \n",
      "Epoch [2971 / 8000] average reconstruction error: 0.0000074664 \n",
      "Epoch [2981 / 8000] average reconstruction error: 0.0000056337 \n",
      "Epoch [2991 / 8000] average reconstruction error: 0.0000078079 \n",
      "Epoch [3001 / 8000] average reconstruction error: 0.0000033158 \n",
      "Epoch [3011 / 8000] average reconstruction error: 0.0000077069 \n",
      "Epoch [3021 / 8000] average reconstruction error: 0.0000057580 \n",
      "Epoch [3031 / 8000] average reconstruction error: 0.0000096524 \n",
      "Epoch [3041 / 8000] average reconstruction error: 0.0000206533 \n",
      "Epoch [3051 / 8000] average reconstruction error: 0.0000094101 \n",
      "Epoch [3061 / 8000] average reconstruction error: 0.0000154973 \n",
      "Epoch [3071 / 8000] average reconstruction error: 0.0000101789 \n",
      "Epoch [3081 / 8000] average reconstruction error: 0.0000565775 \n",
      "Epoch [3091 / 8000] average reconstruction error: 0.0000104892 \n",
      "Epoch [3101 / 8000] average reconstruction error: 0.0000094944 \n",
      "Epoch [3111 / 8000] average reconstruction error: 0.0000096603 \n",
      "Epoch [3121 / 8000] average reconstruction error: 0.0000090295 \n",
      "Epoch [3131 / 8000] average reconstruction error: 0.0000090153 \n",
      "Epoch [3141 / 8000] average reconstruction error: 0.0000232728 \n",
      "Epoch [3151 / 8000] average reconstruction error: 0.0000043108 \n",
      "Epoch [3161 / 8000] average reconstruction error: 0.0000114999 \n",
      "Epoch [3171 / 8000] average reconstruction error: 0.0000020830 \n",
      "Epoch [3181 / 8000] average reconstruction error: 0.0000031982 \n",
      "Epoch [3191 / 8000] average reconstruction error: 0.0000091526 \n",
      "Epoch [3201 / 8000] average reconstruction error: 0.0000027295 \n",
      "Epoch [3211 / 8000] average reconstruction error: 0.0000090367 \n",
      "Epoch [3221 / 8000] average reconstruction error: 0.0000096261 \n",
      "Epoch [3231 / 8000] average reconstruction error: 0.0000035920 \n",
      "Epoch [3241 / 8000] average reconstruction error: 0.0000075550 \n",
      "Epoch [3251 / 8000] average reconstruction error: 0.0000058412 \n",
      "Epoch [3261 / 8000] average reconstruction error: 0.0000088240 \n",
      "Epoch [3271 / 8000] average reconstruction error: 0.0000088384 \n",
      "Epoch [3281 / 8000] average reconstruction error: 0.0000059826 \n",
      "Epoch [3291 / 8000] average reconstruction error: 0.0000068563 \n",
      "Epoch [3301 / 8000] average reconstruction error: 0.0000100131 \n",
      "Epoch [3311 / 8000] average reconstruction error: 0.0000110722 \n",
      "Epoch [3321 / 8000] average reconstruction error: 0.0000278053 \n",
      "Epoch [3331 / 8000] average reconstruction error: 0.0000104914 \n",
      "Epoch [3341 / 8000] average reconstruction error: 0.0000099228 \n",
      "Epoch [3351 / 8000] average reconstruction error: 0.0000223629 \n",
      "Epoch [3361 / 8000] average reconstruction error: 0.0000122434 \n",
      "Epoch [3371 / 8000] average reconstruction error: 0.0000478570 \n",
      "Epoch [3381 / 8000] average reconstruction error: 0.0000245722 \n",
      "Epoch [3391 / 8000] average reconstruction error: 0.0000564994 \n",
      "Epoch [3401 / 8000] average reconstruction error: 0.0000131177 \n",
      "Epoch [3411 / 8000] average reconstruction error: 0.0000101310 \n",
      "Epoch [3421 / 8000] average reconstruction error: 0.0000063708 \n",
      "Epoch [3431 / 8000] average reconstruction error: 0.0000027839 \n",
      "Epoch [3441 / 8000] average reconstruction error: 0.0000051234 \n",
      "Epoch [3451 / 8000] average reconstruction error: 0.0000066635 \n",
      "Epoch [3461 / 8000] average reconstruction error: 0.0000022893 \n",
      "Epoch [3471 / 8000] average reconstruction error: 0.0000084456 \n",
      "Epoch [3481 / 8000] average reconstruction error: 0.0000096692 \n",
      "Epoch [3491 / 8000] average reconstruction error: 0.0000074946 \n",
      "Epoch [3501 / 8000] average reconstruction error: 0.0000078967 \n",
      "Epoch [3511 / 8000] average reconstruction error: 0.0000025749 \n",
      "Epoch [3521 / 8000] average reconstruction error: 0.0000087355 \n",
      "Epoch [3531 / 8000] average reconstruction error: 0.0000029469 \n",
      "Epoch [3541 / 8000] average reconstruction error: 0.0000049335 \n",
      "Epoch [3551 / 8000] average reconstruction error: 0.0000058741 \n",
      "Epoch [3561 / 8000] average reconstruction error: 0.0000031844 \n",
      "Epoch [3571 / 8000] average reconstruction error: 0.0000080149 \n",
      "Epoch [3581 / 8000] average reconstruction error: 0.0000137761 \n",
      "Epoch [3591 / 8000] average reconstruction error: 0.0000121571 \n",
      "Epoch [3601 / 8000] average reconstruction error: 0.0000011254 \n",
      "Epoch [3611 / 8000] average reconstruction error: 0.0000057135 \n",
      "Epoch [3621 / 8000] average reconstruction error: 0.0000064367 \n",
      "Epoch [3631 / 8000] average reconstruction error: 0.0000026507 \n",
      "Epoch [3641 / 8000] average reconstruction error: 0.0000031402 \n",
      "Epoch [3651 / 8000] average reconstruction error: 0.0000019498 \n",
      "Epoch [3661 / 8000] average reconstruction error: 0.0000047373 \n",
      "Epoch [3671 / 8000] average reconstruction error: 0.0000024370 \n",
      "Epoch [3681 / 8000] average reconstruction error: 0.0000048913 \n",
      "Epoch [3691 / 8000] average reconstruction error: 0.0000070391 \n",
      "Epoch [3701 / 8000] average reconstruction error: 0.0000050556 \n",
      "Epoch [3711 / 8000] average reconstruction error: 0.0000098513 \n",
      "Epoch [3721 / 8000] average reconstruction error: 0.0000234209 \n",
      "Epoch [3731 / 8000] average reconstruction error: 0.0000035211 \n",
      "Epoch [3741 / 8000] average reconstruction error: 0.0003922005 \n",
      "Epoch [3751 / 8000] average reconstruction error: 0.0000576295 \n",
      "Epoch [3761 / 8000] average reconstruction error: 0.0000072300 \n",
      "Epoch [3771 / 8000] average reconstruction error: 0.0000062568 \n",
      "Epoch [3781 / 8000] average reconstruction error: 0.0000024407 \n",
      "Epoch [3791 / 8000] average reconstruction error: 0.0000069880 \n",
      "Epoch [3801 / 8000] average reconstruction error: 0.0000020795 \n",
      "Epoch [3811 / 8000] average reconstruction error: 0.0000016601 \n",
      "Epoch [3821 / 8000] average reconstruction error: 0.0000064981 \n",
      "Epoch [3831 / 8000] average reconstruction error: 0.0000019535 \n",
      "Epoch [3841 / 8000] average reconstruction error: 0.0000062346 \n",
      "Epoch [3851 / 8000] average reconstruction error: 0.0000068076 \n",
      "Epoch [3861 / 8000] average reconstruction error: 0.0000057900 \n",
      "Epoch [3871 / 8000] average reconstruction error: 0.0000077476 \n",
      "Epoch [3881 / 8000] average reconstruction error: 0.0000042403 \n",
      "Epoch [3891 / 8000] average reconstruction error: 0.0000022997 \n",
      "Epoch [3901 / 8000] average reconstruction error: 0.0000067858 \n",
      "Epoch [3911 / 8000] average reconstruction error: 0.0000081386 \n",
      "Epoch [3921 / 8000] average reconstruction error: 0.0000060618 \n",
      "Epoch [3931 / 8000] average reconstruction error: 0.0000063534 \n",
      "Epoch [3941 / 8000] average reconstruction error: 0.0000077063 \n",
      "Epoch [3951 / 8000] average reconstruction error: 0.0000073101 \n",
      "Epoch [3961 / 8000] average reconstruction error: 0.0000038144 \n",
      "Epoch [3971 / 8000] average reconstruction error: 0.0000084736 \n",
      "Epoch [3981 / 8000] average reconstruction error: 0.0000315983 \n",
      "Epoch [3991 / 8000] average reconstruction error: 0.0000172914 \n",
      "Epoch [4001 / 8000] average reconstruction error: 0.0000090698 \n",
      "Epoch [4011 / 8000] average reconstruction error: 0.0000100907 \n",
      "Epoch [4021 / 8000] average reconstruction error: 0.0000059841 \n",
      "Epoch [4031 / 8000] average reconstruction error: 0.0000116612 \n",
      "Epoch [4041 / 8000] average reconstruction error: 0.0000024568 \n",
      "Epoch [4051 / 8000] average reconstruction error: 0.0000049497 \n",
      "Epoch [4061 / 8000] average reconstruction error: 0.0000077725 \n",
      "Epoch [4071 / 8000] average reconstruction error: 0.0000050399 \n",
      "Epoch [4081 / 8000] average reconstruction error: 0.0000074259 \n",
      "Epoch [4091 / 8000] average reconstruction error: 0.0000080281 \n",
      "Epoch [4101 / 8000] average reconstruction error: 0.0000042299 \n",
      "Epoch [4111 / 8000] average reconstruction error: 0.0000043384 \n",
      "Epoch [4121 / 8000] average reconstruction error: 0.0000190974 \n",
      "Epoch [4131 / 8000] average reconstruction error: 0.0000236644 \n",
      "Epoch [4141 / 8000] average reconstruction error: 0.0000078741 \n",
      "Epoch [4151 / 8000] average reconstruction error: 0.0000076036 \n",
      "Epoch [4161 / 8000] average reconstruction error: 0.0000045108 \n",
      "Epoch [4171 / 8000] average reconstruction error: 0.0000075058 \n",
      "Epoch [4181 / 8000] average reconstruction error: 0.0000697417 \n",
      "Epoch [4191 / 8000] average reconstruction error: 0.0001604965 \n",
      "Epoch [4201 / 8000] average reconstruction error: 0.0000614421 \n",
      "Epoch [4211 / 8000] average reconstruction error: 0.0000125134 \n",
      "Epoch [4221 / 8000] average reconstruction error: 0.0000092725 \n",
      "Epoch [4231 / 8000] average reconstruction error: 0.0000148663 \n",
      "Epoch [4241 / 8000] average reconstruction error: 0.0000100677 \n",
      "Epoch [4251 / 8000] average reconstruction error: 0.0000069329 \n",
      "Epoch [4261 / 8000] average reconstruction error: 0.0000076194 \n",
      "Epoch [4271 / 8000] average reconstruction error: 0.0000060390 \n",
      "Epoch [4281 / 8000] average reconstruction error: 0.0000070972 \n",
      "Epoch [4291 / 8000] average reconstruction error: 0.0000035465 \n",
      "Epoch [4301 / 8000] average reconstruction error: 0.0000054147 \n",
      "Epoch [4311 / 8000] average reconstruction error: 0.0000053742 \n",
      "Epoch [4321 / 8000] average reconstruction error: 0.0000020635 \n",
      "Epoch [4331 / 8000] average reconstruction error: 0.0000004287 \n",
      "Epoch [4341 / 8000] average reconstruction error: 0.0000058902 \n",
      "Epoch [4351 / 8000] average reconstruction error: 0.0000047056 \n",
      "Epoch [4361 / 8000] average reconstruction error: 0.0000017952 \n",
      "Epoch [4371 / 8000] average reconstruction error: 0.0000019156 \n",
      "Epoch [4381 / 8000] average reconstruction error: 0.0000028855 \n",
      "Epoch [4391 / 8000] average reconstruction error: 0.0000057917 \n",
      "Epoch [4401 / 8000] average reconstruction error: 0.0000055429 \n",
      "Epoch [4411 / 8000] average reconstruction error: 0.0000036736 \n",
      "Epoch [4421 / 8000] average reconstruction error: 0.0000028881 \n",
      "Epoch [4431 / 8000] average reconstruction error: 0.0000024119 \n",
      "Epoch [4441 / 8000] average reconstruction error: 0.0000031981 \n",
      "Epoch [4451 / 8000] average reconstruction error: 0.0000043731 \n",
      "Epoch [4461 / 8000] average reconstruction error: 0.0000042385 \n",
      "Epoch [4471 / 8000] average reconstruction error: 0.0000060536 \n",
      "Epoch [4481 / 8000] average reconstruction error: 0.0000017731 \n",
      "Epoch [4491 / 8000] average reconstruction error: 0.0000044803 \n",
      "Epoch [4501 / 8000] average reconstruction error: 0.0000038397 \n",
      "Epoch [4511 / 8000] average reconstruction error: 0.0000044004 \n",
      "Epoch [4521 / 8000] average reconstruction error: 0.0000024678 \n",
      "Epoch [4531 / 8000] average reconstruction error: 0.0000019090 \n",
      "Epoch [4541 / 8000] average reconstruction error: 0.0000013974 \n",
      "Epoch [4551 / 8000] average reconstruction error: 0.0000010533 \n",
      "Epoch [4561 / 8000] average reconstruction error: 0.0000035874 \n",
      "Epoch [4571 / 8000] average reconstruction error: 0.0000397161 \n",
      "Epoch [4581 / 8000] average reconstruction error: 0.0000136912 \n",
      "Epoch [4591 / 8000] average reconstruction error: 0.0000150693 \n",
      "Epoch [4601 / 8000] average reconstruction error: 0.0000259184 \n",
      "Epoch [4611 / 8000] average reconstruction error: 0.0000131250 \n",
      "Epoch [4621 / 8000] average reconstruction error: 0.0000124995 \n",
      "Epoch [4631 / 8000] average reconstruction error: 0.0000854164 \n",
      "Epoch [4641 / 8000] average reconstruction error: 0.0000096995 \n",
      "Epoch [4651 / 8000] average reconstruction error: 0.0000272938 \n",
      "Epoch [4661 / 8000] average reconstruction error: 0.0000095918 \n",
      "Epoch [4671 / 8000] average reconstruction error: 0.0000080559 \n",
      "Epoch [4681 / 8000] average reconstruction error: 0.0000018422 \n",
      "Epoch [4691 / 8000] average reconstruction error: 0.0000023238 \n",
      "Epoch [4701 / 8000] average reconstruction error: 0.0000004178 \n",
      "Epoch [4711 / 8000] average reconstruction error: 0.0000035305 \n",
      "Epoch [4721 / 8000] average reconstruction error: 0.0000051807 \n",
      "Epoch [4731 / 8000] average reconstruction error: 0.0000049018 \n",
      "Epoch [4741 / 8000] average reconstruction error: 0.0000018348 \n",
      "Epoch [4751 / 8000] average reconstruction error: 0.0000049907 \n",
      "Epoch [4761 / 8000] average reconstruction error: 0.0000049530 \n",
      "Epoch [4771 / 8000] average reconstruction error: 0.0000058194 \n",
      "Epoch [4781 / 8000] average reconstruction error: 0.0000067948 \n",
      "Epoch [4791 / 8000] average reconstruction error: 0.0000059236 \n",
      "Epoch [4801 / 8000] average reconstruction error: 0.0000016364 \n",
      "Epoch [4811 / 8000] average reconstruction error: 0.0000090156 \n",
      "Epoch [4821 / 8000] average reconstruction error: 0.0000284429 \n",
      "Epoch [4831 / 8000] average reconstruction error: 0.0000065144 \n",
      "Epoch [4841 / 8000] average reconstruction error: 0.0000091006 \n",
      "Epoch [4851 / 8000] average reconstruction error: 0.0000765417 \n",
      "Epoch [4861 / 8000] average reconstruction error: 0.0000221602 \n",
      "Epoch [4871 / 8000] average reconstruction error: 0.0000193680 \n",
      "Epoch [4881 / 8000] average reconstruction error: 0.0000314095 \n",
      "Epoch [4891 / 8000] average reconstruction error: 0.0000059328 \n",
      "Epoch [4901 / 8000] average reconstruction error: 0.0000073854 \n",
      "Epoch [4911 / 8000] average reconstruction error: 0.0000053686 \n",
      "Epoch [4921 / 8000] average reconstruction error: 0.0000034474 \n",
      "Epoch [4931 / 8000] average reconstruction error: 0.0000025462 \n",
      "Epoch [4941 / 8000] average reconstruction error: 0.0000033757 \n",
      "Epoch [4951 / 8000] average reconstruction error: 0.0000039224 \n",
      "Epoch [4961 / 8000] average reconstruction error: 0.0000008588 \n",
      "Epoch [4971 / 8000] average reconstruction error: 0.0000017768 \n",
      "Epoch [4981 / 8000] average reconstruction error: 0.0000059689 \n",
      "Epoch [4991 / 8000] average reconstruction error: 0.0000083708 \n",
      "Epoch [5001 / 8000] average reconstruction error: 0.0000024181 \n",
      "Epoch [5011 / 8000] average reconstruction error: 0.0000039892 \n",
      "Epoch [5021 / 8000] average reconstruction error: 0.0000036523 \n",
      "Epoch [5031 / 8000] average reconstruction error: 0.0000031250 \n",
      "Epoch [5041 / 8000] average reconstruction error: 0.0000150324 \n",
      "Epoch [5051 / 8000] average reconstruction error: 0.0000989426 \n",
      "Epoch [5061 / 8000] average reconstruction error: 0.0000075119 \n",
      "Epoch [5071 / 8000] average reconstruction error: 0.0000166893 \n",
      "Epoch [5081 / 8000] average reconstruction error: 0.0000102067 \n",
      "Epoch [5091 / 8000] average reconstruction error: 0.0001160295 \n",
      "Epoch [5101 / 8000] average reconstruction error: 0.0000086386 \n",
      "Epoch [5111 / 8000] average reconstruction error: 0.0000030518 \n",
      "Epoch [5121 / 8000] average reconstruction error: 0.0000038301 \n",
      "Epoch [5131 / 8000] average reconstruction error: 0.0000035642 \n",
      "Epoch [5141 / 8000] average reconstruction error: 0.0000053491 \n",
      "Epoch [5151 / 8000] average reconstruction error: 0.0000041616 \n",
      "Epoch [5161 / 8000] average reconstruction error: 0.0000038076 \n",
      "Epoch [5171 / 8000] average reconstruction error: 0.0000044946 \n",
      "Epoch [5181 / 8000] average reconstruction error: 0.0000049294 \n",
      "Epoch [5191 / 8000] average reconstruction error: 0.0000012239 \n",
      "Epoch [5201 / 8000] average reconstruction error: 0.0000042853 \n",
      "Epoch [5211 / 8000] average reconstruction error: 0.0000012372 \n",
      "Epoch [5221 / 8000] average reconstruction error: 0.0000011593 \n",
      "Epoch [5231 / 8000] average reconstruction error: 0.0000037876 \n",
      "Epoch [5241 / 8000] average reconstruction error: 0.0000042157 \n",
      "Epoch [5251 / 8000] average reconstruction error: 0.0000003505 \n",
      "Epoch [5261 / 8000] average reconstruction error: 0.0000017551 \n",
      "Epoch [5271 / 8000] average reconstruction error: 0.0000070706 \n",
      "Epoch [5281 / 8000] average reconstruction error: 0.0000093811 \n",
      "Epoch [5291 / 8000] average reconstruction error: 0.0000033261 \n",
      "Epoch [5301 / 8000] average reconstruction error: 0.0000024123 \n",
      "Epoch [5311 / 8000] average reconstruction error: 0.0000047976 \n",
      "Epoch [5321 / 8000] average reconstruction error: 0.0000015751 \n",
      "Epoch [5331 / 8000] average reconstruction error: 0.0000062679 \n",
      "Epoch [5341 / 8000] average reconstruction error: 0.0000017266 \n",
      "Epoch [5351 / 8000] average reconstruction error: 0.0000902036 \n",
      "Epoch [5361 / 8000] average reconstruction error: 0.0002754570 \n",
      "Epoch [5371 / 8000] average reconstruction error: 0.0000733129 \n",
      "Epoch [5381 / 8000] average reconstruction error: 0.0000491624 \n",
      "Epoch [5391 / 8000] average reconstruction error: 0.0000043541 \n",
      "Epoch [5401 / 8000] average reconstruction error: 0.0000315326 \n",
      "Epoch [5411 / 8000] average reconstruction error: 0.0000020940 \n",
      "Epoch [5421 / 8000] average reconstruction error: 0.0000058127 \n",
      "Epoch [5431 / 8000] average reconstruction error: 0.0000140869 \n",
      "Epoch [5441 / 8000] average reconstruction error: 0.0000033371 \n",
      "Epoch [5451 / 8000] average reconstruction error: 0.0000042695 \n",
      "Epoch [5461 / 8000] average reconstruction error: 0.0000050360 \n",
      "Epoch [5471 / 8000] average reconstruction error: 0.0000044799 \n",
      "Epoch [5481 / 8000] average reconstruction error: 0.0000200310 \n",
      "Epoch [5491 / 8000] average reconstruction error: 0.0000217126 \n",
      "Epoch [5501 / 8000] average reconstruction error: 0.0000024573 \n",
      "Epoch [5511 / 8000] average reconstruction error: 0.0000235120 \n",
      "Epoch [5521 / 8000] average reconstruction error: 0.0000130538 \n",
      "Epoch [5531 / 8000] average reconstruction error: 0.0000023379 \n",
      "Epoch [5541 / 8000] average reconstruction error: 0.0000037427 \n",
      "Epoch [5551 / 8000] average reconstruction error: 0.0000030165 \n",
      "Epoch [5561 / 8000] average reconstruction error: 0.0000011238 \n",
      "Epoch [5571 / 8000] average reconstruction error: 0.0000002360 \n",
      "Epoch [5581 / 8000] average reconstruction error: 0.0000009380 \n",
      "Epoch [5591 / 8000] average reconstruction error: 0.0000009023 \n",
      "Epoch [5601 / 8000] average reconstruction error: 0.0000008378 \n",
      "Epoch [5611 / 8000] average reconstruction error: 0.0000027657 \n",
      "Epoch [5621 / 8000] average reconstruction error: 0.0000012680 \n",
      "Epoch [5631 / 8000] average reconstruction error: 0.0000030997 \n",
      "Epoch [5641 / 8000] average reconstruction error: 0.0000035200 \n",
      "Epoch [5651 / 8000] average reconstruction error: 0.0000030076 \n",
      "Epoch [5661 / 8000] average reconstruction error: 0.0000033289 \n",
      "Epoch [5671 / 8000] average reconstruction error: 0.0000046403 \n",
      "Epoch [5681 / 8000] average reconstruction error: 0.0000049721 \n",
      "Epoch [5691 / 8000] average reconstruction error: 0.0000171837 \n",
      "Epoch [5701 / 8000] average reconstruction error: 0.0000230015 \n",
      "Epoch [5711 / 8000] average reconstruction error: 0.0000234513 \n",
      "Epoch [5721 / 8000] average reconstruction error: 0.0000029228 \n",
      "Epoch [5731 / 8000] average reconstruction error: 0.0000035145 \n",
      "Epoch [5741 / 8000] average reconstruction error: 0.0000031380 \n",
      "Epoch [5751 / 8000] average reconstruction error: 0.0000007275 \n",
      "Epoch [5761 / 8000] average reconstruction error: 0.0000028476 \n",
      "Epoch [5771 / 8000] average reconstruction error: 0.0000040743 \n",
      "Epoch [5781 / 8000] average reconstruction error: 0.0000003805 \n",
      "Epoch [5791 / 8000] average reconstruction error: 0.0000029250 \n",
      "Epoch [5801 / 8000] average reconstruction error: 0.0000020292 \n",
      "Epoch [5811 / 8000] average reconstruction error: 0.0000021602 \n",
      "Epoch [5821 / 8000] average reconstruction error: 0.0000010000 \n",
      "Epoch [5831 / 8000] average reconstruction error: 0.0000023776 \n",
      "Epoch [5841 / 8000] average reconstruction error: 0.0000017055 \n",
      "Epoch [5851 / 8000] average reconstruction error: 0.0000010025 \n",
      "Epoch [5861 / 8000] average reconstruction error: 0.0000024231 \n",
      "Epoch [5871 / 8000] average reconstruction error: 0.0000022848 \n",
      "Epoch [5881 / 8000] average reconstruction error: 0.0000028800 \n",
      "Epoch [5891 / 8000] average reconstruction error: 0.0000074893 \n",
      "Epoch [5901 / 8000] average reconstruction error: 0.0000067173 \n",
      "Epoch [5911 / 8000] average reconstruction error: 0.0000025094 \n",
      "Epoch [5921 / 8000] average reconstruction error: 0.0000022030 \n",
      "Epoch [5931 / 8000] average reconstruction error: 0.0000056096 \n",
      "Epoch [5941 / 8000] average reconstruction error: 0.0001773729 \n",
      "Epoch [5951 / 8000] average reconstruction error: 0.0000669177 \n",
      "Epoch [5961 / 8000] average reconstruction error: 0.0000083006 \n",
      "Epoch [5971 / 8000] average reconstruction error: 0.0000076096 \n",
      "Epoch [5981 / 8000] average reconstruction error: 0.0000070750 \n",
      "Epoch [5991 / 8000] average reconstruction error: 0.0000029255 \n",
      "Epoch [6001 / 8000] average reconstruction error: 0.0000031782 \n",
      "Epoch [6011 / 8000] average reconstruction error: 0.0000040942 \n",
      "Epoch [6021 / 8000] average reconstruction error: 0.0000024394 \n",
      "Epoch [6031 / 8000] average reconstruction error: 0.0000015409 \n",
      "Epoch [6041 / 8000] average reconstruction error: 0.0000022201 \n",
      "Epoch [6051 / 8000] average reconstruction error: 0.0000021020 \n",
      "Epoch [6061 / 8000] average reconstruction error: 0.0000008133 \n",
      "Epoch [6071 / 8000] average reconstruction error: 0.0000007359 \n",
      "Epoch [6081 / 8000] average reconstruction error: 0.0000021474 \n",
      "Epoch [6091 / 8000] average reconstruction error: 0.0000006607 \n",
      "Epoch [6101 / 8000] average reconstruction error: 0.0000007217 \n",
      "Epoch [6111 / 8000] average reconstruction error: 0.0000002465 \n",
      "Epoch [6121 / 8000] average reconstruction error: 0.0000022715 \n",
      "Epoch [6131 / 8000] average reconstruction error: 0.0000017973 \n",
      "Epoch [6141 / 8000] average reconstruction error: 0.0000009692 \n",
      "Epoch [6151 / 8000] average reconstruction error: 0.0000017491 \n",
      "Epoch [6161 / 8000] average reconstruction error: 0.0000006359 \n",
      "Epoch [6171 / 8000] average reconstruction error: 0.0000017559 \n",
      "Epoch [6181 / 8000] average reconstruction error: 0.0000023621 \n",
      "Epoch [6191 / 8000] average reconstruction error: 0.0000016669 \n",
      "Epoch [6201 / 8000] average reconstruction error: 0.0000021341 \n",
      "Epoch [6211 / 8000] average reconstruction error: 0.0000017667 \n",
      "Epoch [6221 / 8000] average reconstruction error: 0.0000016968 \n",
      "Epoch [6231 / 8000] average reconstruction error: 0.0000014867 \n",
      "Epoch [6241 / 8000] average reconstruction error: 0.0000017420 \n",
      "Epoch [6251 / 8000] average reconstruction error: 0.0000014168 \n",
      "Epoch [6261 / 8000] average reconstruction error: 0.0000006733 \n",
      "Epoch [6271 / 8000] average reconstruction error: 0.0000014106 \n",
      "Epoch [6281 / 8000] average reconstruction error: 0.0000001643 \n",
      "Epoch [6291 / 8000] average reconstruction error: 0.0000009045 \n",
      "Epoch [6301 / 8000] average reconstruction error: 0.0000010372 \n",
      "Epoch [6311 / 8000] average reconstruction error: 0.0000010603 \n",
      "Epoch [6321 / 8000] average reconstruction error: 0.0000009331 \n",
      "Epoch [6331 / 8000] average reconstruction error: 0.0000013170 \n",
      "Epoch [6341 / 8000] average reconstruction error: 0.0000011686 \n",
      "Epoch [6351 / 8000] average reconstruction error: 0.0000005268 \n",
      "Epoch [6361 / 8000] average reconstruction error: 0.0000012708 \n",
      "Epoch [6371 / 8000] average reconstruction error: 0.0000014741 \n",
      "Epoch [6381 / 8000] average reconstruction error: 0.0000006349 \n",
      "Epoch [6391 / 8000] average reconstruction error: 0.0000012539 \n",
      "Epoch [6401 / 8000] average reconstruction error: 0.0000028450 \n",
      "Epoch [6411 / 8000] average reconstruction error: 0.0000236376 \n",
      "Epoch [6421 / 8000] average reconstruction error: 0.0000088191 \n",
      "Epoch [6431 / 8000] average reconstruction error: 0.0000167383 \n",
      "Epoch [6441 / 8000] average reconstruction error: 0.0000209769 \n",
      "Epoch [6451 / 8000] average reconstruction error: 0.0000243422 \n",
      "Epoch [6461 / 8000] average reconstruction error: 0.0002942482 \n",
      "Epoch [6471 / 8000] average reconstruction error: 0.0007264051 \n",
      "Epoch [6481 / 8000] average reconstruction error: 0.0000326031 \n",
      "Epoch [6491 / 8000] average reconstruction error: 0.0000102279 \n",
      "Epoch [6501 / 8000] average reconstruction error: 0.0000056772 \n",
      "Epoch [6511 / 8000] average reconstruction error: 0.0000030482 \n",
      "Epoch [6521 / 8000] average reconstruction error: 0.0000125636 \n",
      "Epoch [6531 / 8000] average reconstruction error: 0.0000086923 \n",
      "Epoch [6541 / 8000] average reconstruction error: 0.0000108418 \n",
      "Epoch [6551 / 8000] average reconstruction error: 0.0000148254 \n",
      "Epoch [6561 / 8000] average reconstruction error: 0.0000040374 \n",
      "Epoch [6571 / 8000] average reconstruction error: 0.0000119455 \n",
      "Epoch [6581 / 8000] average reconstruction error: 0.0000071226 \n",
      "Epoch [6591 / 8000] average reconstruction error: 0.0000029834 \n",
      "Epoch [6601 / 8000] average reconstruction error: 0.0000022954 \n",
      "Epoch [6611 / 8000] average reconstruction error: 0.0000070224 \n",
      "Epoch [6621 / 8000] average reconstruction error: 0.0000032351 \n",
      "Epoch [6631 / 8000] average reconstruction error: 0.0000035418 \n",
      "Epoch [6641 / 8000] average reconstruction error: 0.0000026935 \n",
      "Epoch [6651 / 8000] average reconstruction error: 0.0000061846 \n",
      "Epoch [6661 / 8000] average reconstruction error: 0.0000043966 \n",
      "Epoch [6671 / 8000] average reconstruction error: 0.0000014251 \n",
      "Epoch [6681 / 8000] average reconstruction error: 0.0000038762 \n",
      "Epoch [6691 / 8000] average reconstruction error: 0.0000037630 \n",
      "Epoch [6701 / 8000] average reconstruction error: 0.0000018260 \n",
      "Epoch [6711 / 8000] average reconstruction error: 0.0000030547 \n",
      "Epoch [6721 / 8000] average reconstruction error: 0.0000036756 \n",
      "Epoch [6731 / 8000] average reconstruction error: 0.0000027252 \n",
      "Epoch [6741 / 8000] average reconstruction error: 0.0000010771 \n",
      "Epoch [6751 / 8000] average reconstruction error: 0.0000041728 \n",
      "Epoch [6761 / 8000] average reconstruction error: 0.0000023085 \n",
      "Epoch [6771 / 8000] average reconstruction error: 0.0000024389 \n",
      "Epoch [6781 / 8000] average reconstruction error: 0.0000021267 \n",
      "Epoch [6791 / 8000] average reconstruction error: 0.0000014560 \n",
      "Epoch [6801 / 8000] average reconstruction error: 0.0000023153 \n",
      "Epoch [6811 / 8000] average reconstruction error: 0.0000009395 \n",
      "Epoch [6821 / 8000] average reconstruction error: 0.0000007800 \n",
      "Epoch [6831 / 8000] average reconstruction error: 0.0000026189 \n",
      "Epoch [6841 / 8000] average reconstruction error: 0.0000011070 \n",
      "Epoch [6851 / 8000] average reconstruction error: 0.0000019934 \n",
      "Epoch [6861 / 8000] average reconstruction error: 0.0000019779 \n",
      "Epoch [6871 / 8000] average reconstruction error: 0.0000013642 \n",
      "Epoch [6881 / 8000] average reconstruction error: 0.0000021046 \n",
      "Epoch [6891 / 8000] average reconstruction error: 0.0000012752 \n",
      "Epoch [6901 / 8000] average reconstruction error: 0.0000014109 \n",
      "Epoch [6911 / 8000] average reconstruction error: 0.0000010836 \n",
      "Epoch [6921 / 8000] average reconstruction error: 0.0000016672 \n",
      "Epoch [6931 / 8000] average reconstruction error: 0.0000022922 \n",
      "Epoch [6941 / 8000] average reconstruction error: 0.0000020713 \n",
      "Epoch [6951 / 8000] average reconstruction error: 0.0000023108 \n",
      "Epoch [6961 / 8000] average reconstruction error: 0.0000008633 \n",
      "Epoch [6971 / 8000] average reconstruction error: 0.0000005686 \n",
      "Epoch [6981 / 8000] average reconstruction error: 0.0000009959 \n",
      "Epoch [6991 / 8000] average reconstruction error: 0.0000019387 \n",
      "Epoch [7001 / 8000] average reconstruction error: 0.0000005949 \n",
      "Epoch [7011 / 8000] average reconstruction error: 0.0000007585 \n",
      "Epoch [7021 / 8000] average reconstruction error: 0.0000017813 \n",
      "Epoch [7031 / 8000] average reconstruction error: 0.0000013140 \n",
      "Epoch [7041 / 8000] average reconstruction error: 0.0000008359 \n",
      "Epoch [7051 / 8000] average reconstruction error: 0.0000002564 \n",
      "Epoch [7061 / 8000] average reconstruction error: 0.0000012535 \n",
      "Epoch [7071 / 8000] average reconstruction error: 0.0000003179 \n",
      "Epoch [7081 / 8000] average reconstruction error: 0.0000007775 \n",
      "Epoch [7091 / 8000] average reconstruction error: 0.0000017508 \n",
      "Epoch [7101 / 8000] average reconstruction error: 0.0000003314 \n",
      "Epoch [7111 / 8000] average reconstruction error: 0.0000006195 \n",
      "Epoch [7121 / 8000] average reconstruction error: 0.0000017918 \n",
      "Epoch [7131 / 8000] average reconstruction error: 0.0000019595 \n",
      "Epoch [7141 / 8000] average reconstruction error: 0.0000006910 \n",
      "Epoch [7151 / 8000] average reconstruction error: 0.0000014389 \n",
      "Epoch [7161 / 8000] average reconstruction error: 0.0000019600 \n",
      "Epoch [7171 / 8000] average reconstruction error: 0.0000015675 \n",
      "Epoch [7181 / 8000] average reconstruction error: 0.0000008999 \n",
      "Epoch [7191 / 8000] average reconstruction error: 0.0000015228 \n",
      "Epoch [7201 / 8000] average reconstruction error: 0.0000008798 \n",
      "Epoch [7211 / 8000] average reconstruction error: 0.0000013756 \n",
      "Epoch [7221 / 8000] average reconstruction error: 0.0000002262 \n",
      "Epoch [7231 / 8000] average reconstruction error: 0.0000016458 \n",
      "Epoch [7241 / 8000] average reconstruction error: 0.0000018203 \n",
      "Epoch [7251 / 8000] average reconstruction error: 0.0000014710 \n",
      "Epoch [7261 / 8000] average reconstruction error: 0.0000006236 \n",
      "Epoch [7271 / 8000] average reconstruction error: 0.0000005675 \n",
      "Epoch [7281 / 8000] average reconstruction error: 0.0000015846 \n",
      "Epoch [7291 / 8000] average reconstruction error: 0.0000014980 \n",
      "Epoch [7301 / 8000] average reconstruction error: 0.0000015052 \n",
      "Epoch [7311 / 8000] average reconstruction error: 0.0000014936 \n",
      "Epoch [7321 / 8000] average reconstruction error: 0.0000013100 \n",
      "Epoch [7331 / 8000] average reconstruction error: 0.0000005264 \n",
      "Epoch [7341 / 8000] average reconstruction error: 0.0000008731 \n",
      "Epoch [7351 / 8000] average reconstruction error: 0.0000016165 \n",
      "Epoch [7361 / 8000] average reconstruction error: 0.0000013721 \n",
      "Epoch [7371 / 8000] average reconstruction error: 0.0000002783 \n",
      "Epoch [7381 / 8000] average reconstruction error: 0.0000002041 \n",
      "Epoch [7391 / 8000] average reconstruction error: 0.0000013300 \n",
      "Epoch [7401 / 8000] average reconstruction error: 0.0000011167 \n",
      "Epoch [7411 / 8000] average reconstruction error: 0.0000017056 \n",
      "Epoch [7421 / 8000] average reconstruction error: 0.0000010192 \n",
      "Epoch [7431 / 8000] average reconstruction error: 0.0000012479 \n",
      "Epoch [7441 / 8000] average reconstruction error: 0.0000009160 \n",
      "Epoch [7451 / 8000] average reconstruction error: 0.0000004189 \n",
      "Epoch [7461 / 8000] average reconstruction error: 0.0000017562 \n",
      "Epoch [7471 / 8000] average reconstruction error: 0.0000013531 \n",
      "Epoch [7481 / 8000] average reconstruction error: 0.0000011741 \n",
      "Epoch [7491 / 8000] average reconstruction error: 0.0000007821 \n",
      "Epoch [7501 / 8000] average reconstruction error: 0.0000011126 \n",
      "Epoch [7511 / 8000] average reconstruction error: 0.0000008154 \n",
      "Epoch [7521 / 8000] average reconstruction error: 0.0000004660 \n",
      "Epoch [7531 / 8000] average reconstruction error: 0.0000004428 \n",
      "Epoch [7541 / 8000] average reconstruction error: 0.0000004586 \n",
      "Epoch [7551 / 8000] average reconstruction error: 0.0000003730 \n",
      "Epoch [7561 / 8000] average reconstruction error: 0.0000004668 \n",
      "Epoch [7571 / 8000] average reconstruction error: 0.0000006448 \n",
      "Epoch [7581 / 8000] average reconstruction error: 0.0000011620 \n",
      "Epoch [7591 / 8000] average reconstruction error: 0.0000003629 \n",
      "Epoch [7601 / 8000] average reconstruction error: 0.0000013258 \n",
      "Epoch [7611 / 8000] average reconstruction error: 0.0000012311 \n",
      "Epoch [7621 / 8000] average reconstruction error: 0.0000014868 \n",
      "Epoch [7631 / 8000] average reconstruction error: 0.0000040524 \n",
      "Epoch [7641 / 8000] average reconstruction error: 0.0000193235 \n",
      "Epoch [7651 / 8000] average reconstruction error: 0.0000285144 \n",
      "Epoch [7661 / 8000] average reconstruction error: 0.0002419673 \n",
      "Epoch [7671 / 8000] average reconstruction error: 0.0000269088 \n",
      "Epoch [7681 / 8000] average reconstruction error: 0.0000037767 \n",
      "Epoch [7691 / 8000] average reconstruction error: 0.0000017529 \n",
      "Epoch [7701 / 8000] average reconstruction error: 0.0000006943 \n",
      "Epoch [7711 / 8000] average reconstruction error: 0.0000016394 \n",
      "Epoch [7721 / 8000] average reconstruction error: 0.0000016444 \n",
      "Epoch [7731 / 8000] average reconstruction error: 0.0000014821 \n",
      "Epoch [7741 / 8000] average reconstruction error: 0.0000008699 \n",
      "Epoch [7751 / 8000] average reconstruction error: 0.0000011782 \n",
      "Epoch [7761 / 8000] average reconstruction error: 0.0000012271 \n",
      "Epoch [7771 / 8000] average reconstruction error: 0.0000004557 \n",
      "Epoch [7781 / 8000] average reconstruction error: 0.0000004376 \n",
      "Epoch [7791 / 8000] average reconstruction error: 0.0000003728 \n",
      "Epoch [7801 / 8000] average reconstruction error: 0.0000003450 \n",
      "Epoch [7811 / 8000] average reconstruction error: 0.0000007053 \n",
      "Epoch [7821 / 8000] average reconstruction error: 0.0000004351 \n",
      "Epoch [7831 / 8000] average reconstruction error: 0.0000007814 \n",
      "Epoch [7841 / 8000] average reconstruction error: 0.0000012878 \n",
      "Epoch [7851 / 8000] average reconstruction error: 0.0000008830 \n",
      "Epoch [7861 / 8000] average reconstruction error: 0.0000010688 \n",
      "Epoch [7871 / 8000] average reconstruction error: 0.0000003906 \n",
      "Epoch [7881 / 8000] average reconstruction error: 0.0000003757 \n",
      "Epoch [7891 / 8000] average reconstruction error: 0.0000012868 \n",
      "Epoch [7901 / 8000] average reconstruction error: 0.0000013570 \n",
      "Epoch [7911 / 8000] average reconstruction error: 0.0000003315 \n",
      "Epoch [7921 / 8000] average reconstruction error: 0.0000012433 \n",
      "Epoch [7931 / 8000] average reconstruction error: 0.0000013435 \n",
      "Epoch [7941 / 8000] average reconstruction error: 0.0000001190 \n",
      "Epoch [7951 / 8000] average reconstruction error: 0.0000009840 \n",
      "Epoch [7961 / 8000] average reconstruction error: 0.0000008166 \n",
      "Epoch [7971 / 8000] average reconstruction error: 0.0000011239 \n",
      "Epoch [7981 / 8000] average reconstruction error: 0.0000012254 \n",
      "Epoch [7991 / 8000] average reconstruction error: 0.0000006284 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        enc_fold = torch_f.Fold(device)\n",
    "        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "        # Collect computation nodes recursively from encoding process\n",
    "        n_nodes = []\n",
    "        for example in batch: #example es un arbolito\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            encode_structure_fold(enc_fold, example)\n",
    "            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "       \n",
    "        # Apply the computations on the encoder model\n",
    "       \n",
    "        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "        \n",
    "        \n",
    "        # Initialize torchfold for *decoding*\n",
    "        dec_fold = torch_f.Fold(device)\n",
    "        # Collect computation nodes recursively from decoding process\n",
    "        dec_fold_nodes = []\n",
    "        kld_fold_nodes = []\n",
    "\n",
    "        t_l = []\n",
    "        for f in enc_fold_nodes:\n",
    "            for t in f:\n",
    "                t_l.append(t)\n",
    "        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\n",
    "            #print(\"example\", example)\n",
    "            #print(\"fnode\", fnode) \n",
    "            #root_code, kl_div = torch.chunk(fnode, 2, 0)\n",
    "            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\n",
    "        # Apply the computations on the decoder model\n",
    "\n",
    "                       \n",
    "        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        #print(\"n\", n_nodes)\n",
    "        total_loss = torch.div(total_loss[0], n_nodes)\n",
    "        #print(\"div\", total_loss)\n",
    "        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        #total_loss = total_loss*10\n",
    "        \n",
    "        #print(\"total_loss\", total_loss)\n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %.10f ' % (epoch+1, epochs, total_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder con batch - decoder sin batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        # Initialize torchfold for *encoding*\\n\\n        \\n        enc_fold = torch_f.Fold(device)\\n        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\\n        # Collect computation nodes recursively from encoding process\\n        n_nodes = []\\n        for example in batch: #example es un arbolito\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            encode_structure_fold(enc_fold, example)\\n            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\\n       \\n        # Apply the computations on the encoder model\\n       \\n        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\\n        encodeado_con_batch = enc_fold_nodes\\n        \\n        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\\n        #print(\"decoded\", decoded)\\n        l = []\\n        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\\n        l = []\\n        ce_loss_list = decoded.traverseInorderCE(decoded, l)\\n            \\n        mse_loss = sum(mse_loss_list) \\n        ce_loss  = sum(ce_loss_list)  \\n        total_loss = (0.5*ce_loss + mse_loss)\\n        #print(\"total_loss\", total_loss)\\n        total_loss = total_loss / len(mse_loss_list)\\n        \\n        \\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        enc_fold = torch_f.Fold(device)\n",
    "        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "        # Collect computation nodes recursively from encoding process\n",
    "        n_nodes = []\n",
    "        for example in batch: #example es un arbolito\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            encode_structure_fold(enc_fold, example)\n",
    "            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "       \n",
    "        # Apply the computations on the encoder model\n",
    "       \n",
    "        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "        encodeado_con_batch = enc_fold_nodes\n",
    "        \n",
    "        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\n",
    "        #print(\"decoded\", decoded)\n",
    "        l = []\n",
    "        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\n",
    "        l = []\n",
    "        ce_loss_list = decoded.traverseInorderCE(decoded, l)\n",
    "            \n",
    "        mse_loss = sum(mse_loss_list) \n",
    "        ce_loss  = sum(ce_loss_list)  \n",
    "        total_loss = (0.5*ce_loss + mse_loss)\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        total_loss = total_loss / len(mse_loss_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder sin batch - decoder con batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        # Initialize torchfold for *encoding*\\n\\n        \\n        \\n        enc_fold_nodes = []\\n        n_nodes = []\\n        for example in batch:\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            enc_fold = encode_structure(example).to(device)\\n        #print(\"encodeado sin batch\", enc_fold)\\n        enc_fold_nodes.append(enc_fold)\\n        encodeado_sin_batch = enc_fold\\n        # Split into a list of fold nodes per example\\n        #enc_fold_nodes = torch.split(enc_fold_nodes[0], 1, 0) #divide ele ncodeado en vectores de un elemento\\n        \\n        \\n        # Initialize torchfold for *decoding*\\n        dec_fold = torch_f.Fold(device)\\n        # Collect computation nodes recursively from decoding process\\n        dec_fold_nodes = []\\n        kld_fold_nodes = []\\n\\n        t_l = []\\n        for f in enc_fold_nodes:\\n            for t in f:\\n                t_l.append(t)\\n        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\\n            #print(\"example\", example)\\n            #print(\"fnode\", fnode) \\n            #root_code, kl_div = torch.chunk(fnode, 2, 0)\\n            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\\n        # Apply the computations on the decoder model\\n        #print(\"dec fold nodes\", dec_fold_nodes)\\n           \\n                       \\n        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\\n        #print(\"total_loss\", total_loss)\\n        n_nodes = torch.tensor(n_nodes, device = device)\\n        #print(\"n\", n_nodes)\\n        total_loss = torch.div(total_loss[0], n_nodes)\\n        #print(\"div\", total_loss)\\n        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\\n        #print(\"total_loss\", total_loss)\\n        \\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        mse_loss_avg[-1] += (mse_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        \n",
    "        enc_fold_nodes = []\n",
    "        n_nodes = []\n",
    "        for example in batch:\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            enc_fold = encode_structure(example).to(device)\n",
    "        #print(\"encodeado sin batch\", enc_fold)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "        encodeado_sin_batch = enc_fold\n",
    "        # Split into a list of fold nodes per example\n",
    "        #enc_fold_nodes = torch.split(enc_fold_nodes[0], 1, 0) #divide ele ncodeado en vectores de un elemento\n",
    "        \n",
    "        \n",
    "        # Initialize torchfold for *decoding*\n",
    "        dec_fold = torch_f.Fold(device)\n",
    "        # Collect computation nodes recursively from decoding process\n",
    "        dec_fold_nodes = []\n",
    "        kld_fold_nodes = []\n",
    "\n",
    "        t_l = []\n",
    "        for f in enc_fold_nodes:\n",
    "            for t in f:\n",
    "                t_l.append(t)\n",
    "        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\n",
    "            #print(\"example\", example)\n",
    "            #print(\"fnode\", fnode) \n",
    "            #root_code, kl_div = torch.chunk(fnode, 2, 0)\n",
    "            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\n",
    "        # Apply the computations on the decoder model\n",
    "        #print(\"dec fold nodes\", dec_fold_nodes)\n",
    "           \n",
    "                       \n",
    "        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        #print(\"n\", n_nodes)\n",
    "        total_loss = torch.div(total_loss[0], n_nodes)\n",
    "        #print(\"div\", total_loss)\n",
    "        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        mse_loss_avg[-1] += (mse_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder sin batch - decoder sin batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n    ce_avg.append(0)\\n    mse_avg.append(0)\\n\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        \\n        enc_fold_nodes = []\\n        n_nodes = []\\n        for example in batch:\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            enc_fold = encode_structure(example).to(device)\\n        #print(\"encodeado sin batch\", enc_fold)\\n        enc_fold_nodes.append(enc_fold)\\n        encodeado_sin_batch = enc_fold\\n        \\n        \\n        \\n        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\\n        #print(\"decoded\", decoded)\\n        l = []\\n        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\\n        l = []\\n        ce_loss_list = decoded.traverseInorderCE(decoded, l)\\n            \\n        mse_loss = sum(mse_loss_list) \\n        ce_loss  = sum(ce_loss_list)  \\n       \\n        ce = [0.4*a for a in ce_loss_list]\\n\\n\\n        total_loss = (0.4*ce_loss + mse_loss)\\n        total_loss = total_loss / len(mse_loss_list)\\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        mse_avg[-1] += (mse_loss.item())\\n        ce_avg[-1] += (ce_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss, \\'mse loss\\': mse_loss, \\'ce loss\\': ce_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "    ce_avg.append(0)\n",
    "    mse_avg.append(0)\n",
    "\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "        enc_fold_nodes = []\n",
    "        n_nodes = []\n",
    "        for example in batch:\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            enc_fold = encode_structure(example).to(device)\n",
    "        #print(\"encodeado sin batch\", enc_fold)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "        encodeado_sin_batch = enc_fold\n",
    "        \n",
    "        \n",
    "        \n",
    "        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\n",
    "        #print(\"decoded\", decoded)\n",
    "        l = []\n",
    "        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\n",
    "        l = []\n",
    "        ce_loss_list = decoded.traverseInorderCE(decoded, l)\n",
    "            \n",
    "        mse_loss = sum(mse_loss_list) \n",
    "        ce_loss  = sum(ce_loss_list)  \n",
    "       \n",
    "        ce = [0.4*a for a in ce_loss_list]\n",
    "\n",
    "\n",
    "        total_loss = (0.4*ce_loss + mse_loss)\n",
    "        total_loss = total_loss / len(mse_loss_list)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        mse_avg[-1] += (mse_loss.item())\n",
    "        ce_avg[-1] += (ce_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss, 'mse loss': mse_loss, 'ce loss': ce_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6205\n"
     ]
    }
   ],
   "source": [
    "encoder = GRASSEncoder(input_size = 4, feature_size=512, hidden_size=1024).to(device)\n",
    "decoder = GRASSDecoder(latent_size=512, hidden_size=1024, mult = mult).to(device)\n",
    "\n",
    "checkpoint = torch.load(\"outputs/best_model.pth\")\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "print(\"epoch\", epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.1121e-08, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_best_model.best_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss [tensor(5.2585e-06, device='cuda:0', grad_fn=<DivBackward0>), tensor(6.3383e-06, device='cuda:0', grad_fn=<DivBackward0>), tensor(6.3383e-06, device='cuda:0', grad_fn=<DivBackward0>), tensor(6.3383e-06, device='cuda:0', grad_fn=<DivBackward0>), tensor(6.9564e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(6.3383e-06, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.4324e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.3296e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(6.3383e-06, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.9762e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(6.3383e-06, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8544e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.9682e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.9303e-06, device='cuda:0', grad_fn=<DivBackward0>), tensor(6.3383e-06, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1381e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.9762e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.9682e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.9762e-05, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "avg testing loss tensor(2.7815e-05, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss_avg.append(0)\n",
    "loss = []\n",
    "for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "    enc_fold_nodes = []\n",
    "    n_nodes = []\n",
    "    for example in batch:\n",
    "        c = []\n",
    "        n = example.count_nodes(example, c)\n",
    "        n_nodes.append(len(n))\n",
    "        enc_fold = encode_structure(example).to(device)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "  \n",
    "    dec = []\n",
    "    for encoded, example in zip(enc_fold_nodes, batch):\n",
    "       dec.append(decode_testing_grass(encoded, example, 100, decoder))\n",
    "       t_l = []\n",
    "    for decoded in dec:\n",
    "        l = []\n",
    "        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\n",
    "        #print(\"m list\", mse_loss_list)\n",
    "        \n",
    "        l = []\n",
    "        ce_loss_list = decoded.traverseInorderCE(decoded, l)\n",
    "        mse_loss_list = [m for m in mse_loss_list if m is not None]\n",
    "        ce_loss_list = [c for c in ce_loss_list if c is not None]\n",
    "        mse_loss = sum(mse_loss_list) \n",
    "        ce_loss  = sum(ce_loss_list)  \n",
    "        total_loss = (0.4*ce_loss + mse_loss)\n",
    "        #print(\"mse\", mse_loss)\n",
    "        t_l.append(total_loss)\n",
    "    \n",
    "    #print(\"total_loss\", t_l)\n",
    "    #print(len(t_l))\n",
    "    #print(len(n_nodes))\n",
    "    n_nodes = torch.tensor(n_nodes, device = device)\n",
    "    t_l = [torch.div(l, n) for l,  n in zip(t_l, n_nodes)]\n",
    "    #print(\"total_loss\", t_l)\n",
    "    #t_l = sum(t_l)/len(batch)\n",
    "    #print(\"total_loss\", t_l)\n",
    "    #print(\"////\")\n",
    "    loss += t_l\n",
    "print(\"testing loss\", loss)\n",
    "avg_testing_loss = sum(loss)/len(loss)\n",
    "#avg_testing_loss = loss.sum() / len(batch) \n",
    "print(\"avg testing loss\", avg_testing_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[ 2.3764e-05,  3.3809e-04,  3.2502e-04, -2.8780e-04]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "1 tensor([[0.2360, 0.1437, 0.1541, 0.2514]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "2 tensor([[0.7056, 0.4276, 0.4606, 0.7500]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "3 tensor([[0.4696, 0.2847, 0.3064, 0.4999]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "4 tensor([[0.7646, 0.8574, 0.8461, 0.7499]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "5 tensor([[1.0003, 0.9987, 0.9995, 0.9993]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "total testing loss tensor(5.7653e-07, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = iter(data_loader).next()[0]\n",
    "enc_fold = torch_f.Fold(device)\n",
    "enc_fold_nodes = []\n",
    "enc_fold_nodes.append(encode_structure_fold(enc_fold, input))\n",
    "enc_fold_nodes = enc_fold.apply(encoder, [enc_fold_nodes])\n",
    "encoded = enc_fold_nodes[0]\n",
    "decoded = decode_testing_grass(encoded, input, 100, decoder)\n",
    "\n",
    "count = []\n",
    "numerar_nodos(decoded, count)\n",
    "decoded.traverseInorder(decoded)\n",
    "c = []\n",
    "n_nodes = len(input.count_nodes(input,c))\n",
    "\n",
    "l = []\n",
    "mse_loss_list = decoded.traverseInorderMSE(decoded, l)\n",
    "l = []\n",
    "ce_loss_list = decoded.traverseInorderCE(decoded, l)\n",
    "mse_loss_list = [m for m in mse_loss_list if m is not None]\n",
    "ce_loss_list = [c for c in ce_loss_list if c is not None]\n",
    "mse_loss = sum(mse_loss_list) \n",
    "ce_loss  = sum(ce_loss_list)  \n",
    "total_loss = (0.4*ce_loss + mse_loss)/n_nodes\n",
    "print(\"total testing loss\", total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9387daf501941ab98bd655073de2b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.5, 0.5,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTree(input, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b900d2f6873240e0907c58afd3932eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.5001527â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTree(decoded, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "n_nodes = input.count_nodes(input,c)\n",
    "len(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "n_nodes = decoded.count_nodes(decoded,c)\n",
    "len(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 1\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "decoded.traverseInorderChilds(decoded, li)\n",
    "zero = [a for a in li if a == 0]\n",
    "one = [a for a in li if a == 1]\n",
    "two = [a for a in li if a == 2]\n",
    "qzero = len(zero)\n",
    "qOne = len(one)\n",
    "qtwo = len(two)\n",
    "print(qzero, qOne, qtwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3 1\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "input.traverseInorderChilds(input, li)\n",
    "zero = [a for a in li if a == 0]\n",
    "one = [a for a in li if a == 1]\n",
    "two = [a for a in li if a == 2]\n",
    "qzero = len(zero)\n",
    "qOne = len(one)\n",
    "qtwo = len(two)\n",
    "print(qzero, qOne, qtwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (20,10))\n",
    "plt.plot(train_loss_avg) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverseleaf(root):\n",
    "    if root is not None:\n",
    "        traverseleaf(root.left)\n",
    "        if root.is_leaf():\n",
    "            print(root.radius)\n",
    "        traverseleaf(root.right)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traversebif(root):\n",
    "    if root is not None:\n",
    "        traversebif(root.left)\n",
    "        if root.is_two_child():\n",
    "            print(root.radius)\n",
    "        traversebif(root.right)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2354, 0.1424, 0.1534, 0.2508]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "////\n",
      "tensor([0.2353, 0.1429, 0.1538, 0.2500], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "traversebif(decoded)\n",
    "print(\"////\")\n",
    "traversebif(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4701, 0.2845, 0.3064, 0.5001]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5303, 0.7124, 0.6917, 0.4980]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "traverseleaf(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9412, 0.5714, 0.6154, 1.0000], device='cuda:0')\n",
      "tensor([0.4706, 0.2857, 0.3077, 0.5000], device='cuda:0')\n",
      "tensor([1., 1., 1., 1.], device='cuda:0')\n",
      "tensor([0.5294, 0.7143, 0.6923, 0.5000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "traverseleaf(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py_torc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f3e717cd274da89498094fde320e6eab1bf0f52911d27cf47473187acb3fe8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
