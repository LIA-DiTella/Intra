{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import raiseExceptions\n",
    "from tokenize import Double\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from vec3 import Vec3\n",
    "import meshplot as mp\n",
    "import torch\n",
    "torch.manual_seed(125)\n",
    "import random\n",
    "random.seed(125)\n",
    "import torch_f as torch_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_fn(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        wrapper.count += 1\n",
    "        return f(*args, **kwargs)\n",
    "    wrapper.count = 0\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase nodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Class Node\n",
    "    \"\"\"\n",
    "    def __init__(self, value, radius, left = None, right = None, position = None, cl_prob= None, ce = None, mse = None, level = None, treelevel = None):\n",
    "        self.left = left\n",
    "        self.data = value\n",
    "        self.radius = radius\n",
    "        self.position = position\n",
    "        self.right = right\n",
    "        self.prob = cl_prob\n",
    "        self.mse = mse\n",
    "        self.ce = ce\n",
    "        self.children = [self.left, self.right]\n",
    "        self.level = level\n",
    "        self.treelevel = treelevel\n",
    "    \n",
    "    def agregarHijo(self, children):\n",
    "\n",
    "        if self.right is None:\n",
    "            self.right = children\n",
    "        elif self.left is None:\n",
    "            self.left = children\n",
    "\n",
    "        else:\n",
    "            raise ValueError (\"solo arbol binario \")\n",
    "\n",
    "\n",
    "    def is_leaf(self):\n",
    "        if self.right is None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_two_child(self):\n",
    "        if self.right is not None and self.left is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_one_child(self):\n",
    "        if self.is_two_child():\n",
    "            return False\n",
    "        elif self.is_leaf():\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def childs(self):\n",
    "        if self.is_leaf():\n",
    "            return 0\n",
    "        if self.is_one_child():\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    \n",
    "    \n",
    "    def traverseInorder(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorder(root.left)\n",
    "            print (root.data, root.radius)\n",
    "            self.traverseInorder(root.right)\n",
    "\n",
    "    def traverseInorderwl(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderwl(root.left)\n",
    "            print (root.data, root.radius, root.level, root.treelevel)\n",
    "            self.traverseInorderwl(root.right)\n",
    "\n",
    "    def get_tree_level(self, root, c):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.get_tree_level(root.left, c)\n",
    "            c.append(root.level)\n",
    "            self.get_tree_level(root.right, c)\n",
    "\n",
    "    def set_tree_level(self, root, c):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.set_tree_level(root.left, c)\n",
    "            root.treelevel = c\n",
    "            self.set_tree_level(root.right, c)\n",
    "\n",
    "    def traverseInorderLoss(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderLoss(root.left, loss)\n",
    "            loss.append(root.prob)\n",
    "            self.traverseInorderLoss(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderMSE(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderMSE(root.left, loss)\n",
    "            loss.append(root.mse)\n",
    "            self.traverseInorderMSE(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderCE(self, root, loss):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderCE(root.left, loss)\n",
    "            loss.append(root.ce)\n",
    "            self.traverseInorderCE(root.right, loss)\n",
    "            return loss\n",
    "\n",
    "    def traverseInorderChilds(self, root, l):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            self.traverseInorderChilds(root.left, l)\n",
    "            l.append(root.childs())\n",
    "            self.traverseInorderChilds(root.right, l)\n",
    "            return l\n",
    "\n",
    "    def preorder(self, root):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            print (root.data, root.radius)\n",
    "            self.preorder(root.left)\n",
    "            self.preorder(root.right)\n",
    "\n",
    "    def cloneBinaryTree(self, root):\n",
    "     \n",
    "        # base case\n",
    "        if root is None:\n",
    "            return None\n",
    "    \n",
    "        # create a new node with the same data as the root node\n",
    "        root_copy = Node(root.data, root.radius)\n",
    "    \n",
    "        # clone the left and right subtree\n",
    "        root_copy.left = self.cloneBinaryTree(root.left)\n",
    "        root_copy.right = self.cloneBinaryTree(root.right)\n",
    "    \n",
    "        # return cloned root node\n",
    "        return root_copy\n",
    "\n",
    "    def height(self, root):\n",
    "    # Check if the binary tree is empty\n",
    "        if root is None:\n",
    "            return 0 \n",
    "        # Recursively call height of each node\n",
    "        leftAns = self.height(root.left)\n",
    "        rightAns = self.height(root.right)\n",
    "    \n",
    "        # Return max(leftHeight, rightHeight) at each iteration\n",
    "        return max(leftAns, rightAns) + 1\n",
    "\n",
    "    # Print nodes at a current level\n",
    "    def printCurrentLevel(self, root, level):\n",
    "        if root is None:\n",
    "            return\n",
    "        if level == 1:\n",
    "            print(root.data, end=\" \")\n",
    "        elif level > 1:\n",
    "            self.printCurrentLevel(root.left, level-1)\n",
    "            self.printCurrentLevel(root.right, level-1)\n",
    "\n",
    "    def printLevelOrder(self, root):\n",
    "        h = self.height(root)\n",
    "        for i in range(1, h+1):\n",
    "            self.printCurrentLevel(root, i)\n",
    "\n",
    "\n",
    "    \n",
    "    def count_nodes(self, root, counter):\n",
    "        if   root is not None:\n",
    "            self.count_nodes(root.left, counter)\n",
    "            counter.append(root.data)\n",
    "            self.count_nodes(root.right, counter)\n",
    "            return counter\n",
    "\n",
    "    \n",
    "    def serialize(self, root):\n",
    "        def post_order(root):\n",
    "            if root:\n",
    "                post_order(root.left)\n",
    "                post_order(root.right)\n",
    "                ret[0] += str(root.data)+'_'+ str(root.radius) +';'\n",
    "                \n",
    "            else:\n",
    "                ret[0] += '#;'           \n",
    "\n",
    "        ret = ['']\n",
    "        post_order(root)\n",
    "        return ret[0][:-1]  # remove last ,\n",
    "\n",
    "    def toGraph( self, graph, index, dec, proc=True):\n",
    "        \n",
    "        \n",
    "        radius = self.radius.cpu().detach().numpy()\n",
    "        if dec:\n",
    "            radius= radius[0]\n",
    "        #print(\"posicion\", self.data, radius)\n",
    "        #print(\"right\", self.right)\n",
    "        \n",
    "        #graph.add_nodes_from( [ (index, {'posicion': radius[0:3], 'radio': radius[3] } ) ])\n",
    "        graph.add_nodes_from( [ (self.data, {'posicion': radius[0:3], 'radio': radius[3] } ) ])\n",
    "        \n",
    "\n",
    "        if self.right is not None:\n",
    "            #leftIndex = self.right.toGraph( graph, index + 1, dec)#\n",
    "            self.right.toGraph( graph, index + 1, dec)#\n",
    "            \n",
    "            #graph.add_edge( index, index + 1 )\n",
    "            graph.add_edge( self.data, self.right.data )\n",
    "            #if proc:\n",
    "            #    nx.set_edge_attributes( graph, {(index, index+1) : {'procesada':False}})\n",
    "        \n",
    "            if self.left is not None:\n",
    "                #retIndex = self.left.toGraph( graph, leftIndex, dec )#\n",
    "                self.left.toGraph( graph, 0, dec )#\n",
    "\n",
    "                #graph.add_edge( index, leftIndex)\n",
    "                graph.add_edge( self.data, self.left.data)\n",
    "                #if proc:\n",
    "                #    nx.set_edge_attributes( graph, {(index, leftIndex) : {'procesada':False}})\n",
    "            \n",
    "            else:\n",
    "                #return leftIndex\n",
    "                return\n",
    "\n",
    "        else:\n",
    "            #return index + 1\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTree( root, dec ):\n",
    "    graph = nx.Graph()\n",
    "    root.toGraph( graph, 0, dec)\n",
    "    edges=nx.get_edge_attributes(graph,'procesada')\n",
    "\n",
    "    p = mp.plot( np.array([ graph.nodes[v]['posicion'] for v in graph.nodes]), shading={'point_size':0.1}, return_plot=True)\n",
    "\n",
    "    for arista in graph.edges:\n",
    "        p.add_lines( graph.nodes[arista[0]]['posicion'], graph.nodes[arista[1]]['posicion'])\n",
    "\n",
    "    return \n",
    "\n",
    "def traverse(root, tree):\n",
    "       \n",
    "        if root is not None:\n",
    "            traverse(root.left, tree)\n",
    "            tree.append((root.radius, root.data))\n",
    "            traverse(root.right, tree)\n",
    "            return tree\n",
    "\n",
    "def traverse_2(tree1, tree2, t_l):\n",
    "       \n",
    "        if tree1 is not None:\n",
    "            traverse_2(tree1.left, tree2.left, t_l)\n",
    "            if tree2:\n",
    "                t_l.append((tree1.radius, tree2.radius))\n",
    "                print((tree1.radius, tree2.radius))\n",
    "            else:\n",
    "                t_l.append(tree1.radius)\n",
    "                print((tree1.radius))\n",
    "            traverse_2(tree1.right, tree2, t_l)\n",
    "            return t_l\n",
    "            \n",
    "\n",
    "def traverse_conexiones(root, tree):\n",
    "        \"\"\"\n",
    "        traverse function will print all the node in the tree.\n",
    "        \"\"\"\n",
    "        if root is not None:\n",
    "            traverse_conexiones(root.left, tree)\n",
    "            if root.right is not None:\n",
    "                tree.append((root.data, root.right.data))\n",
    "            if root.left is not None:\n",
    "                tree.append((root.data, root.left.data))\n",
    "            traverse_conexiones(root.right, tree)\n",
    "            return tree\n",
    "\n",
    "def arbolAGrafo (nodoRaiz):\n",
    "    \n",
    "    conexiones = []\n",
    "    lineas = traverse_conexiones(nodoRaiz, conexiones)\n",
    "    tree = []\n",
    "    tree = traverse(nodoRaiz, tree)\n",
    "\n",
    "    vertices = []\n",
    "    verticesCrudos = []\n",
    "    for node in tree:\n",
    "        vertice = node[0][0][:3]\n",
    "        rad = node[0][0][-1]\n",
    "        num = node[1]\n",
    "        \n",
    "        #vertices.append((num, {'posicion': Vec3( vertice[0], vertice[1], vertice[2]), 'radio': rad} ))\n",
    "        vertices.append((len(verticesCrudos),{'posicion': Vec3( vertice[0], vertice[1], vertice[2]), 'radio': rad}))\n",
    "        verticesCrudos.append(vertice)\n",
    "\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from( vertices )\n",
    "    G.add_edges_from( lineas )\n",
    "    \n",
    "    return G\n",
    "\n",
    "@count_fn\n",
    "def createNode(data, radius, position = None, left = None, right = None, cl_prob = None, ce = None, mse=None):\n",
    "        \"\"\"\n",
    "        Utility function to create a node.\n",
    "        \"\"\"\n",
    "        return Node(data, radius, position, left, right, cl_prob, ce, mse)\n",
    " \n",
    "def deserialize(data):\n",
    "    if  not data:\n",
    "        return \n",
    "    nodes = data.split(';')  \n",
    "    #print(\"node\",nodes[3])\n",
    "    def post_order(nodes):\n",
    "                \n",
    "        if nodes[-1] == '#':\n",
    "            nodes.pop()\n",
    "            return None\n",
    "        node = nodes.pop().split('_')\n",
    "        data = int(node[0])\n",
    "        #radius = float(node[1])\n",
    "        #print(\"node\", node)\n",
    "        #breakpoint()\n",
    "        radius = node[1]\n",
    "        #print(\"radius\", radius)\n",
    "        rad = radius.split(\",\")\n",
    "        rad [0] = rad[0].replace('[','')\n",
    "        rad [3] = rad[3].replace(']','')\n",
    "        r = []\n",
    "        for value in rad:\n",
    "            r.append(float(value))\n",
    "        #r =[float(num) for num in radius if num.isdigit()]\n",
    "        r = torch.tensor(r, device=device)\n",
    "        #breakpoint()\n",
    "        root = createNode(data, r)\n",
    "        root.right = post_order(nodes)\n",
    "        root.left = post_order(nodes)\n",
    "        \n",
    "        return root    \n",
    "    return post_order(nodes)    \n",
    "\n",
    "\n",
    "def read_tree(filename):\n",
    "    with open('./trees/' +filename, \"r\") as f:\n",
    "        byte = f.read() \n",
    "        return byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternalEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, feature_size: int, hidden_size: int):\n",
    "        super(InternalEncoder, self).__init__()\n",
    "\n",
    "        #print(\"init\")\n",
    "        # Encoders atributos\n",
    "        self.attribute_lin_encoder_1 = nn.Linear(input_size,hidden_size)\n",
    "        self.attribute_lin_encoder_2 = nn.Linear(hidden_size,feature_size)\n",
    "\n",
    "        # Encoders derecho e izquierdo\n",
    "        self.right_lin_encoder_1 = nn.Linear(feature_size,feature_size)\n",
    "        self.left_lin_encoder_1  = nn.Linear(feature_size,feature_size)\n",
    "\n",
    "        # Encoder final\n",
    "        self.final_lin_encoder_1 = nn.Linear(2*feature_size, feature_size)\n",
    "\n",
    "        # Funciones / Parametros utiles\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.feature_size = feature_size\n",
    "        #print(\"fin\")\n",
    "\n",
    "    def forward(self, input, right_input, left_input):\n",
    "        #print(\"estoy encodeando\")\n",
    "        #print(\"input\", input)\n",
    "        # Encodeo los atributos\n",
    "        attributes = self.attribute_lin_encoder_1(input)\n",
    "        attributes = self.tanh(attributes)\n",
    "        attributes = self.attribute_lin_encoder_2(attributes)\n",
    "        attributes = self.tanh(attributes)\n",
    "        #print(\"attributes\", attributes)\n",
    "\n",
    "        # Encodeo el derecho\n",
    "        if right_input is not None:\n",
    "            #print(\"right input\", right_input)\n",
    "            context = self.right_lin_encoder_1(right_input)\n",
    "            #print(\"context derecho\", context)\n",
    "            # Encodeo el izquierdo\n",
    "            #print(\"left input\", left_input)\n",
    "            if left_input is not None:\n",
    "                \n",
    "                context += self.left_lin_encoder_1(left_input)\n",
    "                #print(\"context izquierdo\", context.shape)\n",
    "        else:\n",
    "            context = torch.zeros(input.shape[0],self.feature_size, requires_grad=True, device=device)\n",
    "        \n",
    "\n",
    "        context = self.tanh(context)\n",
    "        #print(\"context shape\", context.shape)\n",
    "        #print(\"attributes shape\", attributes.shape)\n",
    "        #if len(attributes.shape) == 1:\n",
    "            #print(\"len(attributes.shape)\",len(attributes.shape))\n",
    "            #attributes = attributes.reshape(1, 128)\n",
    "        #print(\"attributes shape\", attributes.shape)\n",
    "\n",
    "        feature = torch.cat((attributes,context), 1)\n",
    "        #print(\"feature cat\", feature.shape)\n",
    "\n",
    "        feature = self.final_lin_encoder_1(feature)\n",
    "        feature = self.tanh(feature)\n",
    "        #print(\"output\", feature)\n",
    "        return feature\n",
    "\n",
    "        #print(\"radius\", radius.shape)\n",
    "        #if right_input is not None:\n",
    "        #    context = self.right(right_input)\n",
    "        #    #print(\"context\", context.shape)\n",
    "        #    if left_input is not None:\n",
    "        #        context += self.left(left_input)\n",
    "        #        #print(\"context2\", context.shape)\n",
    "        #    context = self.tanh(context)\n",
    "        #    #print(\"context3\", context.shape)\n",
    "        #    feature = torch.cat((radius,context), 1)\n",
    "        #    #print(\"feature\", feature.shape)\n",
    "        #    feature = self.encoder(feature)\n",
    "        #else:\n",
    "        #    feature = self.l3(radius)\n",
    "        #    feature = self.tanh(feature)\n",
    "        #    feature = self.leafencoder(radius)\n",
    "        #    #print(\"feature\", feature.shape)\n",
    "\n",
    "        #feature = self.tanh(feature)\n",
    "        #return feature\n",
    "    \n",
    "\n",
    "class GRASSEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, feature_size : int, hidden_size: int):\n",
    "        super(GRASSEncoder, self).__init__()\n",
    "        self.leaf_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        self.internal_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        self.bifurcation_encoder = InternalEncoder(input_size,feature_size, hidden_size)\n",
    "        \n",
    "    def leafEncoder(self, node, right=None, left = None):\n",
    "        return self.internal_encoder(node, right, left)\n",
    "    def internalEncoder(self, node, right, left = None):\n",
    "        return self.internal_encoder(node, right, left)\n",
    "    def bifurcationEncoder(self, node, right, left):\n",
    "        \n",
    "        return self.bifurcation_encoder(node, right, left)\n",
    "\n",
    "Grassencoder = GRASSEncoder(input_size = 4, feature_size=128, hidden_size=256)\n",
    "Grassencoder = Grassencoder.to(device)\n",
    "\n",
    "\n",
    "def encode_structure_fold(fold, root):\n",
    "    \n",
    "    \n",
    "    def encode_node(node):\n",
    "        \n",
    "        if node is None:\n",
    "            return\n",
    "        \n",
    "        if node.is_leaf():\n",
    "            return fold.add('leafEncoder', node.radius)\n",
    "        else:\n",
    "            left = encode_node(node.left)\n",
    "            right = encode_node(node.right)\n",
    "            if left is not None:\n",
    "             \n",
    "                return fold.add('bifurcationEncoder', node.radius, right, left)\n",
    "            else:\n",
    "                return fold.add('internalEncoder', node.radius, right)\n",
    "        \n",
    "\n",
    "    encoding = encode_node(root)\n",
    "    \n",
    "    return encoding\n",
    "  \n",
    "def encode_structure(root):\n",
    "    \n",
    "    def encode_node(node):\n",
    "          \n",
    "        if node is None:\n",
    "            return\n",
    "        if node.is_leaf():\n",
    "            return Grassencoder.leafEncoder(node.radius.reshape(-1,4))\n",
    "        else :\n",
    "            left = encode_node(node.left)\n",
    "            right = encode_node(node.right)\n",
    "            if left is not None:\n",
    "                return Grassencoder.bifurcationEncoder(node.radius.reshape(-1,4), right, left)\n",
    "            else:\n",
    "                return Grassencoder.internalEncoder(node.radius.reshape(-1,4), right)\n",
    "        \n",
    "\n",
    "    encoding = encode_node(root)\n",
    "   \n",
    "    return encoding\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerar_nodos(root, count):\n",
    "    if root is not None:\n",
    "        numerar_nodos(root.left, count)\n",
    "        root.data = len(count)\n",
    "        count.append(1)\n",
    "        numerar_nodos(root.right, count)\n",
    "        return \n",
    "\n",
    "\n",
    "def traversefeatures(root, features):\n",
    "       \n",
    "    if root is not None:\n",
    "        traversefeatures(root.left, features)\n",
    "        features.append(root.radius)\n",
    "        traversefeatures(root.right, features)\n",
    "        return features\n",
    "\n",
    "def norm(root, minx, miny, minz, minr, maxx, maxy, maxz, maxr):\n",
    "    \n",
    "    if root is not None:\n",
    "        mx = minx.clone().detach()\n",
    "        my = miny.clone().detach()\n",
    "        mz = minz.clone().detach()\n",
    "        mr = minr.clone().detach()\n",
    "        Mx = maxx.clone().detach()\n",
    "        My = maxy.clone().detach()\n",
    "        Mz = maxz.clone().detach()\n",
    "        Mr = maxr.clone().detach()\n",
    "       \n",
    "        root.radius[0] = (root.radius[0] - minx)/(maxx - minx)\n",
    "        root.radius[1] = (root.radius[1] - miny)/(maxy - miny)\n",
    "        root.radius[2] = (root.radius[2] - minz)/(maxz - minz)\n",
    "        root.radius[3] = (root.radius[3] - minr)/(maxr - minr)\n",
    "        \n",
    "        norm(root.left, mx, my, mz, mr, Mx, My, Mz, Mr)\n",
    "        norm(root.right, mx, my, mz, mr, Mx, My, Mz, Mr)\n",
    "        return \n",
    "\n",
    "def normalize_features(root):\n",
    "    features = []\n",
    "    features = traversefeatures(root, features)\n",
    "    \n",
    "    x = [tensor[0] for tensor in features]\n",
    "    y = [tensor[1] for tensor in features]\n",
    "    z = [tensor[2] for tensor in features]\n",
    "    r = [tensor[3] for tensor in features]\n",
    " \n",
    "    norm(root, min(x), min(y), min(z), min(r), max(x), max(y), max(z), max(r))\n",
    "\n",
    "    return \n",
    "\n",
    "def traversefeatures(root, features):\n",
    "       \n",
    "    if root is not None:\n",
    "        traversefeatures(root.left, features)\n",
    "        features.append(root.radius)\n",
    "        traversefeatures(root.right, features)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ArteryObjAN1-17.dat']\n"
     ]
    }
   ],
   "source": [
    "def my_collate(batch):\n",
    "    return batch\n",
    "\n",
    "#t_list = ['ArteryObjAN1-7.dat','ArteryObjAN1-0.dat', 'ArteryObjAN1-17.dat',  'ArteryObjAN1-11.dat']\n",
    "\n",
    "#t_list = ['ArteryObjAN1-0.dat','ArteryObjAN1-7.dat', 'ArteryObjAN1-17.dat',  'ArteryObjAN1-11.dat', 'ArteryObjAN1-19.dat', 'ArteryObjAN2-4.dat', 'ArteryObjAN2-6.dat', \n",
    "#           'ArteryObjAN25-18.dat']\n",
    "t_list = ['ArteryObjAN1-17.dat']\n",
    "#t_list = ['test2.dat']\n",
    "\n",
    "#t_list = ['ArteryObjAN31-14.dat']\n",
    "#t_list = os.listdir(\"./trees\")[:20]\n",
    "print(t_list)\n",
    "class tDataset(Dataset):\n",
    "    def __init__(self, dir, transform=None):\n",
    "        self.names = dir\n",
    "        self.transform = transform\n",
    "        self.data = [] #lista con las strings de todos los arboles\n",
    "        for file in self.names:\n",
    "            self.data.append(read_tree(file))\n",
    "        self.trees = []\n",
    "        for tree in self.data:\n",
    "            deserial = deserialize(tree)\n",
    "            normalize_features(deserial)\n",
    "            self.trees.append(deserial)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #file = self.names[idx]\n",
    "        #string = read_tree(file)\n",
    "        tree = self.trees[idx]\n",
    "        return tree\n",
    "\n",
    "batch_size = 1\n",
    "dataset = tDataset(t_list)\n",
    "data_loader = DataLoader(dataset, batch_size = batch_size, shuffle=True, collate_fn=my_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88962e58fae44a228eb4f02f73a18cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.5, 0.5,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input = iter(data_loader).next()[0]\n",
    "plotTree(input, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED con batch [tensor([[ 1.7088e-01,  5.7667e-02, -1.6756e-02, -1.2504e-02, -4.9982e-02,\n",
      "          6.8105e-02, -2.3267e-02, -7.2470e-03,  9.0715e-02,  1.4259e-01,\n",
      "          5.5995e-02, -8.2506e-02, -5.3420e-02, -5.8751e-02,  1.9328e-01,\n",
      "         -1.1576e-01, -1.9203e-02,  3.2975e-02, -8.0903e-03,  1.1759e-01,\n",
      "          1.7988e-01, -1.5306e-02, -9.0030e-02, -9.0971e-02,  7.2639e-02,\n",
      "          1.7016e-02, -6.9104e-02,  1.5200e-03, -1.1797e-01, -5.8712e-02,\n",
      "          1.2983e-02, -4.1682e-02,  3.3190e-02,  1.3802e-01,  8.3385e-02,\n",
      "          3.0466e-02, -4.8336e-02,  7.3928e-02, -8.3052e-02, -6.8093e-02,\n",
      "          1.5390e-02,  7.3295e-02,  1.9098e-01, -3.0443e-02,  4.9926e-02,\n",
      "         -1.0355e-01, -1.2399e-01, -1.5295e-04, -3.3392e-02,  1.6384e-02,\n",
      "          5.1597e-02, -5.4770e-02,  2.6982e-02, -5.0428e-04,  5.4312e-02,\n",
      "          9.2675e-02,  7.1426e-02,  1.0998e-01,  4.2531e-02, -7.8952e-02,\n",
      "          3.0379e-02, -6.0322e-02,  3.1251e-02, -1.1379e-01, -1.2618e-01,\n",
      "          1.7883e-02,  9.8987e-02,  1.2022e-01,  2.0115e-01,  8.5639e-02,\n",
      "         -3.7920e-02, -1.4083e-01,  1.8620e-01, -5.1934e-02,  6.3612e-02,\n",
      "          1.6647e-01,  9.8487e-02,  9.6793e-04,  6.5733e-02,  8.9749e-02,\n",
      "         -6.5316e-02,  1.1851e-02,  1.4960e-01, -1.2662e-01, -1.5223e-02,\n",
      "         -7.8190e-02,  9.9393e-02, -1.7278e-01, -4.6147e-02,  1.9487e-02,\n",
      "          7.0930e-02, -4.9623e-02,  7.1386e-02,  1.3412e-01,  5.2796e-03,\n",
      "          3.0244e-02, -3.0122e-03, -1.4469e-02, -1.8648e-02, -6.5253e-02,\n",
      "          4.7816e-02, -1.0109e-01,  8.1684e-02, -4.8753e-02, -1.8414e-02,\n",
      "          2.6165e-02, -2.6642e-02,  6.8751e-02,  1.4296e-01,  9.5672e-02,\n",
      "          1.0816e-01,  5.2439e-02, -9.3278e-02,  5.8254e-03,  4.7443e-02,\n",
      "          9.8460e-04, -5.2663e-04,  5.1282e-02,  1.2290e-01,  6.6048e-02,\n",
      "         -9.3475e-02,  6.2717e-02,  1.3403e-01,  3.9112e-02,  3.3562e-02,\n",
      "         -1.5922e-01,  3.7014e-03, -6.3041e-02]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "import torch_f\n",
    "enc_fold = torch_f.Fold(device)\n",
    "enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "batch = iter(data_loader).next()\n",
    "for example in batch:\n",
    "        #enc_fold.add('leafEncoder', example.radius)\n",
    "        #enc_fold_nodes.append(enc_fold.add('leafEncoder', example.radius))\n",
    "        enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "\n",
    "        #print(\"enc fold nodes\", enc_fold)\n",
    "enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "print(\"ENCODED con batch\",enc_fold_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encodeado sin batch tensor([[ 1.7088e-01,  5.7667e-02, -1.6756e-02, -1.2504e-02, -4.9982e-02,\n",
      "          6.8105e-02, -2.3267e-02, -7.2470e-03,  9.0715e-02,  1.4259e-01,\n",
      "          5.5995e-02, -8.2506e-02, -5.3420e-02, -5.8751e-02,  1.9328e-01,\n",
      "         -1.1576e-01, -1.9203e-02,  3.2975e-02, -8.0903e-03,  1.1759e-01,\n",
      "          1.7988e-01, -1.5306e-02, -9.0030e-02, -9.0971e-02,  7.2639e-02,\n",
      "          1.7016e-02, -6.9104e-02,  1.5200e-03, -1.1797e-01, -5.8712e-02,\n",
      "          1.2983e-02, -4.1682e-02,  3.3190e-02,  1.3802e-01,  8.3385e-02,\n",
      "          3.0466e-02, -4.8336e-02,  7.3928e-02, -8.3052e-02, -6.8093e-02,\n",
      "          1.5390e-02,  7.3295e-02,  1.9098e-01, -3.0443e-02,  4.9926e-02,\n",
      "         -1.0355e-01, -1.2399e-01, -1.5295e-04, -3.3392e-02,  1.6384e-02,\n",
      "          5.1597e-02, -5.4770e-02,  2.6982e-02, -5.0428e-04,  5.4312e-02,\n",
      "          9.2675e-02,  7.1426e-02,  1.0998e-01,  4.2531e-02, -7.8952e-02,\n",
      "          3.0379e-02, -6.0322e-02,  3.1251e-02, -1.1379e-01, -1.2618e-01,\n",
      "          1.7883e-02,  9.8987e-02,  1.2022e-01,  2.0115e-01,  8.5639e-02,\n",
      "         -3.7920e-02, -1.4083e-01,  1.8620e-01, -5.1934e-02,  6.3612e-02,\n",
      "          1.6647e-01,  9.8487e-02,  9.6793e-04,  6.5733e-02,  8.9749e-02,\n",
      "         -6.5316e-02,  1.1851e-02,  1.4960e-01, -1.2662e-01, -1.5223e-02,\n",
      "         -7.8190e-02,  9.9393e-02, -1.7278e-01, -4.6147e-02,  1.9487e-02,\n",
      "          7.0930e-02, -4.9623e-02,  7.1386e-02,  1.3412e-01,  5.2796e-03,\n",
      "          3.0244e-02, -3.0122e-03, -1.4469e-02, -1.8648e-02, -6.5253e-02,\n",
      "          4.7816e-02, -1.0109e-01,  8.1684e-02, -4.8753e-02, -1.8414e-02,\n",
      "          2.6165e-02, -2.6642e-02,  6.8751e-02,  1.4296e-01,  9.5672e-02,\n",
      "          1.0816e-01,  5.2439e-02, -9.3278e-02,  5.8254e-03,  4.7443e-02,\n",
      "          9.8460e-04, -5.2663e-04,  5.1282e-02,  1.2290e-01,  6.6048e-02,\n",
      "         -9.3475e-02,  6.2717e-02,  1.3403e-01,  3.9112e-02,  3.3562e-02,\n",
      "         -1.5922e-01,  3.7014e-03, -6.3041e-02]], device='cuda:0',\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for data in data_loader:\n",
    "    data = data[0]\n",
    "\n",
    "    enc_f = encode_structure(data).to(device)\n",
    "\n",
    "print(\"encodeado sin batch\", enc_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_fold_nodes[0]-enc_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[70]\n",
      "70.0\n",
      "3.0\n",
      "65.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "n_no = []\n",
    "qzero = 0\n",
    "qOne = 0\n",
    "qtwo = 0\n",
    "\n",
    "for batch in data_loader:\n",
    "    for tree in batch:\n",
    "        count = []\n",
    "        n = tree.count_nodes(tree, count)\n",
    "        n_no.append(len(n))\n",
    "        li = []\n",
    "        tree.traverseInorderChilds(tree, li)\n",
    "        zero = [a for a in li if a == 0]\n",
    "        one = [a for a in li if a == 1]\n",
    "        two = [a for a in li if a == 2]\n",
    "        qzero += len(zero)\n",
    "        qOne += len(one)\n",
    "        qtwo += len(two)\n",
    "\n",
    "print(len(data_loader)*batch_size)\n",
    "print(n_no)\n",
    "nprom = np.mean(n_no)\n",
    "print(nprom)\n",
    "qzero /= len(data_loader)*batch_size\n",
    "qOne /= len(data_loader)*batch_size\n",
    "qtwo /= len(data_loader)*batch_size\n",
    "\n",
    "print(qzero)\n",
    "print(qOne)\n",
    "print(qtwo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en el loop creo un fold, mando este fold con cada uno de los arboles del batch a encode_structure_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.mlp1 = nn.Linear(latent_size, hidden_size)\n",
    "        self.mlp2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.mlp3 = nn.Linear(hidden_size, 3)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_feature):\n",
    "        #print(\"classifier input\", input_feature)\n",
    "        output = self.mlp1(input_feature)\n",
    "        output = self.tanh(output)\n",
    "        output = self.mlp2(output)\n",
    "        output = self.tanh(output)\n",
    "        output = self.mlp3(output)\n",
    "        #print(\"classifier output\", output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class InternalDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size: int):\\n        super(InternalDecoder, self).__init__()\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.lp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp_right = nn.Linear(latent_size, latent_size)\\n        self.tanh = nn.Tanh()\\n        self.mlp2 = nn.Linear(latent_size,4)\\n\\n    def forward(self, parent_feature):\\n        #print(\"internal decoder\")\\n        #print(\"input\", parent_feature.shape)\\n        vector = self.mlp(parent_feature)\\n        vector = self.tanh(vector)\\n        vector = self.lp2(vector)\\n        vector = self.tanh(vector)\\n        right_feature = self.mlp_right(vector)\\n        right_feature = self.tanh(right_feature)\\n        rad_feature = self.mlp2(vector)\\n\\n        return right_feature, rad_feature\\n\\nclass BifurcationDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size : int):\\n        super(BifurcationDecoder, self).__init__()\\n        #self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.lp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp_left = nn.Linear(latent_size, latent_size)\\n        self.mlp_right = nn.Linear(latent_size, latent_size)\\n        self.mlp2 = nn.Linear(latent_size,4)\\n        self.tanh = nn.Tanh()\\n\\n    def forward(self, parent_feature):\\n        #print(\"bifurcation decoder input\", parent_feature.shape)\\n        parent_feature = parent_feature.reshape(-1,128)\\n        #print(\"bifurcation decoder input\", parent_feature.shape)\\n        vector = self.mlp(parent_feature)\\n        #print(\"v1\", vector.shape)\\n        vector = self.tanh(vector)\\n        #print(\"v2\", vector.shape)\\n        vector = self.lp2(vector)\\n        #print(\"v3\", vector.shape)\\n        vector = self.tanh(vector)\\n        left_feature = self.mlp_left(vector)\\n        left_feature = self.tanh(left_feature)\\n        right_feature = self.mlp_right(vector)\\n        right_feature = self.tanh(right_feature)\\n        rad_feature = self.mlp2(vector)\\n        #print(\"exiting bif dec\")\\n        return left_feature, right_feature, rad_feature\\n\\n\\nclass featureDecoder(nn.Module):\\n    \\n    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\\n    def __init__(self, latent_size : int, hidden_size: int):\\n        super(featureDecoder, self).__init__()\\n        self.mlp = nn.Linear(latent_size,hidden_size)\\n        self.mlp2 = nn.Linear(hidden_size, latent_size)\\n        self.mlp3 = nn.Linear(latent_size, latent_size)\\n        self.tanh = nn.Tanh()\\n        self.mlp4 = nn.Linear(latent_size,4)\\n\\n    def forward(self, parent_feature):\\n        #print(\"feature decoder input\", parent_feature.shape)\\n\\n        vector = self.mlp(parent_feature)\\n        vector = self.tanh(vector)\\n        vector = self.mlp2(vector)\\n        vector = self.tanh(vector)\\n        vector = self.mlp3(vector)\\n        vector = self.tanh(vector)\\n        vector = self.mlp4(vector)\\n       \\n        return vector\\n\\n\\n\\nclass GRASSDecoder(nn.Module):\\n    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\\n        super(GRASSDecoder, self).__init__()\\n        self.feature_decoder = featureDecoder(latent_size, hidden_size)\\n        self.internal_decoder = InternalDecoder(latent_size, hidden_size)\\n        self.bifurcation_decoder = BifurcationDecoder(latent_size, hidden_size)\\n        self.node_classifier = NodeClassifier(latent_size, hidden_size)\\n        self.mseLoss = nn.MSELoss()  # pytorch\\'s mean squared error loss\\n        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch\\'s cross entropy loss (NOTE: no softmax is needed before)\\n\\n    def featureDecoder(self, feature):\\n        return self.feature_decoder(feature)\\n\\n    def internalDecoder(self, feature):\\n        return self.internal_decoder(feature)\\n\\n    def bifurcationDecoder(self, feature):\\n        return self.bifurcation_decoder(feature)\\n\\n    def nodeClassifier(self, feature):\\n        return self.node_classifier(feature)\\n\\n    def calcularLossAtributo(self, nodo, radio):\\n        if nodo is None:\\n            return\\n        else:\\n            #print(\"radio\", radio)\\n            #print(\"nodo\", nodo)\\n            nodo = torch.stack(nodo)\\n            #print(\"nodo stack\", nodo)\\n            #radio = radio.reshape(-1,4)\\n        \\n            #return mse\\n            #return torch.cat([self.mseLoss(b, gt) for b, gt in zip(radio, nodo)], 0)\\n            z = zip(radio.reshape(-1,4), nodo.reshape(-1,4))\\n            \\n            #for b, gt in z:\\n            #    print(\"bgt\", b, gt)\\n                \\n            \\n            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\\n            #print(\"loss\", l)\\n            return l\\n\\n\\n    def classifyLossEstimator(self, label_vector, original):\\n        if original is None:\\n            return\\n        else:\\n           \\n            v = []\\n            for o in original:\\n                if o == 0:\\n                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\\n                if o == 1:\\n                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\\n                if o == 2:\\n                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\\n                v.append(vector)\\n\\n            v = torch.stack(v)\\n            z = zip(label_vector.reshape(-1,3), v.reshape(-1,3))   \\n            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\\n            \\n            return l\\n            #return c\\n       # return torch.cat([self.creLoss(l.unsqueeze(0), gt).mul(0.2) for l, gt in zip(label_vector, gt_label_vector)], 0)\\n\\n    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\\n        \\n        v = v1.add(v2)\\n        #print(\"v0\", v)\\n        if v3 is not None:\\n            v = v.add(v3)\\n        #print(\"v0\", v)\\n        if v4 is not None:\\n            v = v.add(v4)\\n       \\n        return v\\nif qzero == 0:\\n    qzero = 1\\nif qOne == 0:\\n    qOne = 1\\nif qtwo == 0:\\n    qtwo = 1\\nmult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\\nGrassdecoder = GRASSDecoder(latent_size=128, hidden_size=256, mult = mult)\\nGrassdecoder = Grassdecoder.to(device)\\n'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class InternalDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size: int):\n",
    "        super(InternalDecoder, self).__init__()\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, latent_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.mlp2 = nn.Linear(latent_size,4)\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"internal decoder\")\n",
    "        #print(\"input\", parent_feature.shape)\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.lp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        rad_feature = self.mlp2(vector)\n",
    "\n",
    "        return right_feature, rad_feature\n",
    "\n",
    "class BifurcationDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(BifurcationDecoder, self).__init__()\n",
    "        #self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp_left = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp2 = nn.Linear(latent_size,4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"bifurcation decoder input\", parent_feature.shape)\n",
    "        parent_feature = parent_feature.reshape(-1,128)\n",
    "        #print(\"bifurcation decoder input\", parent_feature.shape)\n",
    "        vector = self.mlp(parent_feature)\n",
    "        #print(\"v1\", vector.shape)\n",
    "        vector = self.tanh(vector)\n",
    "        #print(\"v2\", vector.shape)\n",
    "        vector = self.lp2(vector)\n",
    "        #print(\"v3\", vector.shape)\n",
    "        vector = self.tanh(vector)\n",
    "        left_feature = self.mlp_left(vector)\n",
    "        left_feature = self.tanh(left_feature)\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        rad_feature = self.mlp2(vector)\n",
    "        #print(\"exiting bif dec\")\n",
    "        return left_feature, right_feature, rad_feature\n",
    "\n",
    "\n",
    "class featureDecoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size: int):\n",
    "        super(featureDecoder, self).__init__()\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp3 = nn.Linear(latent_size, latent_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.mlp4 = nn.Linear(latent_size,4)\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "        #print(\"feature decoder input\", parent_feature.shape)\n",
    "\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp3(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.mlp4(vector)\n",
    "       \n",
    "        return vector\n",
    "\n",
    "\n",
    "\n",
    "class GRASSDecoder(nn.Module):\n",
    "    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\n",
    "        super(GRASSDecoder, self).__init__()\n",
    "        self.feature_decoder = featureDecoder(latent_size, hidden_size)\n",
    "        self.internal_decoder = InternalDecoder(latent_size, hidden_size)\n",
    "        self.bifurcation_decoder = BifurcationDecoder(latent_size, hidden_size)\n",
    "        self.node_classifier = NodeClassifier(latent_size, hidden_size)\n",
    "        self.mseLoss = nn.MSELoss()  # pytorch's mean squared error loss\n",
    "        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch's cross entropy loss (NOTE: no softmax is needed before)\n",
    "\n",
    "    def featureDecoder(self, feature):\n",
    "        return self.feature_decoder(feature)\n",
    "\n",
    "    def internalDecoder(self, feature):\n",
    "        return self.internal_decoder(feature)\n",
    "\n",
    "    def bifurcationDecoder(self, feature):\n",
    "        return self.bifurcation_decoder(feature)\n",
    "\n",
    "    def nodeClassifier(self, feature):\n",
    "        return self.node_classifier(feature)\n",
    "\n",
    "    def calcularLossAtributo(self, nodo, radio):\n",
    "        if nodo is None:\n",
    "            return\n",
    "        else:\n",
    "            #print(\"radio\", radio)\n",
    "            #print(\"nodo\", nodo)\n",
    "            nodo = torch.stack(nodo)\n",
    "            #print(\"nodo stack\", nodo)\n",
    "            #radio = radio.reshape(-1,4)\n",
    "        \n",
    "            #return mse\n",
    "            #return torch.cat([self.mseLoss(b, gt) for b, gt in zip(radio, nodo)], 0)\n",
    "            z = zip(radio.reshape(-1,4), nodo.reshape(-1,4))\n",
    "            \n",
    "            #for b, gt in z:\n",
    "            #    print(\"bgt\", b, gt)\n",
    "                \n",
    "            \n",
    "            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\n",
    "            #print(\"loss\", l)\n",
    "            return l\n",
    "\n",
    "\n",
    "    def classifyLossEstimator(self, label_vector, original):\n",
    "        if original is None:\n",
    "            return\n",
    "        else:\n",
    "           \n",
    "            v = []\n",
    "            for o in original:\n",
    "                if o == 0:\n",
    "                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\n",
    "                if o == 1:\n",
    "                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\n",
    "                if o == 2:\n",
    "                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\n",
    "                v.append(vector)\n",
    "\n",
    "            v = torch.stack(v)\n",
    "            z = zip(label_vector.reshape(-1,3), v.reshape(-1,3))   \n",
    "            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\n",
    "            \n",
    "            return l\n",
    "            #return c\n",
    "       # return torch.cat([self.creLoss(l.unsqueeze(0), gt).mul(0.2) for l, gt in zip(label_vector, gt_label_vector)], 0)\n",
    "\n",
    "    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\n",
    "        \n",
    "        v = v1.add(v2)\n",
    "        #print(\"v0\", v)\n",
    "        if v3 is not None:\n",
    "            v = v.add(v3)\n",
    "        #print(\"v0\", v)\n",
    "        if v4 is not None:\n",
    "            v = v.add(v4)\n",
    "       \n",
    "        return v\n",
    "if qzero == 0:\n",
    "    qzero = 1\n",
    "if qOne == 0:\n",
    "    qOne = 1\n",
    "if qtwo == 0:\n",
    "    qtwo = 1\n",
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "Grassdecoder = GRASSDecoder(latent_size=128, hidden_size=256, mult = mult)\n",
    "Grassdecoder = Grassdecoder.to(device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Decode an input (parent) feature into a left-child and a right-child feature \"\"\"\n",
    "    def __init__(self, latent_size : int, hidden_size : int):\n",
    "        super(Decoder, self).__init__()\n",
    "        #self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.mlp = nn.Linear(latent_size,hidden_size)\n",
    "        self.lp2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.mlp_left = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp_right = nn.Linear(latent_size, latent_size)\n",
    "        self.mlp3 = nn.Linear(latent_size,latent_size)\n",
    "        self.mlp2 = nn.Linear(latent_size,4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def common_branch(self, parent_feature):\n",
    "        vector = self.mlp(parent_feature)\n",
    "        vector = self.tanh(vector)\n",
    "        vector = self.lp2(vector)\n",
    "        vector = self.tanh(vector)\n",
    "        return vector\n",
    "\n",
    "    def attr_branch(self, vector):\n",
    "        vector = self.mlp2(vector)        \n",
    "        return vector\n",
    "\n",
    "    def right_branch(self, vector):\n",
    "        right_feature = self.mlp_right(vector)\n",
    "        right_feature = self.tanh(right_feature)\n",
    "        return right_feature\n",
    "\n",
    "    def left_branch(self, vector):\n",
    "        left_feature = self.mlp_left(vector)\n",
    "        left_feature = self.tanh(left_feature)\n",
    "        return left_feature\n",
    "\n",
    "    def forward(self, parent_feature):\n",
    "      \n",
    "        vector      = self.common_branch(parent_feature)\n",
    "        attr_vector = self.attr_branch(vector)\n",
    "        return attr_vector \n",
    "\n",
    "    def forward1(self, parent_feature):\n",
    "    \n",
    "\n",
    "        vector       = self.common_branch(parent_feature)\n",
    "        attr_vector  = self.attr_branch(vector)\n",
    "        right_vector = self.right_branch(vector)\n",
    "        \n",
    "        #print(\"right vector\", right_vector)\n",
    "        #print(\"radius\", attr_vector)\n",
    "        return right_vector, attr_vector\n",
    "\n",
    "    def forward2(self, parent_feature):\n",
    "       \n",
    "\n",
    "        vector       = self.common_branch(parent_feature)\n",
    "        attr_vector  = self.attr_branch(vector)\n",
    "        right_vector = self.right_branch(vector)\n",
    "        left_vector  = self.left_branch(vector)\n",
    "        #print(\"left vector\", left_vector)\n",
    "        #print(\"right vector\", right_vector)\n",
    "        #print(\"radius\", attr_vector)\n",
    "        return left_vector, right_vector, attr_vector\n",
    "\n",
    "\n",
    "\n",
    "class GRASSDecoder(nn.Module):\n",
    "    def __init__(self, latent_size : int, hidden_size: int, mult: torch.Tensor):\n",
    "        super(GRASSDecoder, self).__init__()\n",
    "        self.decoder = Decoder(latent_size, hidden_size)\n",
    "        self.node_classifier = NodeClassifier(latent_size, hidden_size)\n",
    "        self.mseLoss = nn.MSELoss()  # pytorch's mean squared error loss\n",
    "        self.ceLoss = nn.CrossEntropyLoss(weight = mult)  # pytorch's cross entropy loss (NOTE: no softmax is needed before)\n",
    "        \n",
    "\n",
    "\n",
    "    def featureDecoder(self, feature):\n",
    "        return self.decoder.forward(feature)\n",
    "\n",
    "    def internalDecoder(self, feature):\n",
    "        return self.decoder.forward1(feature)\n",
    "\n",
    "    def bifurcationDecoder(self, feature):\n",
    "        return self.decoder.forward2(feature)\n",
    "\n",
    "    def nodeClassifier(self, feature):\n",
    "        return self.node_classifier(feature)\n",
    "\n",
    "    def calcularLossAtributo(self, nodo, radio):\n",
    "        #print(\"nodo\", nodo)\n",
    "        #print(\"radio\", radio)\n",
    "        a, b = list(zip(*nodo))# a son los atributos, b los pesos\n",
    "        if nodo is None:\n",
    "            return\n",
    "        else:\n",
    "            nodo = torch.stack(list(a))\n",
    "        \n",
    "            l = [self.mseLoss(b.reshape(1,4), gt.reshape(1,4)) for b, gt in zip(radio.reshape(-1,4), nodo.reshape(-1,4))]\n",
    "            #print(\"mse\", l)\n",
    "            return l\n",
    "\n",
    "\n",
    "    def classifyLossEstimator(self, label_vector, original):\n",
    "        if original is None:\n",
    "            return\n",
    "        else:\n",
    "           \n",
    "            v = []\n",
    "            for o in original:\n",
    "                if o == 0:\n",
    "                    vector = torch.tensor([1, 0, 0], device=device, dtype = torch.float)\n",
    "                if o == 1:\n",
    "                    vector = torch.tensor([0, 1, 0], device=device, dtype = torch.float)\n",
    "                if o == 2:\n",
    "                    vector = torch.tensor([0, 0, 1], device=device, dtype = torch.float)\n",
    "                v.append(vector)\n",
    "            #print(\"////\")\n",
    "            #print(\"original\", v)\n",
    "            #print(\"prediction\", label_vector)\n",
    "            #print(\"////\")\n",
    "\n",
    "            v = torch.stack(v)\n",
    "            #for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3)):\n",
    "            #    print(\"...\")\n",
    "            #    print(\"b\", b)\n",
    "            #    print(\"gt\", gt)\n",
    "            #    print(\"...\")\n",
    "            l = [self.ceLoss(b.reshape(1,3), gt.reshape(1,3)).mul(0.4) for b, gt in zip(label_vector.reshape(-1,3), v.reshape(-1,3))]\n",
    "            #print(\"ce\", l)\n",
    "\n",
    "            return l\n",
    "            #return c\n",
    "       # return torch.cat([self.creLoss(l.unsqueeze(0), gt).mul(0.2) for l, gt in zip(label_vector, gt_label_vector)], 0)\n",
    "\n",
    "    '''\n",
    "    def vectorAdder(self, v1, v2, v3 = None, v4 = None):\n",
    "        \n",
    "        print(\"loss estructura\", v1)\n",
    "        print(\"loss atributo\", v2)\n",
    "        print(\"right loss\", v3)\n",
    "        print(\"left loss\", v4)\n",
    "\n",
    "\n",
    "        v = v1.add(v2)\n",
    "        #print(\"v0\", v)\n",
    "        if v3 is not None:\n",
    "            v = v.add(v3)\n",
    "        #print(\"v0\", v)\n",
    "        if v4 is not None:\n",
    "            v = v.add(v4)\n",
    "       \n",
    "        return v\n",
    "\n",
    "    '''\n",
    "    def vectorAdder(self, v1, v2):\n",
    "        v = v1.add(v2)\n",
    "        return v\n",
    "\n",
    "    def vectorMult(self, m, v):\n",
    "        #print(\"v\", v)\n",
    "        #print(\"m\", m)\n",
    "        z = zip(v, m)\n",
    "        r = []\n",
    "        for c, d in z:\n",
    "            #print(\"v\", c)\n",
    "            #print(\"m\", d)\n",
    "            r.append(torch.mul(c, d))\n",
    "        #res = [torch.mul(v, m) for v, m in zip(v, m)]\n",
    "        #print(\"res\", r)\n",
    "        return r\n",
    "\n",
    "if qzero == 0:\n",
    "    qzero = 1\n",
    "if qOne == 0:\n",
    "    qOne = 1\n",
    "if qtwo == 0:\n",
    "    qtwo = 1\n",
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "\n",
    "Grassdecoder = GRASSDecoder(latent_size=128, hidden_size=256, mult = mult)\n",
    "Grassdecoder = Grassdecoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3333, 0.0154, 0.5000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "print(mult)\n",
    "def calcularLossEstructura(cl_p, original):\n",
    "    \n",
    "    mult = torch.tensor([1/round(qzero),1/round(qOne),1/round(qtwo)], device = device)\n",
    "    ce = nn.CrossEntropyLoss(weight = mult)\n",
    "\n",
    "    if original.childs() == 0:\n",
    "        vector = [1, 0, 0] \n",
    "    if original.childs() == 1:\n",
    "        vector = [0, 1, 0]\n",
    "    if original.childs() == 2:\n",
    "        vector = [0, 0, 1] \n",
    "\n",
    "    #print(\"original\", vector)\n",
    "    #print(\"prediction\", cl_p)\n",
    "    c = ce(cl_p, torch.tensor(vector, device=device, dtype = torch.float).reshape(1, 3))\n",
    "    #print(\"ce\", 0.4*c)\n",
    "    return c\n",
    "\n",
    "\n",
    "def calcularLossAtributo(nodo, radio):\n",
    "    #print(\"nodo\", nodo)\n",
    "    #print(\"radio\", radio)\n",
    "\n",
    "    radio = radio.reshape(-1,4)\n",
    "    nodo = nodo.reshape(-1,4)\n",
    "    l2    = nn.MSELoss()\n",
    "   \n",
    "    mse = l2(radio, nodo)\n",
    "    #print(\"mse\", mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchNode(node, key):\n",
    "     \n",
    "    if (node == None):\n",
    "        return False\n",
    " \n",
    "    if (node.data == key):\n",
    "        return node\n",
    "        \n",
    " \n",
    "    \"\"\" then recur on left subtree \"\"\"\n",
    "    res1 = searchNode(node.left, key)\n",
    "    # node found, no need to look further\n",
    "    if res1:\n",
    "        return res1\n",
    " \n",
    "    \"\"\" node is not found in left,\n",
    "    so recur on right subtree \"\"\"\n",
    "    res2 = searchNode(node.right, key)\n",
    "    return res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "nodo = searchNode(input, 4)\n",
    "print(nodo.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0.5616, 0.4881, 0.2191, 0.3577], device='cuda:0')\n",
      "1 tensor([0.5440, 0.4732, 0.2243, 0.3409], device='cuda:0')\n",
      "2 tensor([0.5258, 0.4560, 0.2368, 0.3089], device='cuda:0')\n",
      "3 tensor([0.5088, 0.4391, 0.2544, 0.2733], device='cuda:0')\n",
      "4 tensor([0.4922, 0.4232, 0.2691, 0.2772], device='cuda:0')\n",
      "5 tensor([0.4757, 0.4107, 0.2818, 0.3116], device='cuda:0')\n",
      "6 tensor([0.4552, 0.3980, 0.2874, 0.3513], device='cuda:0')\n",
      "7 tensor([0.4357, 0.3855, 0.2870, 0.3686], device='cuda:0')\n",
      "8 tensor([0.4128, 0.3646, 0.3112, 0.4315], device='cuda:0')\n",
      "9 tensor([0.3303, 0.3139, 0.3256, 0.5333], device='cuda:0')\n",
      "10 tensor([0.3130, 0.3037, 0.3267, 0.5102], device='cuda:0')\n",
      "11 tensor([0.2797, 0.2858, 0.3186, 0.5458], device='cuda:0')\n",
      "12 tensor([0.2424, 0.2630, 0.3149, 0.6090], device='cuda:0')\n",
      "13 tensor([0.1990, 0.2435, 0.2849, 0.6247], device='cuda:0')\n",
      "14 tensor([0.1616, 0.2183, 0.2728, 0.7113], device='cuda:0')\n",
      "15 tensor([0.0480, 0.1620, 0.2025, 0.3170], device='cuda:0')\n",
      "16 tensor([0.0471, 0.1617, 0.1982, 0.3186], device='cuda:0')\n",
      "17 tensor([0.0316, 0.1559, 0.1722, 0.2437], device='cuda:0')\n",
      "18 tensor([0.0137, 0.1445, 0.1257, 0.2667], device='cuda:0')\n",
      "19 tensor([0.0067, 0.1302, 0.0938, 0.2569], device='cuda:0')\n",
      "20 tensor([0.0014, 0.1132, 0.0622, 0.2465], device='cuda:0')\n",
      "21 tensor([0.0000, 0.1004, 0.0475, 0.2097], device='cuda:0')\n",
      "22 tensor([0.0014, 0.0887, 0.0369, 0.1579], device='cuda:0')\n",
      "23 tensor([0.0037, 0.0775, 0.0271, 0.1002], device='cuda:0')\n",
      "24 tensor([0.0086, 0.0617, 0.0169, 0.0466], device='cuda:0')\n",
      "25 tensor([0.0152, 0.0482, 0.0068, 0.0191], device='cuda:0')\n",
      "26 tensor([0.0221, 0.0346, 0.0035, 0.0000], device='cuda:0')\n",
      "27 tensor([0.0446, 0.0000, 0.0000, 0.1431], device='cuda:0')\n",
      "28 tensor([0.1163, 0.1714, 0.2870, 1.0000], device='cuda:0')\n",
      "29 tensor([0.1279, 0.1194, 0.3936, 0.3007], device='cuda:0')\n",
      "30 tensor([0.1310, 0.1089, 0.4269, 0.2334], device='cuda:0')\n",
      "31 tensor([0.1331, 0.0966, 0.4698, 0.1891], device='cuda:0')\n",
      "32 tensor([0.1367, 0.0838, 0.5463, 0.2081], device='cuda:0')\n",
      "33 tensor([0.1397, 0.0735, 0.6332, 0.2744], device='cuda:0')\n",
      "34 tensor([0.1374, 0.0682, 0.7149, 0.3040], device='cuda:0')\n",
      "35 tensor([0.1306, 0.0686, 0.7835, 0.3171], device='cuda:0')\n",
      "36 tensor([0.1195, 0.0777, 0.8675, 0.3916], device='cuda:0')\n",
      "37 tensor([0.1043, 0.0832, 0.9164, 0.3878], device='cuda:0')\n",
      "38 tensor([0.0634, 0.1035, 1.0000, 0.5592], device='cuda:0')\n",
      "39 tensor([0.5789, 0.5054, 0.2145, 0.3307], device='cuda:0')\n",
      "40 tensor([0.5960, 0.5207, 0.2056, 0.2884], device='cuda:0')\n",
      "41 tensor([0.6114, 0.5347, 0.1960, 0.2388], device='cuda:0')\n",
      "42 tensor([0.6278, 0.5506, 0.1884, 0.2152], device='cuda:0')\n",
      "43 tensor([0.6430, 0.5657, 0.1783, 0.1882], device='cuda:0')\n",
      "44 tensor([0.6595, 0.5804, 0.1668, 0.2134], device='cuda:0')\n",
      "45 tensor([0.6764, 0.5950, 0.1611, 0.2280], device='cuda:0')\n",
      "46 tensor([0.6919, 0.6085, 0.1569, 0.2398], device='cuda:0')\n",
      "47 tensor([0.7077, 0.6204, 0.1528, 0.2374], device='cuda:0')\n",
      "48 tensor([0.7221, 0.6315, 0.1500, 0.2235], device='cuda:0')\n",
      "49 tensor([0.7357, 0.6426, 0.1507, 0.2100], device='cuda:0')\n",
      "50 tensor([0.7505, 0.6554, 0.1533, 0.1918], device='cuda:0')\n",
      "51 tensor([0.7644, 0.6687, 0.1587, 0.1849], device='cuda:0')\n",
      "52 tensor([0.7810, 0.6845, 0.1669, 0.1697], device='cuda:0')\n",
      "53 tensor([0.8008, 0.7034, 0.1695, 0.1416], device='cuda:0')\n",
      "54 tensor([0.8209, 0.7195, 0.1880, 0.1583], device='cuda:0')\n",
      "55 tensor([0.8394, 0.7371, 0.2081, 0.1803], device='cuda:0')\n",
      "56 tensor([0.8545, 0.7538, 0.2239, 0.1736], device='cuda:0')\n",
      "57 tensor([0.8686, 0.7713, 0.2359, 0.1779], device='cuda:0')\n",
      "58 tensor([0.8831, 0.7861, 0.2549, 0.1887], device='cuda:0')\n",
      "59 tensor([0.9002, 0.8049, 0.2781, 0.2560], device='cuda:0')\n",
      "60 tensor([0.9160, 0.8208, 0.2926, 0.2945], device='cuda:0')\n",
      "61 tensor([0.9274, 0.8349, 0.3052, 0.2923], device='cuda:0')\n",
      "62 tensor([0.9391, 0.8510, 0.3190, 0.2867], device='cuda:0')\n",
      "63 tensor([0.9538, 0.8696, 0.3377, 0.3038], device='cuda:0')\n",
      "64 tensor([0.9657, 0.8867, 0.3622, 0.3097], device='cuda:0')\n",
      "65 tensor([0.9739, 0.9021, 0.3899, 0.2735], device='cuda:0')\n",
      "66 tensor([0.9843, 0.9205, 0.4337, 0.2695], device='cuda:0')\n",
      "67 tensor([0.9941, 0.9472, 0.4795, 0.3092], device='cuda:0')\n",
      "68 tensor([1.0000, 0.9678, 0.5346, 0.3284], device='cuda:0')\n",
      "69 tensor([0.9985, 1.0000, 0.6220, 0.4260], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "count = []\n",
    "numerar_nodos(input, count)\n",
    "input.traverseInorder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_nodes 70\n"
     ]
    }
   ],
   "source": [
    "def getLevelUtil(node, data, level):\n",
    "    if (node == None):\n",
    "        return 0\n",
    " \n",
    "    if (node.data == data):\n",
    "        return level\n",
    " \n",
    "    downlevel = getLevelUtil(node.left, data, level + 1)\n",
    "\n",
    "    if (downlevel != 0):\n",
    "        return downlevel\n",
    " \n",
    "    downlevel = getLevelUtil(node.right, data, level + 1)\n",
    "    return downlevel\n",
    " \n",
    "# Returns level of given data value\n",
    " \n",
    " \n",
    "def getLevel(node, data):\n",
    "    return getLevelUtil(node, data, 1)\n",
    " \n",
    "\n",
    "c = []\n",
    "n_nodes = input.count_nodes(input, c)\n",
    "print(\"n_nodes\", len(n_nodes))\n",
    "for x in range(0, len(n_nodes)):\n",
    "        level = getLevel(input, x)\n",
    "        if (level):\n",
    "            #print(\"Level of\", x, \"is\", getLevel(input, x))\n",
    "            node = searchNode(input, x)\n",
    "            node.level = getLevel(input, x)\n",
    "        else:\n",
    "            print(x, \"is not present in tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0.5616, 0.4881, 0.2191, 0.3577], device='cuda:0') 2 None\n",
      "1 tensor([0.5440, 0.4732, 0.2243, 0.3409], device='cuda:0') 3 None\n",
      "2 tensor([0.5258, 0.4560, 0.2368, 0.3089], device='cuda:0') 4 None\n",
      "3 tensor([0.5088, 0.4391, 0.2544, 0.2733], device='cuda:0') 5 None\n",
      "4 tensor([0.4922, 0.4232, 0.2691, 0.2772], device='cuda:0') 6 None\n",
      "5 tensor([0.4757, 0.4107, 0.2818, 0.3116], device='cuda:0') 7 None\n",
      "6 tensor([0.4552, 0.3980, 0.2874, 0.3513], device='cuda:0') 8 None\n",
      "7 tensor([0.4357, 0.3855, 0.2870, 0.3686], device='cuda:0') 9 None\n",
      "8 tensor([0.4128, 0.3646, 0.3112, 0.4315], device='cuda:0') 10 None\n",
      "9 tensor([0.3303, 0.3139, 0.3256, 0.5333], device='cuda:0') 11 None\n",
      "10 tensor([0.3130, 0.3037, 0.3267, 0.5102], device='cuda:0') 12 None\n",
      "11 tensor([0.2797, 0.2858, 0.3186, 0.5458], device='cuda:0') 13 None\n",
      "12 tensor([0.2424, 0.2630, 0.3149, 0.6090], device='cuda:0') 14 None\n",
      "13 tensor([0.1990, 0.2435, 0.2849, 0.6247], device='cuda:0') 15 None\n",
      "14 tensor([0.1616, 0.2183, 0.2728, 0.7113], device='cuda:0') 16 None\n",
      "15 tensor([0.0480, 0.1620, 0.2025, 0.3170], device='cuda:0') 18 None\n",
      "16 tensor([0.0471, 0.1617, 0.1982, 0.3186], device='cuda:0') 19 None\n",
      "17 tensor([0.0316, 0.1559, 0.1722, 0.2437], device='cuda:0') 20 None\n",
      "18 tensor([0.0137, 0.1445, 0.1257, 0.2667], device='cuda:0') 21 None\n",
      "19 tensor([0.0067, 0.1302, 0.0938, 0.2569], device='cuda:0') 22 None\n",
      "20 tensor([0.0014, 0.1132, 0.0622, 0.2465], device='cuda:0') 23 None\n",
      "21 tensor([0.0000, 0.1004, 0.0475, 0.2097], device='cuda:0') 24 None\n",
      "22 tensor([0.0014, 0.0887, 0.0369, 0.1579], device='cuda:0') 25 None\n",
      "23 tensor([0.0037, 0.0775, 0.0271, 0.1002], device='cuda:0') 26 None\n",
      "24 tensor([0.0086, 0.0617, 0.0169, 0.0466], device='cuda:0') 27 None\n",
      "25 tensor([0.0152, 0.0482, 0.0068, 0.0191], device='cuda:0') 28 None\n",
      "26 tensor([0.0221, 0.0346, 0.0035, 0.0000], device='cuda:0') 29 None\n",
      "27 tensor([0.0446, 0.0000, 0.0000, 0.1431], device='cuda:0') 30 None\n",
      "28 tensor([0.1163, 0.1714, 0.2870, 1.0000], device='cuda:0') 17 None\n",
      "29 tensor([0.1279, 0.1194, 0.3936, 0.3007], device='cuda:0') 18 None\n",
      "30 tensor([0.1310, 0.1089, 0.4269, 0.2334], device='cuda:0') 19 None\n",
      "31 tensor([0.1331, 0.0966, 0.4698, 0.1891], device='cuda:0') 20 None\n",
      "32 tensor([0.1367, 0.0838, 0.5463, 0.2081], device='cuda:0') 21 None\n",
      "33 tensor([0.1397, 0.0735, 0.6332, 0.2744], device='cuda:0') 22 None\n",
      "34 tensor([0.1374, 0.0682, 0.7149, 0.3040], device='cuda:0') 23 None\n",
      "35 tensor([0.1306, 0.0686, 0.7835, 0.3171], device='cuda:0') 24 None\n",
      "36 tensor([0.1195, 0.0777, 0.8675, 0.3916], device='cuda:0') 25 None\n",
      "37 tensor([0.1043, 0.0832, 0.9164, 0.3878], device='cuda:0') 26 None\n",
      "38 tensor([0.0634, 0.1035, 1.0000, 0.5592], device='cuda:0') 27 None\n",
      "39 tensor([0.5789, 0.5054, 0.2145, 0.3307], device='cuda:0') 1 None\n",
      "40 tensor([0.5960, 0.5207, 0.2056, 0.2884], device='cuda:0') 2 None\n",
      "41 tensor([0.6114, 0.5347, 0.1960, 0.2388], device='cuda:0') 3 None\n",
      "42 tensor([0.6278, 0.5506, 0.1884, 0.2152], device='cuda:0') 4 None\n",
      "43 tensor([0.6430, 0.5657, 0.1783, 0.1882], device='cuda:0') 5 None\n",
      "44 tensor([0.6595, 0.5804, 0.1668, 0.2134], device='cuda:0') 6 None\n",
      "45 tensor([0.6764, 0.5950, 0.1611, 0.2280], device='cuda:0') 7 None\n",
      "46 tensor([0.6919, 0.6085, 0.1569, 0.2398], device='cuda:0') 8 None\n",
      "47 tensor([0.7077, 0.6204, 0.1528, 0.2374], device='cuda:0') 9 None\n",
      "48 tensor([0.7221, 0.6315, 0.1500, 0.2235], device='cuda:0') 10 None\n",
      "49 tensor([0.7357, 0.6426, 0.1507, 0.2100], device='cuda:0') 11 None\n",
      "50 tensor([0.7505, 0.6554, 0.1533, 0.1918], device='cuda:0') 12 None\n",
      "51 tensor([0.7644, 0.6687, 0.1587, 0.1849], device='cuda:0') 13 None\n",
      "52 tensor([0.7810, 0.6845, 0.1669, 0.1697], device='cuda:0') 14 None\n",
      "53 tensor([0.8008, 0.7034, 0.1695, 0.1416], device='cuda:0') 15 None\n",
      "54 tensor([0.8209, 0.7195, 0.1880, 0.1583], device='cuda:0') 16 None\n",
      "55 tensor([0.8394, 0.7371, 0.2081, 0.1803], device='cuda:0') 17 None\n",
      "56 tensor([0.8545, 0.7538, 0.2239, 0.1736], device='cuda:0') 18 None\n",
      "57 tensor([0.8686, 0.7713, 0.2359, 0.1779], device='cuda:0') 19 None\n",
      "58 tensor([0.8831, 0.7861, 0.2549, 0.1887], device='cuda:0') 20 None\n",
      "59 tensor([0.9002, 0.8049, 0.2781, 0.2560], device='cuda:0') 21 None\n",
      "60 tensor([0.9160, 0.8208, 0.2926, 0.2945], device='cuda:0') 22 None\n",
      "61 tensor([0.9274, 0.8349, 0.3052, 0.2923], device='cuda:0') 23 None\n",
      "62 tensor([0.9391, 0.8510, 0.3190, 0.2867], device='cuda:0') 24 None\n",
      "63 tensor([0.9538, 0.8696, 0.3377, 0.3038], device='cuda:0') 25 None\n",
      "64 tensor([0.9657, 0.8867, 0.3622, 0.3097], device='cuda:0') 26 None\n",
      "65 tensor([0.9739, 0.9021, 0.3899, 0.2735], device='cuda:0') 27 None\n",
      "66 tensor([0.9843, 0.9205, 0.4337, 0.2695], device='cuda:0') 28 None\n",
      "67 tensor([0.9941, 0.9472, 0.4795, 0.3092], device='cuda:0') 29 None\n",
      "68 tensor([1.0000, 0.9678, 0.5346, 0.3284], device='cuda:0') 30 None\n",
      "69 tensor([0.9985, 1.0000, 0.6220, 0.4260], device='cuda:0') 31 None\n",
      "tree level 1185\n",
      "0 tensor([0.5616, 0.4881, 0.2191, 0.3577], device='cuda:0') 2 1185\n",
      "1 tensor([0.5440, 0.4732, 0.2243, 0.3409], device='cuda:0') 3 1185\n",
      "2 tensor([0.5258, 0.4560, 0.2368, 0.3089], device='cuda:0') 4 1185\n",
      "3 tensor([0.5088, 0.4391, 0.2544, 0.2733], device='cuda:0') 5 1185\n",
      "4 tensor([0.4922, 0.4232, 0.2691, 0.2772], device='cuda:0') 6 1185\n",
      "5 tensor([0.4757, 0.4107, 0.2818, 0.3116], device='cuda:0') 7 1185\n",
      "6 tensor([0.4552, 0.3980, 0.2874, 0.3513], device='cuda:0') 8 1185\n",
      "7 tensor([0.4357, 0.3855, 0.2870, 0.3686], device='cuda:0') 9 1185\n",
      "8 tensor([0.4128, 0.3646, 0.3112, 0.4315], device='cuda:0') 10 1185\n",
      "9 tensor([0.3303, 0.3139, 0.3256, 0.5333], device='cuda:0') 11 1185\n",
      "10 tensor([0.3130, 0.3037, 0.3267, 0.5102], device='cuda:0') 12 1185\n",
      "11 tensor([0.2797, 0.2858, 0.3186, 0.5458], device='cuda:0') 13 1185\n",
      "12 tensor([0.2424, 0.2630, 0.3149, 0.6090], device='cuda:0') 14 1185\n",
      "13 tensor([0.1990, 0.2435, 0.2849, 0.6247], device='cuda:0') 15 1185\n",
      "14 tensor([0.1616, 0.2183, 0.2728, 0.7113], device='cuda:0') 16 1185\n",
      "15 tensor([0.0480, 0.1620, 0.2025, 0.3170], device='cuda:0') 18 1185\n",
      "16 tensor([0.0471, 0.1617, 0.1982, 0.3186], device='cuda:0') 19 1185\n",
      "17 tensor([0.0316, 0.1559, 0.1722, 0.2437], device='cuda:0') 20 1185\n",
      "18 tensor([0.0137, 0.1445, 0.1257, 0.2667], device='cuda:0') 21 1185\n",
      "19 tensor([0.0067, 0.1302, 0.0938, 0.2569], device='cuda:0') 22 1185\n",
      "20 tensor([0.0014, 0.1132, 0.0622, 0.2465], device='cuda:0') 23 1185\n",
      "21 tensor([0.0000, 0.1004, 0.0475, 0.2097], device='cuda:0') 24 1185\n",
      "22 tensor([0.0014, 0.0887, 0.0369, 0.1579], device='cuda:0') 25 1185\n",
      "23 tensor([0.0037, 0.0775, 0.0271, 0.1002], device='cuda:0') 26 1185\n",
      "24 tensor([0.0086, 0.0617, 0.0169, 0.0466], device='cuda:0') 27 1185\n",
      "25 tensor([0.0152, 0.0482, 0.0068, 0.0191], device='cuda:0') 28 1185\n",
      "26 tensor([0.0221, 0.0346, 0.0035, 0.0000], device='cuda:0') 29 1185\n",
      "27 tensor([0.0446, 0.0000, 0.0000, 0.1431], device='cuda:0') 30 1185\n",
      "28 tensor([0.1163, 0.1714, 0.2870, 1.0000], device='cuda:0') 17 1185\n",
      "29 tensor([0.1279, 0.1194, 0.3936, 0.3007], device='cuda:0') 18 1185\n",
      "30 tensor([0.1310, 0.1089, 0.4269, 0.2334], device='cuda:0') 19 1185\n",
      "31 tensor([0.1331, 0.0966, 0.4698, 0.1891], device='cuda:0') 20 1185\n",
      "32 tensor([0.1367, 0.0838, 0.5463, 0.2081], device='cuda:0') 21 1185\n",
      "33 tensor([0.1397, 0.0735, 0.6332, 0.2744], device='cuda:0') 22 1185\n",
      "34 tensor([0.1374, 0.0682, 0.7149, 0.3040], device='cuda:0') 23 1185\n",
      "35 tensor([0.1306, 0.0686, 0.7835, 0.3171], device='cuda:0') 24 1185\n",
      "36 tensor([0.1195, 0.0777, 0.8675, 0.3916], device='cuda:0') 25 1185\n",
      "37 tensor([0.1043, 0.0832, 0.9164, 0.3878], device='cuda:0') 26 1185\n",
      "38 tensor([0.0634, 0.1035, 1.0000, 0.5592], device='cuda:0') 27 1185\n",
      "39 tensor([0.5789, 0.5054, 0.2145, 0.3307], device='cuda:0') 1 1185\n",
      "40 tensor([0.5960, 0.5207, 0.2056, 0.2884], device='cuda:0') 2 1185\n",
      "41 tensor([0.6114, 0.5347, 0.1960, 0.2388], device='cuda:0') 3 1185\n",
      "42 tensor([0.6278, 0.5506, 0.1884, 0.2152], device='cuda:0') 4 1185\n",
      "43 tensor([0.6430, 0.5657, 0.1783, 0.1882], device='cuda:0') 5 1185\n",
      "44 tensor([0.6595, 0.5804, 0.1668, 0.2134], device='cuda:0') 6 1185\n",
      "45 tensor([0.6764, 0.5950, 0.1611, 0.2280], device='cuda:0') 7 1185\n",
      "46 tensor([0.6919, 0.6085, 0.1569, 0.2398], device='cuda:0') 8 1185\n",
      "47 tensor([0.7077, 0.6204, 0.1528, 0.2374], device='cuda:0') 9 1185\n",
      "48 tensor([0.7221, 0.6315, 0.1500, 0.2235], device='cuda:0') 10 1185\n",
      "49 tensor([0.7357, 0.6426, 0.1507, 0.2100], device='cuda:0') 11 1185\n",
      "50 tensor([0.7505, 0.6554, 0.1533, 0.1918], device='cuda:0') 12 1185\n",
      "51 tensor([0.7644, 0.6687, 0.1587, 0.1849], device='cuda:0') 13 1185\n",
      "52 tensor([0.7810, 0.6845, 0.1669, 0.1697], device='cuda:0') 14 1185\n",
      "53 tensor([0.8008, 0.7034, 0.1695, 0.1416], device='cuda:0') 15 1185\n",
      "54 tensor([0.8209, 0.7195, 0.1880, 0.1583], device='cuda:0') 16 1185\n",
      "55 tensor([0.8394, 0.7371, 0.2081, 0.1803], device='cuda:0') 17 1185\n",
      "56 tensor([0.8545, 0.7538, 0.2239, 0.1736], device='cuda:0') 18 1185\n",
      "57 tensor([0.8686, 0.7713, 0.2359, 0.1779], device='cuda:0') 19 1185\n",
      "58 tensor([0.8831, 0.7861, 0.2549, 0.1887], device='cuda:0') 20 1185\n",
      "59 tensor([0.9002, 0.8049, 0.2781, 0.2560], device='cuda:0') 21 1185\n",
      "60 tensor([0.9160, 0.8208, 0.2926, 0.2945], device='cuda:0') 22 1185\n",
      "61 tensor([0.9274, 0.8349, 0.3052, 0.2923], device='cuda:0') 23 1185\n",
      "62 tensor([0.9391, 0.8510, 0.3190, 0.2867], device='cuda:0') 24 1185\n",
      "63 tensor([0.9538, 0.8696, 0.3377, 0.3038], device='cuda:0') 25 1185\n",
      "64 tensor([0.9657, 0.8867, 0.3622, 0.3097], device='cuda:0') 26 1185\n",
      "65 tensor([0.9739, 0.9021, 0.3899, 0.2735], device='cuda:0') 27 1185\n",
      "66 tensor([0.9843, 0.9205, 0.4337, 0.2695], device='cuda:0') 28 1185\n",
      "67 tensor([0.9941, 0.9472, 0.4795, 0.3092], device='cuda:0') 29 1185\n",
      "68 tensor([1.0000, 0.9678, 0.5346, 0.3284], device='cuda:0') 30 1185\n",
      "69 tensor([0.9985, 1.0000, 0.6220, 0.4260], device='cuda:0') 31 1185\n"
     ]
    }
   ],
   "source": [
    "input.traverseInorderwl(input)\n",
    "tree_level = []\n",
    "input.get_tree_level(input, tree_level)\n",
    "print(\"tree level\", sum(tree_level))\n",
    "input.set_tree_level(input, sum(tree_level))\n",
    "input.traverseInorderwl(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_structure_fold_grass(fold, v, root):\n",
    "   \n",
    "    def decode_node(fold, v, node):\n",
    "        \n",
    "        \n",
    "        if node.childs() == 0 : \n",
    "\n",
    "            radio = fold.add('featureDecoder', v)\n",
    "            lossAtributo = fold.add('calcularLossAtributo', node, radio)\n",
    "\n",
    "            label = fold.add('nodeClassifier', v)\n",
    "            \n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)  \n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            \n",
    "            loss =  fold.add('vectorAdder', losse, lossAtributo)       \n",
    "            return loss\n",
    "\n",
    "            \n",
    "            \n",
    "        elif node.childs() == 1 :\n",
    "            right, radius = fold.add('internalDecoder', v).split(2)\n",
    "            label = fold.add('nodeClassifier', v)\n",
    "            nodoSiguiente = node.right\n",
    "            if nodoSiguiente is not None:\n",
    "                right_loss = decode_node(fold, right, nodoSiguiente)\n",
    "\n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)\n",
    "            lossAtributo = fold.add('calcularLossAtributo', node, radius)\n",
    "            multipl = node.level/node.treelevel\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            loss = fold.add('vectorAdder', losse, lossAtributo)\n",
    "            \n",
    "        \n",
    "            loss2 = fold.add('vectorAdder', loss, right_loss)\n",
    "            return loss2\n",
    "            \n",
    "            \n",
    "\n",
    "        elif node.childs() == 2 :\n",
    "            left, right, radius = fold.add('bifurcationDecoder', v).split(3)\n",
    "            \n",
    "            label = fold.add('nodeClassifier', v)            \n",
    "            \n",
    "            nodoSiguienteRight = node.right\n",
    "            nodoSiguienteLeft = node.left\n",
    "\n",
    "\n",
    "            if nodoSiguienteRight is not None:\n",
    "                right_loss = decode_node(fold, right, nodoSiguienteRight)\n",
    "             \n",
    "            if nodoSiguienteLeft is not None:\n",
    "                left_loss  = decode_node(fold, left, nodoSiguienteLeft)\n",
    "\n",
    "            multipl = node.level/node.treelevel\n",
    "            lossEstructura = fold.add('classifyLossEstimator', label, node)\n",
    "            losse = fold.add('vectorMult', multipl, lossEstructura)\n",
    "            lossAtributo   = fold.add('calcularLossAtributo', node, radius)\n",
    "            loss = fold.add('vectorAdder', losse, lossAtributo)\n",
    "            loss2 = fold.add('vectorAdder', loss, right_loss)\n",
    "            loss3 = fold.add('vectorAdder', loss2, left_loss)\n",
    "            \n",
    "            return loss3\n",
    "            \n",
    "\n",
    "    dec = decode_node (fold, v, root)\n",
    "    return dec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_testing_grass(v, root, max, decoder):\n",
    "    def decode_node(v, node, max, decoder):\n",
    "        cl = decoder.nodeClassifier(v)\n",
    "        _, label = torch.max(cl, 1)\n",
    "        label = label.data\n",
    "        \n",
    "        \n",
    "        if label == 0 and createNode.count <= max: ##output del classifier\n",
    "           \n",
    "            #lossEstructura = Grassdecoder.classifyLossEstimator(cl, node)\n",
    "            radio = decoder.featureDecoder(v)\n",
    "            #print(\"radius\", radio)\n",
    "            #lossAtrs = Grassdecoder.calcularLossAtributo( node, radio )\n",
    "           \n",
    "            return createNode(1,radio)\n",
    "\n",
    "        elif label == 1 and createNode.count <= max:\n",
    "       \n",
    "            right, radius = decoder.internalDecoder(v)\n",
    "            #print(\"radius\", radius)\n",
    "            \n",
    "            d = createNode(1, radius) \n",
    "            #print(\"d\", d.radius)\n",
    "             \n",
    "            if not node is None:\n",
    "                if not node.right is None:\n",
    "                    nodoSiguiente = node.right\n",
    "                else:\n",
    "                    nodoSiguiente = None\n",
    "            else:\n",
    "                nodoSiguiente = None\n",
    "            \n",
    "            d.right = decode_node(right, nodoSiguiente, max, decoder)\n",
    "            \n",
    "\n",
    "            return d\n",
    "       \n",
    "        elif label == 2 and createNode.count <= max:\n",
    "            left, right, radius = decoder.bifurcationDecoder(v)\n",
    "            #print(\"radius\", radius)\n",
    "            \n",
    "            d = createNode(1, radius )\n",
    "  \n",
    "            if not node is None: #el nodo existe, me fijo si tiene hijo der/izq\n",
    "                if not node.right is None:\n",
    "                    nodoSiguienteRight = node.right\n",
    "                else:\n",
    "                    nodoSiguienteRight = None\n",
    "                if not node.left is None:\n",
    "                    nodoSiguienteLeft = node.left\n",
    "                else:\n",
    "                    nodoSiguienteLeft = None\n",
    "            else: #el nodo no existe\n",
    "                nodoSiguienteRight = None\n",
    "                nodoSiguienteLeft = None\n",
    "            \n",
    "            d.right = decode_node(right, nodoSiguienteRight, max, decoder)\n",
    "            d.left = decode_node(left, nodoSiguienteLeft, max, decoder)\n",
    "            \n",
    "           \n",
    "            return d\n",
    "            \n",
    "    createNode.count = 0\n",
    "    dec = decode_node (v, root, max, decoder)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, encoder, decoder, optimizer\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            #print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            #print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            #'classifier_state_dict': classifier.state_dict(),\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                \n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, 'outputs/best_model.pth')\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_structure_fold_(v, root):\n",
    "    \n",
    "    def decode_node(v, node):\n",
    "        cl = Grassdecoder.nodeClassifier(v)\n",
    "        _, label = torch.max(cl, 1)\n",
    "        label = label.data\n",
    "\n",
    "        \n",
    "        if node.childs() == 0 : ##output del classifier\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            radio = Grassdecoder.featureDecoder(v)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radio )\n",
    "            nd = createNode(1,radio, ce = lossEstructura,  mse = lossAtrs)\n",
    "            return nd\n",
    "\n",
    "        elif node.childs() == 1 :\n",
    "        \n",
    "            right, radius = Grassdecoder.internalDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radius )\n",
    "            nd = createNode(1, radius, cl_prob = lossAtrs , ce = lossEstructura, mse = lossAtrs) \n",
    "            \n",
    "            nodoSiguiente = node.right\n",
    "           \n",
    "            if nodoSiguiente is not None:\n",
    "                nd.right = decode_node(right, nodoSiguiente)\n",
    "               \n",
    "            return nd\n",
    "\n",
    "        elif node.childs() == 2 :\n",
    "            left, right, radius = Grassdecoder.bifurcationDecoder(v)\n",
    "            lossEstructura = calcularLossEstructura(cl, node)\n",
    "            lossAtrs = calcularLossAtributo( node.radius, radius )\n",
    "            nd = createNode(1, radius, cl_prob = lossAtrs, ce = lossEstructura, mse = lossAtrs)\n",
    "            \n",
    "            nodoSiguienteRight = node.right\n",
    "            nodoSiguienteLeft = node.left\n",
    "\n",
    "            \n",
    "            if nodoSiguienteRight is not None:\n",
    "                nd.right = decode_node(right, nodoSiguienteRight)\n",
    "             \n",
    "            if nodoSiguienteLeft is not None:\n",
    "                nd.left  = decode_node(left, nodoSiguienteLeft)\n",
    "            \n",
    "            return nd\n",
    "            \n",
    "    createNode.count = 0\n",
    "    dec = decode_node (v, root)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder con batch - decoder con batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3i0dtbbo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd617e08ec1e4a88a44a128eed6ea392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▃▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1774</td></tr><tr><td>loss</td><td>0.05001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">stilted-salad-171</strong>: <a href=\"https://wandb.ai/paufeldman/autoencoder3/runs/3i0dtbbo\" target=\"_blank\">https://wandb.ai/paufeldman/autoencoder3/runs/3i0dtbbo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221117_181147-3i0dtbbo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3i0dtbbo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43adbe646f14831804c85ceb9fbac85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666687282, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\wandb\\run-20221117_181533-2l6o4ukh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/paufeldman/autoencoder3/runs/2l6o4ukh\" target=\"_blank\">restful-surf-172</a></strong> to <a href=\"https://wandb.ai/paufeldman/autoencoder3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/paufeldman/autoencoder3/runs/2l6o4ukh?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2c7d3ef3e20>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 6000\n",
    "learning_rate = 1e-5\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "#opt = torch.optim.Adam(params, lr=learning_rate, weight_decay=0.0001) \n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "#opt = torch.optim.SGD(params, lr=learning_rate, momentum = 0.96) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[100], gamma=0.2)\n",
    "import wandb\n",
    "config = {\n",
    "  \"learning_rate\": learning_rate,\n",
    "  \"epochs\": epochs,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"dataset\": t_list,\n",
    "  \"number of trees\": len(data_loader)*batch_size,\n",
    "  \"optim\": opt\n",
    "}\n",
    "wandb.init(project=\"autoencoder3\", entity=\"paufeldman\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 6000] average reconstruction error: 1.95905197 \n",
      "Epoch [11 / 6000] average reconstruction error: 1.91309071 \n",
      "Epoch [21 / 6000] average reconstruction error: 1.86809528 \n",
      "Epoch [31 / 6000] average reconstruction error: 1.82392526 \n",
      "Epoch [41 / 6000] average reconstruction error: 1.78067183 \n",
      "Epoch [51 / 6000] average reconstruction error: 1.73768640 \n",
      "Epoch [61 / 6000] average reconstruction error: 1.69518852 \n",
      "Epoch [71 / 6000] average reconstruction error: 1.65274644 \n",
      "Epoch [81 / 6000] average reconstruction error: 1.61019063 \n",
      "Epoch [91 / 6000] average reconstruction error: 1.56725144 \n",
      "Epoch [101 / 6000] average reconstruction error: 1.52386618 \n",
      "Epoch [111 / 6000] average reconstruction error: 1.47983694 \n",
      "Epoch [121 / 6000] average reconstruction error: 1.43487585 \n",
      "Epoch [131 / 6000] average reconstruction error: 1.38895106 \n",
      "Epoch [141 / 6000] average reconstruction error: 1.34195733 \n",
      "Epoch [151 / 6000] average reconstruction error: 1.29365695 \n",
      "Epoch [161 / 6000] average reconstruction error: 1.24395740 \n",
      "Epoch [171 / 6000] average reconstruction error: 1.19301605 \n",
      "Epoch [181 / 6000] average reconstruction error: 1.14082456 \n",
      "Epoch [191 / 6000] average reconstruction error: 1.08757508 \n",
      "Epoch [201 / 6000] average reconstruction error: 1.03340495 \n",
      "Epoch [211 / 6000] average reconstruction error: 0.97910821 \n",
      "Epoch [221 / 6000] average reconstruction error: 0.92528164 \n",
      "Epoch [231 / 6000] average reconstruction error: 0.87328744 \n",
      "Epoch [241 / 6000] average reconstruction error: 0.82475692 \n",
      "Epoch [251 / 6000] average reconstruction error: 0.78185743 \n",
      "Epoch [261 / 6000] average reconstruction error: 0.74674183 \n",
      "Epoch [271 / 6000] average reconstruction error: 0.72086483 \n",
      "Epoch [281 / 6000] average reconstruction error: 0.70414490 \n",
      "Epoch [291 / 6000] average reconstruction error: 0.69457948 \n",
      "Epoch [301 / 6000] average reconstruction error: 0.68939006 \n",
      "Epoch [311 / 6000] average reconstruction error: 0.68646061 \n",
      "Epoch [321 / 6000] average reconstruction error: 0.68458045 \n",
      "Epoch [331 / 6000] average reconstruction error: 0.68331689 \n",
      "Epoch [341 / 6000] average reconstruction error: 0.68241781 \n",
      "Epoch [351 / 6000] average reconstruction error: 0.68176860 \n",
      "Epoch [361 / 6000] average reconstruction error: 0.68119454 \n",
      "Epoch [371 / 6000] average reconstruction error: 0.68063194 \n",
      "Epoch [381 / 6000] average reconstruction error: 0.68011409 \n",
      "Epoch [391 / 6000] average reconstruction error: 0.67969650 \n",
      "Epoch [401 / 6000] average reconstruction error: 0.67921072 \n",
      "Epoch [411 / 6000] average reconstruction error: 0.67875552 \n",
      "Epoch [421 / 6000] average reconstruction error: 0.67825824 \n",
      "Epoch [431 / 6000] average reconstruction error: 0.67774689 \n",
      "Epoch [441 / 6000] average reconstruction error: 0.67727983 \n",
      "Epoch [451 / 6000] average reconstruction error: 0.67684281 \n",
      "Epoch [461 / 6000] average reconstruction error: 0.67638272 \n",
      "Epoch [471 / 6000] average reconstruction error: 0.67585886 \n",
      "Epoch [481 / 6000] average reconstruction error: 0.67538595 \n",
      "Epoch [491 / 6000] average reconstruction error: 0.67490792 \n",
      "Epoch [501 / 6000] average reconstruction error: 0.67440951 \n",
      "Epoch [511 / 6000] average reconstruction error: 0.67386836 \n",
      "Epoch [521 / 6000] average reconstruction error: 0.67338288 \n",
      "Epoch [531 / 6000] average reconstruction error: 0.67288768 \n",
      "Epoch [541 / 6000] average reconstruction error: 0.67231750 \n",
      "Epoch [551 / 6000] average reconstruction error: 0.67179048 \n",
      "Epoch [561 / 6000] average reconstruction error: 0.67128748 \n",
      "Epoch [571 / 6000] average reconstruction error: 0.67068374 \n",
      "Epoch [581 / 6000] average reconstruction error: 0.67014039 \n",
      "Epoch [591 / 6000] average reconstruction error: 0.66957152 \n",
      "Epoch [601 / 6000] average reconstruction error: 0.66901141 \n",
      "Epoch [611 / 6000] average reconstruction error: 0.66838515 \n",
      "Epoch [621 / 6000] average reconstruction error: 0.66775650 \n",
      "Epoch [631 / 6000] average reconstruction error: 0.66711897 \n",
      "Epoch [641 / 6000] average reconstruction error: 0.66643441 \n",
      "Epoch [651 / 6000] average reconstruction error: 0.66572553 \n",
      "Epoch [661 / 6000] average reconstruction error: 0.66505396 \n",
      "Epoch [671 / 6000] average reconstruction error: 0.66432428 \n",
      "Epoch [681 / 6000] average reconstruction error: 0.66353321 \n",
      "Epoch [691 / 6000] average reconstruction error: 0.66266590 \n",
      "Epoch [701 / 6000] average reconstruction error: 0.66187024 \n",
      "Epoch [711 / 6000] average reconstruction error: 0.66094363 \n",
      "Epoch [721 / 6000] average reconstruction error: 0.66004640 \n",
      "Epoch [731 / 6000] average reconstruction error: 0.65901017 \n",
      "Epoch [741 / 6000] average reconstruction error: 0.65800965 \n",
      "Epoch [751 / 6000] average reconstruction error: 0.65685976 \n",
      "Epoch [761 / 6000] average reconstruction error: 0.65571618 \n",
      "Epoch [771 / 6000] average reconstruction error: 0.65443963 \n",
      "Epoch [781 / 6000] average reconstruction error: 0.65309596 \n",
      "Epoch [791 / 6000] average reconstruction error: 0.65159464 \n",
      "Epoch [801 / 6000] average reconstruction error: 0.65002060 \n",
      "Epoch [811 / 6000] average reconstruction error: 0.64827073 \n",
      "Epoch [821 / 6000] average reconstruction error: 0.64637351 \n",
      "Epoch [831 / 6000] average reconstruction error: 0.64431435 \n",
      "Epoch [841 / 6000] average reconstruction error: 0.64198440 \n",
      "Epoch [851 / 6000] average reconstruction error: 0.63944328 \n",
      "Epoch [861 / 6000] average reconstruction error: 0.63654572 \n",
      "Epoch [871 / 6000] average reconstruction error: 0.63329726 \n",
      "Epoch [881 / 6000] average reconstruction error: 0.62958908 \n",
      "Epoch [891 / 6000] average reconstruction error: 0.62523019 \n",
      "Epoch [901 / 6000] average reconstruction error: 0.62001026 \n",
      "Epoch [911 / 6000] average reconstruction error: 0.61376703 \n",
      "Epoch [921 / 6000] average reconstruction error: 0.60616243 \n",
      "Epoch [931 / 6000] average reconstruction error: 0.59660220 \n",
      "Epoch [941 / 6000] average reconstruction error: 0.58403206 \n",
      "Epoch [951 / 6000] average reconstruction error: 0.56743103 \n",
      "Epoch [961 / 6000] average reconstruction error: 0.54546970 \n",
      "Epoch [971 / 6000] average reconstruction error: 0.52239424 \n",
      "Epoch [981 / 6000] average reconstruction error: 0.50967836 \n",
      "Epoch [991 / 6000] average reconstruction error: 0.50363696 \n",
      "Epoch [1001 / 6000] average reconstruction error: 0.49880034 \n",
      "Epoch [1011 / 6000] average reconstruction error: 0.49426678 \n",
      "Epoch [1021 / 6000] average reconstruction error: 0.49010712 \n",
      "Epoch [1031 / 6000] average reconstruction error: 0.48632276 \n",
      "Epoch [1041 / 6000] average reconstruction error: 0.48246962 \n",
      "Epoch [1051 / 6000] average reconstruction error: 0.47804031 \n",
      "Epoch [1061 / 6000] average reconstruction error: 0.47230875 \n",
      "Epoch [1071 / 6000] average reconstruction error: 0.46210888 \n",
      "Epoch [1081 / 6000] average reconstruction error: 0.42398247 \n",
      "Epoch [1091 / 6000] average reconstruction error: 0.35647005 \n",
      "Epoch [1101 / 6000] average reconstruction error: 0.32906935 \n",
      "Epoch [1111 / 6000] average reconstruction error: 0.31568116 \n",
      "Epoch [1121 / 6000] average reconstruction error: 0.30352736 \n",
      "Epoch [1131 / 6000] average reconstruction error: 0.29369602 \n",
      "Epoch [1141 / 6000] average reconstruction error: 0.28521904 \n",
      "Epoch [1151 / 6000] average reconstruction error: 0.27745125 \n",
      "Epoch [1161 / 6000] average reconstruction error: 0.27024761 \n",
      "Epoch [1171 / 6000] average reconstruction error: 0.26340792 \n",
      "Epoch [1181 / 6000] average reconstruction error: 0.25713575 \n",
      "Epoch [1191 / 6000] average reconstruction error: 0.25126159 \n",
      "Epoch [1201 / 6000] average reconstruction error: 0.24586117 \n",
      "Epoch [1211 / 6000] average reconstruction error: 0.24089271 \n",
      "Epoch [1221 / 6000] average reconstruction error: 0.23630673 \n",
      "Epoch [1231 / 6000] average reconstruction error: 0.23211715 \n",
      "Epoch [1241 / 6000] average reconstruction error: 0.22828889 \n",
      "Epoch [1251 / 6000] average reconstruction error: 0.22478543 \n",
      "Epoch [1261 / 6000] average reconstruction error: 0.22152492 \n",
      "Epoch [1271 / 6000] average reconstruction error: 0.21854334 \n",
      "Epoch [1281 / 6000] average reconstruction error: 0.21580413 \n",
      "Epoch [1291 / 6000] average reconstruction error: 0.21330668 \n",
      "Epoch [1301 / 6000] average reconstruction error: 0.21096233 \n",
      "Epoch [1311 / 6000] average reconstruction error: 0.20886827 \n",
      "Epoch [1321 / 6000] average reconstruction error: 0.20699731 \n",
      "Epoch [1331 / 6000] average reconstruction error: 0.20536789 \n",
      "Epoch [1341 / 6000] average reconstruction error: 0.20400310 \n",
      "Epoch [1351 / 6000] average reconstruction error: 0.20283300 \n",
      "Epoch [1361 / 6000] average reconstruction error: 0.20186585 \n",
      "Epoch [1371 / 6000] average reconstruction error: 0.20106283 \n",
      "Epoch [1381 / 6000] average reconstruction error: 0.20036319 \n",
      "Epoch [1391 / 6000] average reconstruction error: 0.19975959 \n",
      "Epoch [1401 / 6000] average reconstruction error: 0.19919807 \n",
      "Epoch [1411 / 6000] average reconstruction error: 0.19869426 \n",
      "Epoch [1421 / 6000] average reconstruction error: 0.19823700 \n",
      "Epoch [1431 / 6000] average reconstruction error: 0.19779316 \n",
      "Epoch [1441 / 6000] average reconstruction error: 0.19738279 \n",
      "Epoch [1451 / 6000] average reconstruction error: 0.19698335 \n",
      "Epoch [1461 / 6000] average reconstruction error: 0.19661069 \n",
      "Epoch [1471 / 6000] average reconstruction error: 0.19626321 \n",
      "Epoch [1481 / 6000] average reconstruction error: 0.19593269 \n",
      "Epoch [1491 / 6000] average reconstruction error: 0.19559216 \n",
      "Epoch [1501 / 6000] average reconstruction error: 0.19529486 \n",
      "Epoch [1511 / 6000] average reconstruction error: 0.19498125 \n",
      "Epoch [1521 / 6000] average reconstruction error: 0.19468576 \n",
      "Epoch [1531 / 6000] average reconstruction error: 0.19442819 \n",
      "Epoch [1541 / 6000] average reconstruction error: 0.19415380 \n",
      "Epoch [1551 / 6000] average reconstruction error: 0.19390383 \n",
      "Epoch [1561 / 6000] average reconstruction error: 0.19366223 \n",
      "Epoch [1571 / 6000] average reconstruction error: 0.19342454 \n",
      "Epoch [1581 / 6000] average reconstruction error: 0.19318661 \n",
      "Epoch [1591 / 6000] average reconstruction error: 0.19295993 \n",
      "Epoch [1601 / 6000] average reconstruction error: 0.19273826 \n",
      "Epoch [1611 / 6000] average reconstruction error: 0.19253480 \n",
      "Epoch [1621 / 6000] average reconstruction error: 0.19231758 \n",
      "Epoch [1631 / 6000] average reconstruction error: 0.19211879 \n",
      "Epoch [1641 / 6000] average reconstruction error: 0.19191816 \n",
      "Epoch [1651 / 6000] average reconstruction error: 0.19173296 \n",
      "Epoch [1661 / 6000] average reconstruction error: 0.19153985 \n",
      "Epoch [1671 / 6000] average reconstruction error: 0.19137213 \n",
      "Epoch [1681 / 6000] average reconstruction error: 0.19118613 \n",
      "Epoch [1691 / 6000] average reconstruction error: 0.19101305 \n",
      "Epoch [1701 / 6000] average reconstruction error: 0.19084418 \n",
      "Epoch [1711 / 6000] average reconstruction error: 0.19067632 \n",
      "Epoch [1721 / 6000] average reconstruction error: 0.19050857 \n",
      "Epoch [1731 / 6000] average reconstruction error: 0.19034652 \n",
      "Epoch [1741 / 6000] average reconstruction error: 0.19019635 \n",
      "Epoch [1751 / 6000] average reconstruction error: 0.19004090 \n",
      "Epoch [1761 / 6000] average reconstruction error: 0.18988483 \n",
      "Epoch [1771 / 6000] average reconstruction error: 0.18973109 \n",
      "Epoch [1781 / 6000] average reconstruction error: 0.18958999 \n",
      "Epoch [1791 / 6000] average reconstruction error: 0.18945037 \n",
      "Epoch [1801 / 6000] average reconstruction error: 0.18931088 \n",
      "Epoch [1811 / 6000] average reconstruction error: 0.18915877 \n",
      "Epoch [1821 / 6000] average reconstruction error: 0.18901585 \n",
      "Epoch [1831 / 6000] average reconstruction error: 0.18888412 \n",
      "Epoch [1841 / 6000] average reconstruction error: 0.18875936 \n",
      "Epoch [1851 / 6000] average reconstruction error: 0.18862131 \n",
      "Epoch [1861 / 6000] average reconstruction error: 0.18849073 \n",
      "Epoch [1871 / 6000] average reconstruction error: 0.18836981 \n",
      "Epoch [1881 / 6000] average reconstruction error: 0.18823603 \n",
      "Epoch [1891 / 6000] average reconstruction error: 0.18811971 \n",
      "Epoch [1901 / 6000] average reconstruction error: 0.18799594 \n",
      "Epoch [1911 / 6000] average reconstruction error: 0.18788108 \n",
      "Epoch [1921 / 6000] average reconstruction error: 0.18773851 \n",
      "Epoch [1931 / 6000] average reconstruction error: 0.18762094 \n",
      "Epoch [1941 / 6000] average reconstruction error: 0.18750125 \n",
      "Epoch [1951 / 6000] average reconstruction error: 0.18738282 \n",
      "Epoch [1961 / 6000] average reconstruction error: 0.18727413 \n",
      "Epoch [1971 / 6000] average reconstruction error: 0.18715239 \n",
      "Epoch [1981 / 6000] average reconstruction error: 0.18704304 \n",
      "Epoch [1991 / 6000] average reconstruction error: 0.18692620 \n",
      "Epoch [2001 / 6000] average reconstruction error: 0.18680589 \n",
      "Epoch [2011 / 6000] average reconstruction error: 0.18668832 \n",
      "Epoch [2021 / 6000] average reconstruction error: 0.18657847 \n",
      "Epoch [2031 / 6000] average reconstruction error: 0.18647374 \n",
      "Epoch [2041 / 6000] average reconstruction error: 0.18635735 \n",
      "Epoch [2051 / 6000] average reconstruction error: 0.18625310 \n",
      "Epoch [2061 / 6000] average reconstruction error: 0.18613878 \n",
      "Epoch [2071 / 6000] average reconstruction error: 0.18604305 \n",
      "Epoch [2081 / 6000] average reconstruction error: 0.18593499 \n",
      "Epoch [2091 / 6000] average reconstruction error: 0.18582556 \n",
      "Epoch [2101 / 6000] average reconstruction error: 0.18571056 \n",
      "Epoch [2111 / 6000] average reconstruction error: 0.18561067 \n",
      "Epoch [2121 / 6000] average reconstruction error: 0.18550234 \n",
      "Epoch [2131 / 6000] average reconstruction error: 0.18539904 \n",
      "Epoch [2141 / 6000] average reconstruction error: 0.18528146 \n",
      "Epoch [2151 / 6000] average reconstruction error: 0.18517500 \n",
      "Epoch [2161 / 6000] average reconstruction error: 0.18507203 \n",
      "Epoch [2171 / 6000] average reconstruction error: 0.18497485 \n",
      "Epoch [2181 / 6000] average reconstruction error: 0.18486330 \n",
      "Epoch [2191 / 6000] average reconstruction error: 0.18475631 \n",
      "Epoch [2201 / 6000] average reconstruction error: 0.18464959 \n",
      "Epoch [2211 / 6000] average reconstruction error: 0.18455750 \n",
      "Epoch [2221 / 6000] average reconstruction error: 0.18445803 \n",
      "Epoch [2231 / 6000] average reconstruction error: 0.18434621 \n",
      "Epoch [2241 / 6000] average reconstruction error: 0.18423930 \n",
      "Epoch [2251 / 6000] average reconstruction error: 0.18413177 \n",
      "Epoch [2261 / 6000] average reconstruction error: 0.18403649 \n",
      "Epoch [2271 / 6000] average reconstruction error: 0.18393935 \n",
      "Epoch [2281 / 6000] average reconstruction error: 0.18382755 \n",
      "Epoch [2291 / 6000] average reconstruction error: 0.18373311 \n",
      "Epoch [2301 / 6000] average reconstruction error: 0.18362036 \n",
      "Epoch [2311 / 6000] average reconstruction error: 0.18351226 \n",
      "Epoch [2321 / 6000] average reconstruction error: 0.18341766 \n",
      "Epoch [2331 / 6000] average reconstruction error: 0.18330473 \n",
      "Epoch [2341 / 6000] average reconstruction error: 0.18321535 \n",
      "Epoch [2351 / 6000] average reconstruction error: 0.18311380 \n",
      "Epoch [2361 / 6000] average reconstruction error: 0.18299402 \n",
      "Epoch [2371 / 6000] average reconstruction error: 0.18289331 \n",
      "Epoch [2381 / 6000] average reconstruction error: 0.18279821 \n",
      "Epoch [2391 / 6000] average reconstruction error: 0.18269512 \n",
      "Epoch [2401 / 6000] average reconstruction error: 0.18256964 \n",
      "Epoch [2411 / 6000] average reconstruction error: 0.18245839 \n",
      "Epoch [2421 / 6000] average reconstruction error: 0.18236277 \n",
      "Epoch [2431 / 6000] average reconstruction error: 0.18226144 \n",
      "Epoch [2441 / 6000] average reconstruction error: 0.18214682 \n",
      "Epoch [2451 / 6000] average reconstruction error: 0.18205833 \n",
      "Epoch [2461 / 6000] average reconstruction error: 0.18193188 \n",
      "Epoch [2471 / 6000] average reconstruction error: 0.18182807 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Documents\\rpoditela\\Intra\\autoencoder\\autoencoder_fold.ipynb Celda 41\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X55sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m#print(\"total_loss\", total_loss)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X55sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X55sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X55sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/rpoditela/Intra/autoencoder/autoencoder_fold.ipynb#X55sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m#scheduler.step()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py_torc\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py_torc\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    \n",
    "train_loss_avg = []\n",
    "\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        enc_fold = torch_f.Fold(device)\n",
    "        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "        # Collect computation nodes recursively from encoding process\n",
    "        n_nodes = []\n",
    "        for example in batch: #example es un arbolito\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            encode_structure_fold(enc_fold, example)\n",
    "            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "       \n",
    "        # Apply the computations on the encoder model\n",
    "       \n",
    "        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "        \n",
    "        \n",
    "        # Initialize torchfold for *decoding*\n",
    "        dec_fold = torch_f.Fold(device)\n",
    "        # Collect computation nodes recursively from decoding process\n",
    "        dec_fold_nodes = []\n",
    "        kld_fold_nodes = []\n",
    "\n",
    "        t_l = []\n",
    "        for f in enc_fold_nodes:\n",
    "            for t in f:\n",
    "                t_l.append(t)\n",
    "        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\n",
    "            #print(\"example\", example)\n",
    "            #print(\"fnode\", fnode) \n",
    "            #root_code, kl_div = torch.chunk(fnode, 2, 0)\n",
    "            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\n",
    "        # Apply the computations on the decoder model\n",
    "\n",
    "                       \n",
    "        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        #print(\"n\", n_nodes)\n",
    "        total_loss = torch.div(total_loss[0], n_nodes)\n",
    "        #print(\"div\", total_loss)\n",
    "        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        total_loss = total_loss#*10\n",
    "        \n",
    "        #print(\"total_loss\", total_loss)\n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %.8f ' % (epoch+1, epochs, total_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder con batch - decoder sin batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        # Initialize torchfold for *encoding*\\n\\n        \\n        enc_fold = torch_f.Fold(device)\\n        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\\n        # Collect computation nodes recursively from encoding process\\n        n_nodes = []\\n        for example in batch: #example es un arbolito\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            encode_structure_fold(enc_fold, example)\\n            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\\n       \\n        # Apply the computations on the encoder model\\n       \\n        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\\n        encodeado_con_batch = enc_fold_nodes\\n        \\n        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\\n        #print(\"decoded\", decoded)\\n        l = []\\n        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\\n        l = []\\n        ce_loss_list = decoded.traverseInorderCE(decoded, l)\\n            \\n        mse_loss = sum(mse_loss_list) \\n        ce_loss  = sum(ce_loss_list)  \\n        total_loss = (0.5*ce_loss + mse_loss)\\n        #print(\"total_loss\", total_loss)\\n        total_loss = total_loss / len(mse_loss_list)\\n        \\n        \\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        enc_fold = torch_f.Fold(device)\n",
    "        enc_fold_nodes = []     # list of fold nodes for encoding, lista con la \"hoja de ruta\" de los dos arboles\n",
    "        # Collect computation nodes recursively from encoding process\n",
    "        n_nodes = []\n",
    "        for example in batch: #example es un arbolito\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            encode_structure_fold(enc_fold, example)\n",
    "            enc_fold_nodes.append(encode_structure_fold(enc_fold, example))\n",
    "       \n",
    "        # Apply the computations on the encoder model\n",
    "       \n",
    "        enc_fold_nodes = enc_fold.apply(Grassencoder, [enc_fold_nodes])\n",
    "        encodeado_con_batch = enc_fold_nodes\n",
    "        \n",
    "        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\n",
    "        #print(\"decoded\", decoded)\n",
    "        l = []\n",
    "        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\n",
    "        l = []\n",
    "        ce_loss_list = decoded.traverseInorderCE(decoded, l)\n",
    "            \n",
    "        mse_loss = sum(mse_loss_list) \n",
    "        ce_loss  = sum(ce_loss_list)  \n",
    "        total_loss = (0.5*ce_loss + mse_loss)\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        total_loss = total_loss / len(mse_loss_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder sin batch - decoder con batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        # Initialize torchfold for *encoding*\\n\\n        \\n        \\n        enc_fold_nodes = []\\n        n_nodes = []\\n        for example in batch:\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            enc_fold = encode_structure(example).to(device)\\n        #print(\"encodeado sin batch\", enc_fold)\\n        enc_fold_nodes.append(enc_fold)\\n        encodeado_sin_batch = enc_fold\\n        # Split into a list of fold nodes per example\\n        #enc_fold_nodes = torch.split(enc_fold_nodes[0], 1, 0) #divide ele ncodeado en vectores de un elemento\\n        \\n        \\n        # Initialize torchfold for *decoding*\\n        dec_fold = torch_f.Fold(device)\\n        # Collect computation nodes recursively from decoding process\\n        dec_fold_nodes = []\\n        kld_fold_nodes = []\\n\\n        t_l = []\\n        for f in enc_fold_nodes:\\n            for t in f:\\n                t_l.append(t)\\n        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\\n            #print(\"example\", example)\\n            #print(\"fnode\", fnode) \\n            #root_code, kl_div = torch.chunk(fnode, 2, 0)\\n            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\\n        # Apply the computations on the decoder model\\n        #print(\"dec fold nodes\", dec_fold_nodes)\\n           \\n                       \\n        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\\n        #print(\"total_loss\", total_loss)\\n        n_nodes = torch.tensor(n_nodes, device = device)\\n        #print(\"n\", n_nodes)\\n        total_loss = torch.div(total_loss[0], n_nodes)\\n        #print(\"div\", total_loss)\\n        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\\n        #print(\"total_loss\", total_loss)\\n        \\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        mse_loss_avg[-1] += (mse_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Initialize torchfold for *encoding*\n",
    "\n",
    "        \n",
    "        \n",
    "        enc_fold_nodes = []\n",
    "        n_nodes = []\n",
    "        for example in batch:\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            enc_fold = encode_structure(example).to(device)\n",
    "        #print(\"encodeado sin batch\", enc_fold)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "        encodeado_sin_batch = enc_fold\n",
    "        # Split into a list of fold nodes per example\n",
    "        #enc_fold_nodes = torch.split(enc_fold_nodes[0], 1, 0) #divide ele ncodeado en vectores de un elemento\n",
    "        \n",
    "        \n",
    "        # Initialize torchfold for *decoding*\n",
    "        dec_fold = torch_f.Fold(device)\n",
    "        # Collect computation nodes recursively from decoding process\n",
    "        dec_fold_nodes = []\n",
    "        kld_fold_nodes = []\n",
    "\n",
    "        t_l = []\n",
    "        for f in enc_fold_nodes:\n",
    "            for t in f:\n",
    "                t_l.append(t)\n",
    "        for example, fnode in zip(batch, t_l): #example es el arbol y fnode el encodeado\n",
    "            #print(\"example\", example)\n",
    "            #print(\"fnode\", fnode) \n",
    "            #root_code, kl_div = torch.chunk(fnode, 2, 0)\n",
    "            dec_fold_nodes.append(decode_structure_fold_grass(dec_fold, fnode, example))\n",
    "        # Apply the computations on the decoder model\n",
    "        #print(\"dec fold nodes\", dec_fold_nodes)\n",
    "           \n",
    "                       \n",
    "        total_loss = dec_fold.apply(Grassdecoder, [dec_fold_nodes])#[0]\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        n_nodes = torch.tensor(n_nodes, device = device)\n",
    "        #print(\"n\", n_nodes)\n",
    "        total_loss = torch.div(total_loss[0], n_nodes)\n",
    "        #print(\"div\", total_loss)\n",
    "        total_loss = total_loss.sum() / len(batch)  #n_nodes[0] #modificar y dividir por el promedio?\n",
    "        #print(\"total_loss\", total_loss)\n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        mse_loss_avg[-1] += (mse_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder sin batch - decoder sin batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \\n\\nopt = torch.optim.Adam(params, lr=learning_rate) \\n\\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\\n    \\ntrain_loss_avg = []\\n#train_loss_avg.append(0)\\nce_avg = []\\nmse_avg = []\\nlr_list = []\\n    \\nfor epoch in range(epochs):\\n    train_loss_avg.append(0)\\n    ce_avg.append(0)\\n    mse_avg.append(0)\\n\\n   #batch es cada arbol del dataloader\\n    for batch_idx, batch in enumerate(data_loader):\\n        \\n        enc_fold_nodes = []\\n        n_nodes = []\\n        for example in batch:\\n            c = []\\n            n = example.count_nodes(example, c)\\n            n_nodes.append(len(n))\\n            enc_fold = encode_structure(example).to(device)\\n        #print(\"encodeado sin batch\", enc_fold)\\n        enc_fold_nodes.append(enc_fold)\\n        encodeado_sin_batch = enc_fold\\n        \\n        \\n        \\n        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\\n        #print(\"decoded\", decoded)\\n        l = []\\n        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\\n        l = []\\n        ce_loss_list = decoded.traverseInorderCE(decoded, l)\\n            \\n        mse_loss = sum(mse_loss_list) \\n        ce_loss  = sum(ce_loss_list)  \\n       \\n        ce = [0.4*a for a in ce_loss_list]\\n\\n\\n        total_loss = (0.4*ce_loss + mse_loss)\\n        total_loss = total_loss / len(mse_loss_list)\\n        \\n        opt.zero_grad()\\n        total_loss.backward()\\n        opt.step()\\n        #scheduler.step()\\n        #decoder_opt.step()\\n        train_loss_avg[-1] += (total_loss.item())\\n        mse_avg[-1] += (mse_loss.item())\\n        ce_avg[-1] += (ce_loss.item())\\n        \\n\\n    wandb.log({\\'epoch\\': epoch+1, \\'loss\\': total_loss, \\'mse loss\\': mse_loss, \\'ce loss\\': ce_loss})\\n    save_best_model(\\n        total_loss, epoch, Grassencoder, Grassdecoder, opt)\\n    if epoch % 10 == 0:\\n        print(\\'Epoch [%d / %d] average reconstruction error: %f \\' % (epoch+1, epochs, total_loss))\\n'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params = list(Grassencoder.parameters()) + list(Grassdecoder.parameters()) \n",
    "\n",
    "opt = torch.optim.Adam(params, lr=learning_rate) \n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[200], gamma=0.1)\n",
    "    \n",
    "train_loss_avg = []\n",
    "#train_loss_avg.append(0)\n",
    "ce_avg = []\n",
    "mse_avg = []\n",
    "lr_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss_avg.append(0)\n",
    "    ce_avg.append(0)\n",
    "    mse_avg.append(0)\n",
    "\n",
    "   #batch es cada arbol del dataloader\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "        enc_fold_nodes = []\n",
    "        n_nodes = []\n",
    "        for example in batch:\n",
    "            c = []\n",
    "            n = example.count_nodes(example, c)\n",
    "            n_nodes.append(len(n))\n",
    "            enc_fold = encode_structure(example).to(device)\n",
    "        #print(\"encodeado sin batch\", enc_fold)\n",
    "        enc_fold_nodes.append(enc_fold)\n",
    "        encodeado_sin_batch = enc_fold\n",
    "        \n",
    "        \n",
    "        \n",
    "        decoded = decode_structure_fold_(enc_fold_nodes[0], batch[0])\n",
    "        #print(\"decoded\", decoded)\n",
    "        l = []\n",
    "        mse_loss_list = decoded.traverseInorderMSE(decoded, l)\n",
    "        l = []\n",
    "        ce_loss_list = decoded.traverseInorderCE(decoded, l)\n",
    "            \n",
    "        mse_loss = sum(mse_loss_list) \n",
    "        ce_loss  = sum(ce_loss_list)  \n",
    "       \n",
    "        ce = [0.4*a for a in ce_loss_list]\n",
    "\n",
    "\n",
    "        total_loss = (0.4*ce_loss + mse_loss)\n",
    "        total_loss = total_loss / len(mse_loss_list)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "        #decoder_opt.step()\n",
    "        train_loss_avg[-1] += (total_loss.item())\n",
    "        mse_avg[-1] += (mse_loss.item())\n",
    "        ce_avg[-1] += (ce_loss.item())\n",
    "        \n",
    "\n",
    "    wandb.log({'epoch': epoch+1, 'loss': total_loss, 'mse loss': mse_loss, 'ce loss': ce_loss})\n",
    "    save_best_model(\n",
    "        total_loss, epoch, Grassencoder, Grassdecoder, opt)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f ' % (epoch+1, epochs, total_loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2038\n"
     ]
    }
   ],
   "source": [
    "encoder = GRASSEncoder(input_size = 4, feature_size=128, hidden_size=256).to(device)\n",
    "decoder = GRASSDecoder(latent_size=128, hidden_size=256, mult = mult).to(device)\n",
    "\n",
    "checkpoint = torch.load(\"outputs/best_model.pth\")\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "print(\"epoch\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[0.0634, 0.7285, 0.3215, 0.0282]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "1 tensor([[0.2109, 0.6542, 0.3689, 0.1998]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "2 tensor([[0.2564, 0.6333, 0.3377, 0.2147]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "3 tensor([[0.3407, 0.6620, 0.3227, 0.2852]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "4 tensor([[0.3836, 0.6828, 0.3292, 0.3032]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "5 tensor([[0.4178, 0.6930, 0.3328, 0.3029]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "6 tensor([[0.4492, 0.6987, 0.3332, 0.2966]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "7 tensor([[0.4816, 0.7032, 0.3314, 0.2889]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "8 tensor([[0.5166, 0.7086, 0.3283, 0.2823]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "9 tensor([[0.5544, 0.7156, 0.3243, 0.2778]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "10 tensor([[0.5954, 0.7250, 0.3194, 0.2766]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "11 tensor([[0.6392, 0.7370, 0.3140, 0.2801]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "12 tensor([[0.6854, 0.7522, 0.3084, 0.2891]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "13 tensor([[0.7332, 0.7712, 0.3029, 0.3052]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "14 tensor([[0.7813, 0.7939, 0.2984, 0.3301]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "15 tensor([[0.8281, 0.8206, 0.2953, 0.3647]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "16 tensor([[0.8718, 0.8509, 0.2946, 0.4101]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "17 tensor([[0.9106, 0.8833, 0.2972, 0.4664]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "18 tensor([[0.9428, 0.9162, 0.3035, 0.5319]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "19 tensor([[0.1009, 0.6642, 0.3970, 0.9955]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "20 tensor([[0.0246, 0.6331, 0.4745, 0.8808]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "21 tensor([[0.0088, 0.5781, 0.5080, 0.7978]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "22 tensor([[0.0042, 0.5353, 0.5364, 0.7646]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "23 tensor([[0.0048, 0.4989, 0.5666, 0.7450]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "24 tensor([[0.0074, 0.4647, 0.5978, 0.7299]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "25 tensor([[0.0115, 0.4310, 0.6292, 0.7172]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "26 tensor([[0.0169, 0.3969, 0.6611, 0.7059]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "27 tensor([[0.0240, 0.3620, 0.6933, 0.6953]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "28 tensor([[0.0331, 0.3265, 0.7259, 0.6850]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "29 tensor([[0.0447, 0.2901, 0.7586, 0.6748]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "30 tensor([[0.0590, 0.2530, 0.7914, 0.6642]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "31 tensor([[0.0767, 0.2151, 0.8238, 0.6529]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "32 tensor([[0.0981, 0.1767, 0.8555, 0.6401]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "33 tensor([[0.1232, 0.1381, 0.8858, 0.6258]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "34 tensor([[0.1524, 0.0999, 0.9143, 0.6095]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "35 tensor([[0.1855, 0.0626, 0.9403, 0.5913]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "36 tensor([[0.2220, 0.0271, 0.9630, 0.5715]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "37 tensor([[ 0.2611, -0.0057,  0.9819,  0.5504]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "38 tensor([[7.8362e-02, 7.4375e-01, 2.9795e-01, 4.9815e-04]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "39 tensor([[0.0892, 0.7562, 0.2732, 0.0154]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "40 tensor([[0.1213, 0.7791, 0.2449, 0.0492]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "41 tensor([[0.1749, 0.7602, 0.2159, 0.0888]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "42 tensor([[0.2122, 0.7487, 0.2046, 0.0943]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "43 tensor([[0.2488, 0.7451, 0.1979, 0.0946]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "44 tensor([[0.2851, 0.7466, 0.1923, 0.0947]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "45 tensor([[0.3209, 0.7508, 0.1864, 0.0947]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "46 tensor([[0.3564, 0.7570, 0.1798, 0.0948]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "47 tensor([[0.3917, 0.7644, 0.1722, 0.0952]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "48 tensor([[0.4271, 0.7731, 0.1637, 0.0962]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "49 tensor([[0.4627, 0.7830, 0.1541, 0.0983]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "50 tensor([[0.4985, 0.7939, 0.1437, 0.1017]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "51 tensor([[0.5345, 0.8063, 0.1326, 0.1070]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "52 tensor([[0.5706, 0.8202, 0.1209, 0.1146]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "53 tensor([[0.6066, 0.8357, 0.1088, 0.1250]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "54 tensor([[0.6420, 0.8527, 0.0966, 0.1387]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "55 tensor([[0.6768, 0.8715, 0.0846, 0.1560]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "56 tensor([[0.7103, 0.8921, 0.0730, 0.1776]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "57 tensor([[0.7422, 0.9144, 0.0622, 0.2039]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "58 tensor([[0.7720, 0.9385, 0.0527, 0.2349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "59 tensor([[0.7993, 0.9642, 0.0445, 0.2710]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = iter(data_loader).next()[0]\n",
    "enc_fold = torch_f.Fold(device)\n",
    "enc_fold_nodes = []\n",
    "enc_fold_nodes.append(encode_structure_fold(enc_fold, input))\n",
    "enc_fold_nodes = enc_fold.apply(encoder, [enc_fold_nodes])\n",
    "encoded = enc_fold_nodes[0]\n",
    "decoded = decode_testing_grass(encoded, input, 100, decoder)\n",
    "\n",
    "count = []\n",
    "numerar_nodos(decoded, count)\n",
    "decoded.traverseInorder(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c644341eec64417856214d5b7cc4071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.5, 0.5,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTree(input, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e818d2c6af4b8cbf96becb4c8c2f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.4734928…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTree(decoded, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7601bb2ec5544959c9c03fb8f211093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.4998749…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTree(decoded, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotTree(decoded, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "n_nodes = input.count_nodes(input,c)\n",
    "len(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "n_nodes = decoded.count_nodes(decoded,c)\n",
    "len(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4 2\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "decoded.traverseInorderChilds(decoded, li)\n",
    "zero = [a for a in li if a == 0]\n",
    "one = [a for a in li if a == 1]\n",
    "two = [a for a in li if a == 2]\n",
    "qzero = len(zero)\n",
    "qOne = len(one)\n",
    "qtwo = len(two)\n",
    "print(qzero, qOne, qtwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4 2\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "input.traverseInorderChilds(input, li)\n",
    "zero = [a for a in li if a == 0]\n",
    "one = [a for a in li if a == 1]\n",
    "two = [a for a in li if a == 2]\n",
    "qzero = len(zero)\n",
    "qOne = len(one)\n",
    "qtwo = len(two)\n",
    "print(qzero, qOne, qtwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (20,10))\n",
    "plt.plot(train_loss_avg) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverseleaf(root):\n",
    "    if root is not None:\n",
    "        traverseleaf(root.left)\n",
    "        if root.is_leaf():\n",
    "            print(root.radius)\n",
    "        traverseleaf(root.right)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traversebif(root):\n",
    "    if root is not None:\n",
    "        traversebif(root.left)\n",
    "        if root.is_two_child():\n",
    "            print(root.radius)\n",
    "        traversebif(root.right)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.1310e-04, -1.5467e-04, -8.6274e-05,  3.2468e-04]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9993, 0.2502, 0.2499, 0.2504]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "////\n",
      "tensor([0., 0., 0., 0.], device='cuda:0')\n",
      "tensor([1.0000, 0.2500, 0.2500, 0.2500], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "traversebif(decoded)\n",
    "print(\"////\")\n",
    "traversebif(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2226, 0.5003, 0.5002, 0.5000]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4444, 1.0000, 0.9999, 0.9997]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7145, 0.6818, 0.6727, 0.6716]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "traverseleaf(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2222, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "tensor([0.4444, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "tensor([0.7778, 0.7500, 0.7500, 0.7500], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "traverseleaf(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py_torc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f3e717cd274da89498094fde320e6eab1bf0f52911d27cf47473187acb3fe8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
